<!DOCTYPE html>
<html lang="de-DE" dir="ltr" itemscope itemtype="http://schema.org/Article">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.135.0">
    <meta name="generator" content="Relearn 6.4.1">
    <meta name="description" content="Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden) Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.
Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die Klasse.">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Entscheidungsbäume (Decision Tree Learner - DTL)">
    <meta name="twitter:description" content="Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden) Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.
Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die Klasse.">
    <meta property="og:url" content="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl.html">
    <meta property="og:title" content="Entscheidungsbäume (Decision Tree Learner - DTL)">
    <meta property="og:description" content="Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden) Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.
Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die Klasse.">
    <meta property="og:locale" content="de_DE">
    <meta property="og:type" content="website">
    <meta itemprop="name" content="Entscheidungsbäume (Decision Tree Learner - DTL)">
    <meta itemprop="description" content="Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden) Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.
Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die Klasse.">
    <meta itemprop="wordCount" content="118">
    <title>Entscheidungsbäume (Decision Tree Learner - DTL)</title>
    <link href="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl.html" rel="canonical" type="text/html" title="Entscheidungsbäume (Decision Tree Learner - DTL)">

    

    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/images/logo.png?1737742242" rel="icon" type="image/png">

    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fontawesome-all.min.css?1737742242" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fontawesome-all.min.css?1737742242" rel="stylesheet"></noscript>
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/nucleus.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/auto-complete.css?1737742242" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/auto-complete.css?1737742242" rel="stylesheet"></noscript>
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/perfect-scrollbar.min.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fonts.css?1737742242" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fonts.css?1737742242" rel="stylesheet"></noscript>
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/theme.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/theme-auto.css?1737742242" rel="stylesheet" id="R-variant-style">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/chroma-auto.css?1737742242" rel="stylesheet" id="R-variant-chroma-style">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/variant.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/print.css?1737742242" rel="stylesheet" media="print">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/format-print.css?1737742242" rel="stylesheet">
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/variant.js?1737742242"></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.relBasePath='.';
      window.relearn.relBaseUri='..\/..\/..\/..\/..';
      window.relearn.absBaseUri='https:\/\/www.hsbi.de\/elearning\/data\/FH-Bielefeld\/lm_data\/lm_1358898';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.index_js_url="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.search.js?1737742242";
      // variant stuff
      window.variants && variants.init( [ 'auto', 'zen-light', 'zen-dark', 'relearn-bright', 'relearn-light', 'relearn-dark' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script><style type="text/css">

 
.center {
    align-content: center;
    text-align: center;
    margin: auto;
}
.alert {
    color: #ff3333;
}
.bsp {
    padding: 0.05cm;
    border-width: 0.05cm;
    border-style: solid;
    border-color: #ddd;
    background-color: #ddd;
    border-radius: 25px;
    float: right;
}
.cbox {
    padding: 0.2cm;
    border-width: 0.1cm;
    border-style: solid;
    border-color: #4070a0;
    background-color: #f2f2f2;
    margin: auto;
    width: 60%;
    text-align: center;
    overflow: auto;
}
.blueArrow {
    color: #4070a0;
    font-family: "Courier New", "Courier", monospace;
    font-weight: bold;
}
.origin {
    background-color: #ededed;
    font-size: 0.8em;
}
.showme {
    background-color: #ededed;
    font-size: 0.8em;
}


 
.tldr {
    background: #dbe4ed;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.recap {
    
    
   margin: 4px 0px 26px 0px;
}
.bib {
    background: #dbe4ed;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.outcomes {
    background: #d9e9d5;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.quizzes {
    background: #d9e9d5;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.challenges {
    background: #ebe4d6;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.assignments {
    background: #ebe4d6;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
h1.tldr, h1.recap, h1.bib, h1.outcomes, h1.quizzes, h1.challenges, h1.assignments {
    padding: 0px;
}


 
.noJsAlert {
    padding: 20px;
    background-color: #f44336;  
    color: white;
    margin-bottom: 15px;
}


 
.embed-video-player {
    position: relative;
    padding-bottom: 56%;
    height: 0;
    overflow: hidden;
}
.youtube-player {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border:0;
}


 
#header-wrapper {
    padding:0.6rem;
}


 
#shortcuts {
    padding-top: 2.0rem;
}


 
#chapter p {
    text-align: left;
}


 
figcaption h4 {
    margin-top:-2.5rem;
}
.border1 {
    border:1px solid black;
}

 
td ul, td ol {
    margin: 0 0 1rem 0.5rem;
    padding: 0 0 0 0.5rem;
}

 
h1 { font-size:2.8rem !important;}
h2 { font-size:2.2rem; margin:1.2rem 0}
h3 { font-size:1.9rem; text-align:left !important; font-weight:400 !important;}
h4 { font-size:1.6rem}
h5 { font-size:1.3rem}
h6 { font-size:1rem}

h2 {
    width:100% !important;
    border-bottom:1px solid #5e5e5e !important;
    padding-bottom: 2px;
}
.tldr h2, .recap h2, .bib h2, .outcomes h2, .quizzes h2, .challenges h2, .assignments h2 {
    margin:0.5rem 0
}

.btn-crossreference, .btn-crossreference:hover {
    cursor: initial;
}

</style>

  </head>
  <body class="mobile-support print disableInlineCopyToClipboard" data-url="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
          </div>
          <span class="topbar-breadcrumbs highlightable">
            Entscheidungsbäume (Decision Tree Learner - DTL)
          </span>
          <div class="topbar-area topbar-area-end" data-area="end">
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="default">
            <header class="headline">
            </header>

<h1 id="entscheidungsbäume-decision-tree-learner---dtl">Entscheidungsbäume (Decision Tree Learner - DTL)</h1>

<p>Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden)
Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum
dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es
eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern
sind die Klassen zugeordnet.</p>
<p>Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für
jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang
aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt,
d.h. üblicherweise die Klasse.</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2.html">CAL2</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl3-pruning.html">Pruning</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl4-cal3.html">CAL3</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy.html">Entropie</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html">ID3 und C4.5</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Entscheidungsbäume (Decision Tree Learner - DTL)</h1>
<article class="default">
<h1>Machine Learning 101</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Lernen wird in der KI oft als Verhaltensänderung (eines Systems) aufgefasst. Dabei soll eine
Gütefunktion optimiert werden.</p>
<p>Je nach verfügbarem Feedback eines &quot;Lehrers&quot; werden typischerweise drei Arten von Lernen
unterschieden: Überwachtes Lernen, Unüberwachtes Lernen, Reinforcement Lernen. Dabei stellt
der Lehrer beim überwachten Lernen Trainingsbeispiele plus eine Vorgabe (Klasse, Funktionswert)
zur Verfügung, während beim unüberwachten Lernen nur die Trainingsbeispiele bereitgestellt
werden und der Algorithmus selbst Zusammenhänge in den Daten erkennen soll. Beim Reinforcement
Learning erfolgt das Feedback am Ende einer Kette von Aktionen, d.h. der Algorithmus muss
diese Bewertung auf die einzelnen Aktionen zurückrechnen.</p>
<p>Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden)
Funktion möglichst nahe kommt. Eine konsistente Hypothese erklärt die Trainingsdaten, eine
generalisierende Hypothese kann auch unbekannte Daten (die aus der selben Quelle stammen, also
zum selben Problem gehören) korrekt bewerten. Es wird unterschieden zwischen Klassifikation
(einige wenige diskrete Label/Klassen, die den Trainingsbeispielen zugeordnet sind) und
Regression (Lernen eines Funktionsverlaufs).</p>
<p>Merkmalsvektoren gruppieren Eigenschaften des Problems bzw. der Objekte, d.h. jedes Objekt
kann über einen Merkmalsvektor beschrieben werden. Trainingsdaten sind ausgewählte Beispielobjekte
(durch Merkmalsvektoren beschrieben) plus die Vorgabe (Klasse oder Funktionswert) vom Lehrer.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/FliWEXQZhsw' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Machine Learning 101</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/c871a589c1e95782173b9a3c1efbb79ac38dfb4d871df8b39fd5851562036619df4d6b31cec0b630c1c88ff928bbcb2429ea194e2cc795720fce214367009242' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Machine Learning 101</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K1) Definition und Arten des Lernens</li> <li>(K2) Überwachtes Lernen: Lernen durch Beobachten (mit Lehrer)</li> <li>(K2) Merkmalsvektoren, Eigenschaften, Ausprägung, Objekte, Trainingsmenge</li></ul>
  </div>
</div>




    <h2 id="was-ist-lernen">Was ist Lernen?</h2>
<blockquote>
<p>Verhaltensänderung eines Agenten in Richtung der Optimierung eines
Gütefunktionals (Bewertungsfunktion) durch Erfahrung.</p>
</blockquote>
<h2 id="warum-lernen">Warum Lernen?</h2>
<ul>
<li>Nicht alle Situationen vorhersehbar</li>
<li>Nicht alle Details modellierbar</li>
<li>Lösung oder Lösungsweg unbekannt, nicht explizit programmierbar</li>
<li>Data Mining: Entdeckung neuen Wissens durch Analyse der Daten</li>
<li>Selbstanpassende Programme</li>
</ul>
<p>=&gt; Lernen wichtige Eigenschaft lebender Wesen :-)</p>
<h2 id="learning-agent">Learning Agent</h2>
<p><a href="#R-image-082c7ca01e066e40bb5f192e6d48d8e8" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/learning.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-082c7ca01e066e40bb5f192e6d48d8e8"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/learning.png?width=80%25&height=auto"></a></p>
<h2 id="feedback-während-des-lernens">Feedback während des Lernens</h2>
<ul>
<li>
<p><strong>Überwachtes Lernen</strong></p>
<ul>
<li>Lernen durch Beobachtung</li>
<li>Vorgabe von Beispielen: Ein- und Ausgabewerte</li>
</ul>
<p>=&gt; Regression, <span class='alert'>Klassifikation</span></p>
</li>
<li>
<p><strong>Unüberwachtes Lernen</strong></p>
<ul>
<li>Erkennen von Mustern in den Inputdaten, Clustering</li>
<li>Kein Feedback (!)</li>
</ul>
</li>
<li>
<p><strong>Reinforcement Lernen</strong></p>
<ul>
<li>Bewertung der Aktionen des Agenten am Ende einer Aktionsfolge</li>
</ul>
</li>
</ul>
<p><strong>Beispiel Kleinkind</strong>: Lernen von Klassen/Konzepten durch Beispiele</p>
<ul>
<li>Zuerst ist alles &quot;Katze&quot; (Übergeneralisierung)</li>
<li>Differenzierung durch Feedback der Umwelt; Erkennung unterschiedlicher Ausprägungen</li>
</ul>
<h2 id="beispiel-kreditrisiko">Beispiel: Kreditrisiko</h2>
<ul>
<li>
<p>Bankkunde beantragt Kredit</p>
</li>
<li>
<p>Soll er aus Sicht der Bank den Kredit bekommen?</p>
</li>
<li>
<p>Bankangestellter betrachtet (relevante) Merkmale des Kunden:</p>
<ul>
<li>Alter, Einkommen, sozialer Status</li>
<li>Kundenhistorie bei der Bank</li>
<li>Höhe des Kredits</li>
</ul>
</li>
<li>
<p>Bewertung des Kreditrisikos:</p>
<ul>
<li><strong>Klassifikation</strong>: Guter oder schlechter Kunde (Binäre Entscheidung: 2 Klassen)</li>
<li><strong>Regression</strong>: Vorhersage Gewinn/Verlust für die Bank (Höhe des Gewinns/Verlusts interessant)</li>
</ul>
</li>
</ul>
<h2 id="beispiel-autoreparatur">Beispiel: Autoreparatur</h2>
<ul>
<li>
<p><strong>Gegeben</strong>: Eigenschaften eines Autos</p>
<p>=&gt; Eigenschaften: Ausprägungen der Merkmale</p>
</li>
<li>
<p><strong>Gesucht</strong>: Diagnose und Reparaturanleitung</p>
<p>=&gt; Hypothese über den Merkmalen (Funktion <span class="math align-center">$\operatorname{h}$</span>)</p>
</li>
</ul>
<h2 id="lernen-durch-beobachten-lernen-einer-funktion-hahahugoshortcode11s1hbhb">Lernen durch Beobachten: Lernen einer Funktion <span class="math align-center">$\operatorname{f}$</span></h2>
<p>Funktionsapproximation: Lernen einer Funktion <span class="math align-center">$\operatorname{f}$</span> anhand von Beispielen</p>
<ul>
<li>
<p>Ein Beispiel ist ein Tupel <span class="math align-center">$(\mathbf{x}, \operatorname{f}(\mathbf{x}))$</span>, etwa
<span class="math align-center">$$
    (\mathbf{x}, \operatorname{f}(\mathbf{x})) = \left(\begin{array}{ccc}
    O & O & X \\
    . & X & . \\
    X & . & .
    \end{array}, +1\right)
    $$</span></p>
</li>
<li>
<p>Aufgabe: Baue Hypothese <span class="math align-center">$\operatorname{h}$</span> auf, so dass <span class="math align-center">$\operatorname{h} \approx \operatorname{f}$</span>.</p>
<ul>
<li>Benutze dazu Menge von Beispielen =&gt; <span class='alert'><strong>Trainingsdaten</strong></span>.</li>
</ul>
</li>
<li>
<p>Ziele:</p>
<ol>
<li><strong>Konsistente Hypothese</strong>: Übereinstimmung bei Trainingsdaten</li>
<li><strong>Generalisierende Hypothese</strong>: Korrekte Vorhersage bei
unbekannten Daten</li>
</ol>
</li>
</ul>
<p><em>Anmerkung</em>: Stark vereinfachtes Modell realen Lernens!</p>
<h2 id="konstruieren-einer-konsistenten-hypothese">Konstruieren einer konsistenten Hypothese</h2>
<p><a href="#R-image-584a2037a65d4722554ab77ee7bfc3d1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams1.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-584a2037a65d4722554ab77ee7bfc3d1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams1.png?width=60%25&height=auto"></a></p>
<p>Welcher Zusammenhang ist hier dargestellt? Offenbar eine Art Funktionsverlauf ...
Wir haben für einige x-Werte die zugehörigen y-Werte vorgegeben.</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-653581bb61aacf184839f27aa50e1248" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams2.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-653581bb61aacf184839f27aa50e1248"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams2.png?width=60%25&height=auto"></a></p>
<p>Die einfachste Approximation wäre eine lineare Funktion. Allerdings werden hierbei
einige Werte mehr oder weniger stark nicht korrekt widergegeben, d.h. man hat einen
relativ hohen (Trainings-) Fehler.</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt-1">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-f0ee7a7e616e9eb2a1516292b46a18e1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams3.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f0ee7a7e616e9eb2a1516292b46a18e1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams3.png?width=60%25&height=auto"></a></p>
<p>Die Hyperbel erklärt die Trainingsdaten bis auf den einen Punkt sehr gut.
Die Frage ist, ob dieser eine Punkt zum zu lernenden Zusammenhang gehört
oder ein Ausreißer ist, den man gefahrlos ignorieren kann?</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt-2">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-de83e68a19d2a3cef14469df05ab7536" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams4.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-de83e68a19d2a3cef14469df05ab7536"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams4.png?width=60%25&height=auto"></a></p>
<p>Die grüne Hypothese ist von allen bisher gezeigten die komplexeste, erklärt
aber alle Datenpunkte. D.h. hier wäre der Trainingsfehler Null. Zwischen den
Trainingsdaten zeigt das Modell eine &quot;glatte&quot; Approximation, d.h. es wird auch
neue Daten, die es beim Training nicht gesehen hat, relativ gut erklären.
(Dabei liegt freilich die Annahme zugrunde, dass alle relevanten Daten in der
Trainingsmenge vorhanden sind, d.h. dass es insbesondere zwischen den Datenpunkten
keine Ausreißer o.ä. gibt.)</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt-3">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-e1156df8090325306925a290899f72c1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams5.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e1156df8090325306925a290899f72c1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams5.png?width=60%25&height=auto"></a></p>
<p>Diese Hypothese erklärt ebenfalls sämtliche Trainingsdaten. Allerdings schwingt
die Funktion zwischen den Daten stark hin und her. Vermutlich entspricht dies
nicht dem zu lernenden Funktionsverlauf. Der Trainingsfehler wäre wie bei der
deutlich einfacheren Hypthese aus dem letzten Schritt Null. Der Generalisierungsfehler
(sprich die Abweichung, wenn man das Modell nach Daten zwischen den Trainingspunkten
fragt) dürfte erheblich höher liegen.</p>
<p>D.h. hier hat das Modell einfach die Trainingsdaten auswendig gelernt, aber nicht
den Zusammenhang zwischen den Daten! Dies ist in der Regel unerwünscht!</p>
<h2 id="occams-razor">Occam's Razor</h2>
<p><strong>Bevorzuge die einfachste konsistente Hypothese!</strong></p>
<ol>
<li>Wenn es mehrere mögliche Erklärungen für einen Sachverhalt gibt, ist die
einfachste Erklärung allen anderen vorzuziehen.</li>
<li>Eine Erklärung ist &quot;einfach&quot;, wenn sie möglichst wenige Variablen und
Annahmen enthält und wenn diese in klaren logischen Beziehungen zueinander
stehen, aus denen der zu erklärende Sachverhalt logisch folgt.</li>
</ol>
<h2 id="trainingsdaten-und-merkmalsvektoren">Trainingsdaten und Merkmalsvektoren</h2>
<p>Lehrer gibt Beispiele vor: Eingabe <span class="math align-center">$\mathbf{x}$</span> und passende Ausgabe <span class="math align-center">$\operatorname{f}(\mathbf{x})$</span></p>
<ul>
<li>
<p>Ausgabe: typischerweise Skalar (Funktionswert oder Klasse)
=&gt; Beispiel: Bewertung eines Spielstandes bei TicTacToe</p>
</li>
<li>
<p>Eingabe: (Beschreibung des) Objekt(s) oder Situation, die zur Ausgabe gehört
=&gt; Beispiel: Spielstand bei TicTacToe</p>
</li>
</ul>
<p><strong>Merkmalsvektoren</strong>:</p>
<ul>
<li>Zusammenfassen der <span class='alert'>relevanten Merkmale</span> zu <span class='alert'>Vektoren</span></li>
</ul>
<h2 id="beispiel-schwimmen-im-see">Beispiel: Schwimmen im See</h2>
<p>Beschreibung der Faktoren, wann ich im See schwimmen möchte:</p>
<ol>
<li>Scheint die Sonne?</li>
<li>Wie warm ist das Wasser?</li>
<li>Wie warm ist die Luft?</li>
</ol>
<ul>
<li>Trainingsbeispiel:
<ul>
<li>Eingabe: Merkmalsvektor <code>(sonnig, warm, warm)</code></li>
<li>Ausgabe: Klasse <code>ja</code></li>
</ul>
</li>
</ul>
<p>Dabei wird davon ausgegangen, dass jeder Faktor (jedes Merkmal) an einer bestimmten
Stelle im Merkmalsvektor aufgeführt ist. Beispielsweise gehört das <code>sonnig</code> zur
Frage &quot;Scheint die Sonne&quot;, <code>warm</code> jeweils zur Wasser- und zur Lufttemperatur.</p>
<p>Damit hat man in einem Vektor eine Situation komplett beschrieben, d.h. einen Zustand
der Welt mit den relevanten Dingen beschrieben. Diesem Zustand kann man beispielsweise
ein Label (Klasse) verpassen, hier in diesem Fall &quot;ja, in dieser Welt möchte ich
schwimmen&quot;.</p>
<p>Die Trainingsmenge baut sich dann beim überwachten Lernen aus vielen solcher Paare
(Merkmalsvektor, Klasse) auf, und die Algorithmen sollen diese Zuordnung lernen, d.h.
ein Modell für diese Daten erzeugen, welches die Daten gut erklärt und darüber hinaus
für neue Daten aus der selben Datenquelle gute Vorhersagen macht.</p>
<h2 id="trainingsdaten----merkmalsvektoren">Trainingsdaten -- Merkmalsvektoren</h2>
<p><strong>Generell</strong>: Merkmalsvektor für Objekt <span class="math align-center">$v$</span>:
<span class="math align-center">$$
    \mathbf{x}(v) = (x_1, x_2, \ldots, x_n)
$$</span></p>
<ul>
<li><span class="math align-center">$n$</span> Merkmale (Attribute)</li>
<li>Attribut <span class="math align-center">$x_t$</span> hat <span class="math align-center">$m_t$</span> mögliche Ausprägungen</li>
<li>Ausprägung von <span class="math align-center">$v$</span> bzgl. <span class="math align-center">$x_t$</span>: <span class="math align-center">$\quad x_t(v) = i \quad$</span> (mit <span class="math align-center">$i = 1 \ldots m_t$</span>)</li>
</ul>
<p><em>Anmerkung</em>: Stellen Sie sich den Merkmalsvektor vielleicht wie
einen Konstruktor einer Klasse <code>x</code> vor: Die einzelnen Attribute <span class="math align-center">$x_t$</span> sind
die Parameter, aus denen der Merkmalsvektor aufgebaut ist/wird. Jedes der
Attribute hat einen Typ und damit eine bestimmte Anzahl erlaubter Werte
(&quot;Ausprägungen&quot;) ...</p>
<p><strong>Trainingsbeispiel</strong>:</p>
<ul>
<li>Tupel aus Merkmalsvektor und zugehöriger Klasse: <span class="math align-center">$\left(\mathbf{x}(v), k\right)$</span></li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>
<p>Lernen ist Verhaltensänderung, Ziel: Optimierung einer Gütefunktion</p>
<ul>
<li>Aufbau einer Hypothese, die beobachtete Daten erklären soll</li>
<li>Arten: Überwachtes Lernen, Unüberwachtes Lernen, Reinforcement Lernen</li>
</ul>
</li>
<li>
<p>Merkmalsvektoren gruppieren Eigenschaften des Problems bzw. der Objekte</p>
</li>
<li>
<p>Trainingsdaten: Beispielobjekte (durch Merkmalsvektoren beschrieben) plus Vorgabe vom Lehrer</p>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106589&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Intro ML (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Modellierung</strong></p>
<p>Sie stehen vor der Entscheidung, ob Sie sich zur Vorbereitung auf die
Flipped-Classroom-Sitzung noch das Skript anschauen. Welche Attribute
benötigen Sie, um die Situation zu beschreiben?</p>
<p><strong>Metriken für Klassifikatoren</strong></p>
<p>Es ist wieder Wahlkampf: Zwei Kandidaten O und M bewerben sich um die Kanzlerschaft. Die folgende Tabelle zeigt die Präferenzen von sieben Wählern.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left">Alter</th>
          <th style="text-align: left">Einkommen</th>
          <th style="text-align: left">Bildung</th>
          <th style="text-align: left">Kandidat</th>
          <th style="text-align: left">Vorhersage</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Abitur</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">O</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left"><span class="math align-center">$< 35$</span></td>
          <td style="text-align: left">niedrig</td>
          <td style="text-align: left">Master</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">O</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Bachelor</td>
          <td style="text-align: left">M</td>
          <td style="text-align: left">M</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">niedrig</td>
          <td style="text-align: left">Abitur</td>
          <td style="text-align: left">M</td>
          <td style="text-align: left">M</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Master</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">O</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left"><span class="math align-center">$< 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Bachelor</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">M</td>
      </tr>
      <tr>
          <td style="text-align: left">7</td>
          <td style="text-align: left"><span class="math align-center">$< 35$</span></td>
          <td style="text-align: left">niedrig</td>
          <td style="text-align: left">Abitur</td>
          <td style="text-align: left">M</td>
          <td style="text-align: left">O</td>
      </tr>
  </tbody>
</table>
<p>Auf diesem Datensatz wurde ein Klassifikator trainiert, die Trainingsergebnisse sind in der Tabelle unter &quot;Vorhersage&quot; angegeben.</p>
<p>Bewerten Sie den Klassifikator.</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Lernen: Abschnitte 19.1 und 19.2</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>CAL2</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden
dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal
bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.</p>
<p>Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden
Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten.
Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die
Klasse.</p>
<p>Den Baum kann man mit dem Algorithmus CAL2 schrittweise aufbauen. Man startet mit &quot;Nichtwissen&quot; (symbolisiert
mit einem &quot;*&quot;) und iteriert durch alle Trainingsbeispiele, bis der Baum sich nicht mehr verändert. Wenn
der Baum auf ein Beispiel einen &quot;*&quot; ausgibt, dann ersetzt man diesen &quot;*&quot; mit der Klasse des eben betrachteten
Beispiels. Wenn der Baum bei einem Beispiel die passende Klasse ausgibt, macht man mit dem nächsten Beispiel
weiter. Wenn der Baum bei einem Beispiel eine andere Klasse ausgibt, muss das Klassensymbol im Baum (an
der Stelle, wo das Objekt gelandet ist) durch den nächsten Test ersetzt werden: Hierzu nimmt man das nächste,
auf diesem konkreten Pfad noch nicht verwendete Merkmal. CAL2 kann nur mit diskreten Attributen und disjunkten
Klassen einen fehlerfreien Baum erzeugen.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/bR_QVYtPRx8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL2</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/66bfcc3ba546355d5a4d41394912380e4641fc8498e8f257a98c602c11dd6ff33eb7f2ddb4fdebd433be30e9fcf91f99aaf1a484b46d2f34feac63a6e777a177' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL2</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Entscheidungsbaumlerner CAL2</li></ul>
  </div>
</div>




    <h2 id="entscheidungsbäume-klassifikation">Entscheidungsbäume: Klassifikation</h2>
<p><a href="#R-image-fdf278f6f66b1fa5e8fc7de2d9fb7123" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2/xor-decision-tree.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fdf278f6f66b1fa5e8fc7de2d9fb7123"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2/xor-decision-tree.png?width=80%25&height=auto"></a></p>
<ul>
<li>Attribute als Knoten im Baum</li>
<li>Ausprägungen als Test (Ausgang, Verzweigung)</li>
<li>Klasse (Funktionswert) als Blatt</li>
</ul>
<p>Erinnern Sie sich an das Beispiel mit der Auto-Reparatur aus der letzten Sitzung.</p>
<p>Die relevanten Eigenschaften (Merkmale) eines Autos würden als Knoten im Baum
repräsentiert. Beispiel: &quot;Motor startet&quot; oder &quot;Farbe&quot;.</p>
<p>Jedes Merkmal hat eine Anzahl von möglichen Ausprägungen, diese entsprechen den
Verzweigungen am Knoten. Beispiel: &quot;startet&quot;, &quot;startet nicht&quot; oder &quot;rot&quot;, &quot;weiß&quot;, &quot;silber&quot;, ... .</p>
<p>Entsprechend kann man durch Abarbeiten des Entscheidungsbaumes am Ende zu einer
Diagnose gelangen (Klasse).</p>
<p>Eine andere Sichtweise ist die Nutzung als Checkliste für eine Reparatur ...</p>
<h2 id="definition-entscheidungsbaum">Definition Entscheidungsbaum</h2>
<ul>
<li>
<p>Erinnerung: <strong>Merkmalsvektor</strong> für Objekt <span class="math align-center">$v$</span>:
<span class="math align-center">$$
        \mathbf{x}(v) = (x_1, x_2, \ldots, x_n)
    $$</span></p>
<ul>
<li><span class="math align-center">$n$</span> Merkmale (Attribute)</li>
<li>Attribut <span class="math align-center">$x_t$</span> hat <span class="math align-center">$m_t$</span> mögliche Ausprägungen</li>
<li>Ausprägung von <span class="math align-center">$v$</span> bzgl. <span class="math align-center">$x_t$</span>: <span class="math align-center">$\quad x_t(v) = i \quad$</span> (mit <span class="math align-center">$i = 1 \ldots m_t$</span>)</li>
</ul>
</li>
<li>
<p><strong>Alphabet</strong> für Baum:
<span class="math align-center">$$
        \lbrace x_t | t=1,\ldots,n \rbrace \cup \lbrace \kappa | \kappa = \ast,A,B,C,\ldots \rbrace \cup \lbrace (,) \rbrace
    $$</span></p>
</li>
<li>
<p><strong>Entscheidungsbaum</strong> <span class="math align-center">$\alpha$</span>:
<span class="math align-center">$$
        \alpha = \left\lbrace  \begin{array}{ll}
            \kappa  & \text{Terminalsymbole: } \kappa = \ast,A,B, \ldots\\
            x_t(\alpha_1, \alpha_2, \ldots, \alpha_{m_t}) & x_t \text{ Testattribut mit } m_t \text{ Ausprägungen}
        \end{array}\right.
    $$</span></p>
</li>
</ul>
<p><em>Anmerkung</em>: Stellen Sie sich die linearisierte Schreibweise wieder
wie den (verschachtelten) Aufruf von Konstruktoren vor. Es gibt die
Oberklasse <code>Baum</code>, von der für jedes Attribut eine Klasse abgeleitet
wird. D.h. der Konstruktor für eine Attributklasse erzeugt letztlich
ein Objekt vom Obertyp <code>Baum</code>. Außerdem sind die Terminalsymbole <code>A</code>,
<code>B</code>, ... Objekte vom Typ <code>Blatt</code>, welches eine Unterklasse von <code>Baum</code>
ist ...</p>
<p>Dabei wird die Anzahl der möglichen Ausprägungen für ein Attribut
berücksichtigt: Jede Ausprägung hat einen Parameter im Konstruktor.
Damit werden die Unterbäume beim Erzeugen des Knotens übergeben.</p>
<h2 id="induktion-von-entscheidungsbäumen-cal2">Induktion von Entscheidungsbäumen: CAL2</h2>
<ol>
<li>
<p>Anfangsschritt: <span class="math align-center">$\alpha^{(0)} = \ast$</span> (totales Unwissen)</p>
</li>
<li>
<p><span class="math align-center">$n$</span>-ter Lernschritt: Objekt <span class="math align-center">$v$</span> mit Klasse <span class="math align-center">$k$</span>, Baum <span class="math align-center">$\alpha^{(n-1)}$</span>
gibt <span class="math align-center">$\kappa$</span> aus</p>
<ul>
<li><span class="math align-center">$\kappa = \ast$</span>: ersetze <span class="math align-center">$\ast$</span> durch <span class="math align-center">$k$</span></li>
<li><span class="math align-center">$\kappa = k$</span>: keine Aktion nötig</li>
<li><span class="math align-center">$\kappa \neq k$</span>: Fehler
<ul>
<li>Ersetze <span class="math align-center">$\kappa$</span> mit neuem Test: <span class="math align-center">$\kappa \gets x_{t+1}(\ast, \ldots, \ast, k, \ast, \ldots, \ast)$</span></li>
<li><span class="math align-center">$x_{t+1}$</span>: nächstes Attribut, auf dem aktuellen Pfad noch nicht verwendet</li>
<li>Symbol <span class="math align-center">$k$</span> an Position <span class="math align-center">$i$</span> wenn <span class="math align-center">$x_{t+1}(v) = i$</span></li>
</ul>
</li>
</ul>
</li>
</ol>
<p><span class="math align-center">$\alpha^{(n)}$</span> bezeichnet den Baum im <span class="math align-center">$n$</span>-ten Lernschritt.</p>
<p>CAL2 ist ein <strong>Meta-Algorithmus</strong>: Es ist ein Algorithmus, um einen Algorithmus
zu lernen :-)</p>
<h2 id="beispiel-mit-cal2">Beispiel mit CAL2</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">4</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">3</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
<p><strong>Ergebnis</strong>: <span class="math align-center">$x_1(x_2(A, B), x_2(A, B))$</span></p>
<p><em>Anmerkung</em>: Denken Sie an die Analogie von oben. <span class="math align-center">$x_1$</span> kann als
Konstruktor einer Klasse <code>x1</code> betrachtet werden, die eine Unterklasse
von <code>Baum</code> ist. Durch den Aufruf des Konstruktors wird als ein <code>Baum</code>
erzeugt.</p>
<p>Es gibt in <span class="math align-center">$x_1$</span> zwei mögliche Ausprägungen, d.h. der Baum hat in
diesem Knoten zwei alternative Ausgänge. Diese Unterbäume werden
dem Konstruktor von <code>x1</code> direkt beim Aufruf übergeben (müssen also
Referenzen vom Typ <code>Baum</code> sein).</p>
<h2 id="cal2-bemerkungen">CAL2: Bemerkungen</h2>
<ul>
<li>
<p>Nur für diskrete Merkmale und disjunkte Klassen</p>
</li>
<li>
<p>Zyklischer Durchlauf durch Trainingsmenge</p>
</li>
<li>
<p>Abbruch:</p>
<ul>
<li>Alle Trainingsobjekte richtig klassifiziert
=&gt; Kein Fehler in einem kompletten Durchlauf</li>
<li>(Differenzierung nötig, aber alle Merkmale verbraucht)</li>
<li>(Lernschrittzahl überschritten)</li>
</ul>
</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Darstellung der Hypothese als Entscheidungsbaum</li>
<li>CAL2: diskrete Attribute, disjunkte Klassen</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106575&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest CAL2 (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Modellierung</strong></p>
<p>Sie stehen vor der Entscheidung, ob Sie sich zur Vorbereitung auf die
Flipped-Classroom-Sitzung noch das Skript anschauen.</p>
<p>Zeichnen Sie einen Entscheidungsbaum, der Ihnen bei der Entscheidung hilft.</p>
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit CAL2.</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
<p><strong>Handsimulation CAL2</strong></p>
<p>Zeigen Sie mit einer Handsimulation, wie CAL2 mit dem folgenden
Trainingsdatensatz schrittweise einen Entscheidungsbaum generiert.
Nutzen Sie die linearisierte Schreibweise.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Beispiel</th>
          <th style="text-align: center"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: center"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: center"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: center">Klasse</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">1</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">2</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">2</td>
      </tr>
      <tr>
          <td style="text-align: center">3</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">4</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">5</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">c</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">6</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">2</td>
      </tr>
  </tbody>
</table>
<p>Welchen Entscheidungsbaum würde CAL2 lernen, wenn dem Trainingsdatensatz
der Vektor <span class="math align-center">$((a,a,b), 2)$</span> als Beispiel Nr. 7 hinzugefügt werden würde?</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Unger1981'>[Unger1981] <strong>Lernfähige Klassifizierungssysteme (Classifier Systems Which Are Able to Learn)</strong><br>Unger, S. und Wysotzki, F., Akademie-Verlag, 1981.<br><em>Der Vollständigkeit halber aufgeführt (Werk ist leider vergriffen und wird nicht mehr verlegt)</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Pruning</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Pruning ist das Entfernen redundanter und irrelevanter Tests (Merkmale).</p>
<p>Irrelevante Merkmale spielen keine Rolle bei der Klassifikation, an jedem Ausgang
eines irrelevanten Merkmals findet sich exakt der selbe Baum. Diese Tests kann man
einfach entfernen und durch einen ihrer Teilbäume ersetzen; dadurch ändert sich
nicht die Klassifikation des Baumes.</p>
<p>Bei redundanten Tests sind alle Ausgänge bis auf einen noch mit &quot;Nichtwissen&quot; (&quot;*&quot;)
markiert. Hier kann man den Test durch den einen bekannten Ausgang ersetzen, wodurch
sich die Klassifikation ändert. Allerdings wird der Klassifikationsfehler nicht größer,
da man ja vorher nur für eine Ausprägung des redundanten Merkmals einen Baum hatte und
für die anderen jeweils mit &quot;*&quot; antworten musste (d.h. hier stets einen Fehler gemacht
hatte).</p>
<p>Über die Transformationsregel kann man einfach die Reihenfolge von Tests im Baum ändern.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/LKt9F2kGYdk' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Pruning</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/badf517191aa92377bf6ca9e63f90e8083d64de43f85b230b336cbf2b56e805d45063cf0974a6292ee39cf010aef11e87d3cf7ff9c9bd7e7c0a64f61128504e2' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Pruning</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Pruning: Entfernen bedingt irrelevanter Tests</li> <li>(K3) Pruning: Entfernen bedingt redundanter Tests</li> <li>(K3) Umformen von Entscheidungsbäumen mit Transformationsregel</li></ul>
  </div>
</div>




    <h2 id="pruning-bedingt-irrelevante-attribute">Pruning: Bedingt irrelevante Attribute</h2>
<p><strong>Baum</strong>: <span class="math align-center">$\alpha = x_1(x_2(A, B),  x_2(A, B),  x_2(A, B))$</span></p>
<p><span class="math align-center">$x_1$</span> ist <span class='alert'>bedingt irrelevant</span>
=&gt; Vereinfachung: <span class="math align-center">$\alpha = x_2(A, B)$</span></p>
<p><strong>Allgemein</strong>:</p>
<ul>
<li>Sei <span class="math align-center">$\tilde{x}$</span> Weg zu Nichtendknoten <span class="math align-center">$x_t$</span></li>
<li>Baum dort <span class="math align-center">$\alpha/\tilde{x} = x_t(\alpha_1, \ldots, \alpha_{m_t})$</span></li>
<li><span class="math align-center">$x_t$</span> ist <span class='alert'><strong>bedingt irrelevant</strong></span> unter der Bedingung
<span class="math align-center">$\tilde{x}$</span>, wenn <span class="math align-center">$\alpha_1 = \alpha_2 = \ldots = \alpha_{m_t}$</span></li>
<li><strong>Vereinfachung</strong>: Ersetze in <span class="math align-center">$\alpha/\tilde{x}$</span> den Test <span class="math align-center">$x_t$</span> durch <span class="math align-center">$\alpha_1$</span></li>
</ul>
<p><em>Anmerkung</em>:
Der durch das Entfernen von bedingt irrelevanten Attributen entstandene Baum
hat <strong>exakt</strong> die selbe Aussage (Klassifikation) wie der Baum vor dem Pruning.</p>
<p><strong>Anmerkung</strong>:
<span class="math align-center">$x_1$</span> im obigen Beispiel ist sogar <span class='alert'><strong>global</strong> irrelevant</span>, da es sich hier
um die Wurzel des Baumes handelt. Der Weg <span class="math align-center">$\tilde{x}$</span> ist in diesem Fall der leere
Weg ...</p>
<h2 id="pruning-bedingt-redundante-attribute">Pruning: Bedingt redundante Attribute</h2>
<p><strong>Baum</strong>: <span class="math align-center">$\alpha = x_1(\ast,  \ast,  x_2(A, B))$</span></p>
<p><span class="math align-center">$x_1$</span> ist <span class='alert'>bedingt redundant</span>
=&gt; Vereinfachung: <span class="math align-center">$\alpha = x_2(A, B)$</span></p>
<p><strong>Allgemein</strong>:</p>
<ul>
<li>Sei <span class="math align-center">$\tilde{x}$</span> Weg zu Nichtendknoten <span class="math align-center">$x_t$</span></li>
<li>Baum dort <span class="math align-center">$\alpha/\tilde{x} = x_t(\ast, \ldots, \ast, \alpha_i, \ast, \ldots, \ast)$</span> (mit <span class="math align-center">$\alpha_i \neq \ast$</span>)</li>
<li><span class="math align-center">$x_t$</span> ist <span class='alert'><strong>bedingt redundant</strong></span> unter der Bedingung <span class="math align-center">$\tilde{x}$</span></li>
<li><strong>Vereinfachung</strong>: Ersetze in <span class="math align-center">$\alpha/\tilde{x}$</span> den Test <span class="math align-center">$x_t$</span> durch <span class="math align-center">$\alpha_i$</span></li>
</ul>
<p><em>Anmerkung</em>:
Der durch das Entfernen von bedingt redundanten Attributen entstandene Baum
hat eine etwas andere Klassifikation als der Baum vor dem Pruning. Wo vorher
ein <code>*</code> ausgegeben wurde, wird nach dem Pruning u.U. ein Klassensymbol
ausgegeben. Der Klassifikationsfehler erhöht sich aber <strong>nicht</strong>, da hier ein
<code>*</code> wie ein falsches Klassensymbol zu werten ist.</p>
<p><strong>Anmerkung</strong>:
<span class="math align-center">$x_1$</span> im obigen Beispiel ist sogar <span class='alert'><strong>global</strong> redundant</span>, da es sich
hier um die Wurzel des Baumes handelt. Der Weg <span class="math align-center">$\tilde{x}$</span> ist in diesem Fall
der leere Weg ...</p>
<h2 id="allgemeine-transformationsregel">Allgemeine Transformationsregel</h2>
<span class="math align-center">$$
    x_1(x_2(a, b),  x_2(c, d))  \Leftrightarrow  x_2(x_1(a, c),  x_1(b, d))
$$</span>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Pruning: Entfernen bedingt redundanter und irrelevanter Tests</li>
<li>Transformationsregel zum Umbauen von Entscheidungsbäumen</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106577&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Pruning (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Entscheidungsbäume: Abschnitt 8.4</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Entscheidungsbäume: Abschnitt 19.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>CAL3</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>
CAL3 ist eine einfache Erweiterung von CAL2 für nicht-disjunkte (überlappende) Klassen. Statt
beim Baumaufbau bei einer Fehlklassifikation sofort zu verzweigen, werden hier zunächst die
im entsprechenden Pfad aufgelaufenen Klassensymbole gezählt. Wenn ausreichend viele davon
gesehen wurden (Schwelle <span class="math align-center">$S_1$</span>), wird eine Entscheidung getroffen: Wenn eine Klasse in diesem
temporären Blatt dominiert (ihre Häufigkeit über einer Schwelle <span class="math align-center">$S_2$</span> liegt), dann entscheidet
man sich in diesem Blatt fest für diese Klasse. Ansonsten (die Häufigkeit aller Klassen in
dem Blatt liegt unter <span class="math align-center">$S_2$</span>) nimmt man analog zu CAL2 den nächsten, auf diesem Pfad noch nicht
verwendeten Test hinzu.
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/9Wj51XvuntM' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL3</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/6acb93574f25ff341b5a09487fc153ea28252e12d3960342bc7d05a463e56b338f53f366338229df44f5c486400465fddf58e727fd8f9cc56904dd67c7c8ecb8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL3</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Meta-Algorithmus CAL3 für überlappende Klassen</li></ul>
  </div>
</div>




    <h2 id="cal3-erweiterung-von-cal2-für-nicht-disjunkte-klassen">CAL3: Erweiterung von CAL2 für nicht-disjunkte Klassen</h2>
<ol>
<li>
<p>Anfangsschritt: <span class="math align-center">$\alpha^{(0)} = \ast$</span> (totales Unwissen)</p>
</li>
<li>
<p><span class="math align-center">$n$</span>-ter Lernschritt: Objekt <span class="math align-center">$v$</span> mit Klasse <span class="math align-center">$k$</span></p>
<ul>
<li>
<p>Rückweisung (Endknoten mit <span class="math align-center">$\ast$</span>):
Ersetze <span class="math align-center">$\ast$</span> durch Vereinigungsklasse <span class="math align-center">$/k1/$</span></p>
</li>
<li>
<p>Endknoten mit Vereinigungsklasse:</p>
<ul>
<li>Zähler für <span class="math align-center">$k$</span> erhöhen, bzw.</li>
<li><span class="math align-center">$k$</span> mit Anzahl <span class="math align-center">$1$</span> in Vereinigungsklasse einfügen</li>
</ul>
</li>
</ul>
<p>Falls nun die Summe aller Klassen am Endknoten größer/gleich <span class="math align-center">$S_1$</span> (Statistikschwelle):</p>
<ul>
<li>
<p>Für <strong>genau eine</strong> Klasse gilt: <span class="math align-center">$P(k | \tilde{x}) \ge S_2$</span>:
=&gt; Abschluss: Ersetze Vereinigungsklasse durch <span class="math align-center">$k$</span> (für immer!)</p>
</li>
<li>
<p>Für <strong>alle</strong> Klassen gilt: <span class="math align-center">$P(k | \tilde{x}) < S_2$</span>:
=&gt; Differenzierung: Ersetze Vereinigungsklasse durch neuen
Test: <span class="math align-center">$\kappa \gets x_{t+1}(\ast, \ldots, \ast, /k1/, \ast, \ldots, \ast)$</span></p>
<p><span class="math align-center">$x_{t+1}$</span>: nächstes Attribut, auf dem aktuellen Pfad <span class="math align-center">$\tilde{x}$</span>
noch nicht verwendet
Symbol <span class="math align-center">$k$</span> mit Anzahl 1 an Position <span class="math align-center">$i$</span> wenn <span class="math align-center">$x_{t+1}(v) = i$</span></p>
</li>
</ul>
</li>
</ol>
<h2 id="beispiel-mit-cal3">Beispiel mit CAL3</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
<ul>
<li><span class="math align-center">$S_1 = 4, S_2 = 0.7$</span></li>
</ul>
<p><strong>Ergebnis</strong>: <span class="math align-center">$x_1(A,  x_2(B, A))$</span></p>
<p>Trainingsfehler: <span class="math align-center">$1/5 = 0.2 < 1-S_2 = 1-0.7 = 0.3$</span></p>
<p><strong>Hinweis</strong>: Bei nicht überlappenden Klassen erzeugt CAL3 u.U. andere Bäume als CAL2 ...</p>
<h2 id="cal3-abbruchbedingungen-und-parameter">CAL3: Abbruchbedingungen und Parameter</h2>
<ul>
<li>
<p><strong>Parameter</strong>:</p>
<ul>
<li><span class="math align-center">$S_1$</span>: Statistikschwelle, problemabhängig wählen</li>
<li><span class="math align-center">$S_2$</span>: <span class="math align-center">$0.5 < S_2 \le 1.0$</span></li>
<li>Klassifikationsfehler kleiner als <span class="math align-center">$1-S_2$</span>
<ul>
<li>kleiner Fehler =&gt; großer Baum</li>
<li>großer Fehler =&gt; kleiner Baum</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Abbruch</strong>:</p>
<ul>
<li>Alle Trainingsobjekte richtig klassifiziert
=&gt; Kein Fehler in einem kompletten Durchlauf</li>
<li>Alle Endknoten mit eindeutigen Klassensymbolen belegt</li>
<li>Differenzierung nötig, aber alle Merkmale verbraucht</li>
<li>Lernschrittzahl überschritten</li>
</ul>
</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>CAL3: Erweiterung von CAL2 für überlappende Klassen
<ul>
<li>Parameter <span class="math align-center">$S_1$</span> (Anzahl Objekte bis Entscheidung), <span class="math align-center">$S_2$</span> (Dominanz?)</li>
<li>Trainingsfehler wg. überlappender Klassen!</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106576&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest CAL3 (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit CAL3 (<span class="math align-center">$S_1=4, S_2=0.6$</span>).</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Unger1981'>[Unger1981] <strong>Lernfähige Klassifizierungssysteme (Classifier Systems Which Are Able to Learn)</strong><br>Unger, S. und Wysotzki, F., Akademie-Verlag, 1981.<br><em>Der Vollständigkeit halber aufgeführt (Werk ist leider vergriffen und wird nicht mehr verlegt)</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Entropie</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Die Entropie kann als Maß für den Informationsgehalt einer Trainingsmenge betrachtet werden:
Wieviele Ja/Nein-Entscheidungen sind nötig, um die Daten fehlerfrei zu repräsentieren?</p>
<p>Nach der Wahl eines Attributs kann die verbleibende mittlere Entropie berechnet werden. Damit
hat man ein Kriterium für die Auswahl von Attributen beim Aufbau von Entscheidungsbäumen:
Nimm das Attribut, welches einen möglichst hohen Informationsgehalt hat. Oder andersherum:
Wähle das Attribut, bei dem die verbleibende mittlere Entropie der Trainingsmenge nach der
Wahl des Attributs am kleinsten ist.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/4IZYA5EWO1k' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Entropie</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/6c84f8e181911aa89818cac70de40087fcab1209f4e0264f77da811289a5420fd284bc89464822690ff7906a735c778bda490949bf69091a5420885cc5cdad69' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Entropie</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Berechnung der Entropie und des Information Gain</li></ul>
  </div>
</div>




    <h2 id="wie-attribute-wählen">Wie Attribute wählen?</h2>
<h3 id="erinnerung-cal2cal3">Erinnerung: CAL2/CAL3</h3>
<ul>
<li>Zyklische Iteration durch die Trainingsmenge</li>
<li>Ausschließlich aktuelles Objekt betrachtet</li>
<li><span class='alert'>Reihenfolge</span> der &quot;richtigen&quot; Attributwahl bei Verzweigung unklar</li>
</ul>
<p>=&gt; Betrachte stattdessen die komplette Trainingsmenge!</p>
<h3 id="relevanz--informationsgehalt">Relevanz =&gt; Informationsgehalt</h3>
<ul>
<li>Shannon/Weaver (1949): <span class='alert'><strong>Entropie</strong></span>
<ul>
<li>Maß für die Unsicherheit einer Zufallsvariablen</li>
<li>Anzahl der Bits zur Darstellung der Ergebnisse eines Zufallsexperiments</li>
</ul>
</li>
</ul>
<h3 id="beispiele">Beispiele</h3>
<ul>
<li>Münze, die immer auf dem Rand landet: keine Unsicherheit, 0 Bit</li>
<li>Faire Münze: Kopf oder Zahl: Entropie 1 Bit</li>
<li>Fairer 4-seitiger Würfel: 4 mögliche Ausgänge: Entropie 2 Bit</li>
<li>Münze, die zu 99% auf einer Seite landet: Entropie nahe Null</li>
</ul>
<p>=&gt; Anzahl der Ja/Nein-Fragen, um zur gleichen Information zu kommen</p>
<h2 id="definition-der-entropie-hahahugoshortcode14s0hbhb-für-zufallsvariable-hahahugoshortcode14s1hbhb">Definition der Entropie <span class="math align-center">$H(V)$</span> für Zufallsvariable <span class="math align-center">$V$</span></h2>
<ul>
<li>Zufallsvariable <span class="math align-center">$V$</span> =&gt; mögliche Werte <span class="math align-center">$v_k$</span></li>
<li>Wahrscheinlichkeit für <span class="math align-center">$v_k$</span> sei <span class="math align-center">$p_k = P(v_k)$</span></li>
</ul>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p><span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<p><span class='alert'>Hinweis</span>:
<span class="math align-center">$\log_2 x = \frac{\log_{10} x}{\log_{10} 2} = \frac{\log x}{\log 2}$</span></p>
</div>
<div class='column'>
<p><a href="#R-image-04d8f80904ded657778dd9ed0f123215" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy/log_range.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-04d8f80904ded657778dd9ed0f123215"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy/log_range.png?width=auto&height=auto"></a></p>
</div>
</div>
<ul>
<li>Nur eine Klasse: <span class="math align-center">$\log_2 1 = 0$</span> =&gt; <span class="math align-center">$H(V) = 0$</span> Bit</li>
<li>Zwei Klassen, gleichwahrscheinlich: <span class="math align-center">$\log_2 0.5 = -1$</span> =&gt; <span class="math align-center">$H(V) = 1$</span> Bit</li>
</ul>
<h2 id="beispiele-entropie-faire-münze">Beispiele Entropie: faire Münze</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>Entropie: <span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<ul>
<li><span class="math align-center">$v_1 = \operatorname{Kopf},  v_2 = \operatorname{Zahl}$</span></li>
<li><span class="math align-center">$p_1 = 0.5,  p_2 = 0.5$</span></li>
<li><span class="math align-center">$H(\operatorname{Fair}) = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1$</span> Bit</li>
</ul>
</div>
<div class='column'>
<span class="math align-center">$\log_2 0.5 = -1$</span>
</div>
</div>
<h2 id="beispiele-entropie-unfaire-münze">Beispiele Entropie: unfaire Münze</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>Entropie: <span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<ul>
<li>
<span class="math align-center">$v_1 = \operatorname{Kopf},  v_2 = \operatorname{Zahl}$</span>
</li>
<li>
<span class="math align-center">$p_1 = 0.99,  p_2 = 0.01$</span>
</li>
<li>
<span class="math align-center">$H(\operatorname{UnFair}) = -(0.99 \log_2 0.99 + 0.01 \log_2 0.01)$</span>
<p><span class="math align-center">$H(\operatorname{UnFair}) \approx 0.08$</span> Bit</p>
</li>
</ul>
</div>
<div class='column'>
<span class="math align-center">$\log_2 0.01 \approx -6.64$</span>
<span class="math align-center">$\log_2 0.99 \approx -0,014$</span>
</div>
</div>
<h2 id="beispiele-entropie-4-seitiger-würfel">Beispiele Entropie: 4-seitiger Würfel</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>Entropie: <span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<ul>
<li><span class="math align-center">$v_1 = 1,  v_2 = 2,   v_3 = 3,   v_4 = 4$</span></li>
<li><span class="math align-center">$p_1 = p_2 = p_3 = p_4 = 0.25$</span></li>
<li><span class="math align-center">$H(\operatorname{Wuerfel}) = -4\cdot(0.25 \log_2 0.25) = 2$</span> Bit</li>
</ul>
</div>
<div class='column'>
<span class="math align-center">$\log_2 0.25 = -2$</span>
</div>
</div>
<h2 id="entropie-der-trainingsmenge-häufigkeit-der-klassen-zählen">Entropie der Trainingsmenge: Häufigkeit der Klassen zählen</h2>
<div class='columns'>
<div class='column'>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
</div>
<div class='column'>
<ul>
<li>Anzahl Klasse <span class="math align-center">$A$</span>: 4</li>
<li>Anzahl Klasse <span class="math align-center">$B$</span>: 2</li>
<li>Gesamtzahl Beispiele: 6</li>
</ul>
<p>Wahrscheinlichkeit für <span class="math align-center">$A$</span>: <span class="math align-center">$p_A = 4/6 = 0.667$</span></p>
<p>Wahrscheinlichkeit für <span class="math align-center">$B$</span>: <span class="math align-center">$p_B = 2/6 = 0.333$</span></p>
</div>
</div>
<span class="math align-center">$$
\begin{array}{rcl}
    H(S) &=& -\sum_k p_k \log_2 p_k\\
         &=& -(4/6 \cdot \log_2 4/6 + 2/6 \cdot \log_2 2/6)\\
         &=& -(-0.39 -0.53) = 0.92 \operatorname{Bit}
\end{array}
$$</span>
<h2 id="mittlere-entropie-nach-betrachtung-von-attribut-hahahugoshortcode14s40hbhb">Mittlere Entropie nach Betrachtung von Attribut <span class="math align-center">$A$</span></h2>
<span class="math align-center">$$
    R(S, A) = \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
$$</span>
<ul>
<li>
<p>Auswahl von Attribut <span class="math align-center">$A$</span> partitioniert die Trainingsmenge:
Je Ausprägung <span class="math align-center">$v$</span> von <span class="math align-center">$A$</span> erhält man eine Submenge <span class="math align-center">$S_v$</span></p>
</li>
<li>
<p><span class="math align-center">$R(S, A)$</span> berechnet die mittlere Entropie der Trainingsmenge, nachdem
Attribut <span class="math align-center">$A$</span> ausgewählt wurde: Unsicherheit/nötige Bits nach Auswahl von
Attribut <span class="math align-center">$A$</span></p>
</li>
</ul>
<h2 id="entropie-der-trainingsmenge-nach-attributwahl">Entropie der Trainingsmenge nach Attributwahl</h2>
<div class='columns'>
<div class='column'>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
</div>
<div class='column'>
<ul>
<li>Sei Attribut <span class="math align-center">$x_1$</span> ausgewählt</li>
<li><span class="math align-center">$x_1$</span> partitioniert die Trainingsmenge
<ul>
<li><span class="math align-center">$x_1=0$</span> liefert <span class="math align-center">$S_0 = \lbrace 1,3,5,6 \rbrace$</span></li>
<li><span class="math align-center">$x_1=1$</span> liefert <span class="math align-center">$S_1 = \lbrace 2,4 \rbrace$</span></li>
<li>Häufigkeit für <span class="math align-center">$x_1=0$</span>: <span class="math align-center">$4/6$</span></li>
<li>Häufigkeit für <span class="math align-center">$x_1=1$</span>: <span class="math align-center">$2/6$</span></li>
<li>Gesamtzahl Beispiele: 6</li>
</ul>
</li>
</ul>
</div>
</div>
<span class="math align-center">$$
\begin{array}{rcl}
    R(S, A) &=& \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)\\
         &=& 4/6 \cdot H(\lbrace 1,3,5,6 \rbrace) + 2/6 \cdot H(\lbrace 2,4 \rbrace)\\
         &=& 4/6\cdot(-3/4 \cdot \log_2 3/4 - 1/4 \cdot \log_2 1/4) +\\
         && 2/6\cdot(-1/2 \cdot \log_2 1/2 - 1/2 \cdot \log_2 1/2)\\
         &=& 0.54 + 0.33 = 0.87 \operatorname{Bit}
\end{array}
$$</span>
<h2 id="ausblick-gini-impurity">Ausblick: Gini Impurity</h2>
<p>Wir haben hier die <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="external" target="_blank">Entropie</a>
als Maß für den Informationsgehalt einer Trainingsmenge genutzt. <span class="math align-center">$R(S,A)$</span> als die mittlere
Entropie nach Betrachtung von Attribut <span class="math align-center">$A$</span> wird von typischen Entscheidungsbaumverfahren
wie ID3 und C4.5 genutzt, um bei einer Verzweigung das nächste möglichst aussagekräftige
Merkmal auszuwählen.</p>
<p>In anderen Entscheidungsbaumlernern wird stattdessen die
<a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="external" target="_blank">Gini Impurity</a>
zur Bestimmung des Informationsgehalts eingesetzt (u.a. CART). Dieses Maß sagt aus,
wie oft man ein zufällig gezogenes Element des Datensatzes falsch klassifizieren
würde, wenn man es mit einer zufälligen Klasse basierend auf der Verteilung der
Klassen im Datensatz labeln würde.</p>
<p>Hierzu drei lesenswerte Blog-Einträge:</p>
<ul>
<li><a href="https://medium.com/poli-data/deep-dive-into-the-basics-of-gini-impurity-in-decision-trees-with-math-intuition-46c721d4aaec" rel="external" target="_blank">Deep dive into the basics of Gini Impurity in Decision Trees with math Intuition</a></li>
<li><a href="https://towardsdatascience.com/decision-trees-explained-d7678c43a59e" rel="external" target="_blank">Decision Trees, Explained</a></li>
<li><a href="https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38" rel="external" target="_blank">Decision Tree Algorithm With Hands-On Example</a></li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Begriff und Berechnung der Entropie: Maß für die Unsicherheit</li>
<li>Begriff und Berechnung des Informationsgewinns
<ul>
<li>Entropie für eine Trainingsmenge</li>
<li>Mittlere Entropie nach Wahl eines Attributs</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106578&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Entropie (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Entropie einer Trainingsmenge</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Geben Sie die Entropie <span class="math align-center">$H(S)$</span> der Trainingsmenge an.</li>
<li>Berechnen Sie <span class="math align-center">$R(H,A)$</span> (die mittlere Entropie der Trainingsmenge, nachdem Attribut <span class="math align-center">$A$</span> gesehen wurde) für die einzelnen Attribute.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Entscheidungsbäume: Abschnitt 8.4</em></li> <li id='id_Mitchell2010'>[Mitchell2010] <strong>Machine Learning</strong><br>Mitchell, T., McGraw-Hill, 2010. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-0711-5467-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-0711-5467-3</a>.<br><em>ID3: Kapitel 3</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Entscheidungsbäume: Abschnitt 19.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>ID3 und C4.5</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Der Entscheidungsbaum-Lernalgorithmus <strong>ID3</strong> nutzt den Informationsgehalt für die Entscheidung
bei der Attributwahl: Nimm das Attribut, welches einen möglichst hohen Informationsgehalt hat.
Oder andersherum: Wähle das Attribut, bei dem die verbleibende mittlere Entropie der Trainingsmenge
nach der Wahl des Attributs am kleinsten ist. Oder noch anders formuliert: Nimm das Attribut, bei
dem die Differenz zwischen der Entropie der Trainingsmenge (vor der Wahl des Attributs) und der
verbleibenden mittleren Entropie (nach der Wahl des Attributs) am größten ist (die Differenz nennt
man auch &quot;<em>Information Gain</em>&quot;). Die Trainingsmenge wird entsprechend der Ausprägung in Bezug auf
das eben gewählte Merkmal aufgeteilt und an die Kinder des Knotens weiter gereicht; dort wird der
Baum rekursiv weiter aufgebaut.</p>
<p>Durch eine Normierung des <em>Information Gain</em> kann eine Verbesserung in Bezug auf mehrwertige
Attribute erreicht werden, dies führt zum Algorithmus <strong>C4.5</strong>.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/Yo1cmeS6BK8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL ID3 und C4.5</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/aa69406cfdf0ce8b2b614dd475926b56c8025239c5b4458ec7741c7733c6077fd192e4db6f58c8a3e39b7b895c2ddedf83327640326bfbedc2617c4f75bc59bd' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL ID3 und C4.5</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Entscheidungsbaumalgorithmen ID3 und C4.5</li></ul>
  </div>
</div>




    <h2 id="wie-attribute-wählen">Wie Attribute wählen?</h2>
<p>Erinnerung: CAL2/CAL3</p>
<ul>
<li>Zyklische Iteration durch die Trainingsmenge</li>
<li>Ausschließlich aktuelles Objekt betrachtet</li>
<li><span class='alert'>Reihenfolge</span> der &quot;richtigen&quot; Attributwahl bei Verzweigung unklar</li>
</ul>
<p>=&gt; Betrachte stattdessen die <strong>komplette</strong> Trainingsmenge!</p>
<h2 id="erinnerung-entropie-maß-für-die-unsicherheit">Erinnerung Entropie: Maß für die Unsicherheit</h2>
<ul>
<li>
<p>Entropie <span class="math align-center">$H(S)$</span> der Trainingsmenge <span class="math align-center">$S$</span>: relative Häufigkeit der Klassen zählen</p>
</li>
<li>
<p>Mittlere Entropie nach Betrachtung von Attribut <span class="math align-center">$A$</span></p>
<span class="math align-center">$$
        R(S, A) = \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
    $$</span>
</li>
<li>
<p>Informationsgewinn durch Betrachtung von Attribut <span class="math align-center">$A$</span></p>
<span class="math align-center">$$
    \begin{array}{rcl}
        \operatorname{Gain}(S, A) &=& H(S) - R(S, A)\\[5pt]
                                &=& H(S) - \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
    \end{array}
    $$</span>
</li>
</ul>
<p><span class="math align-center">$R(S,A)$</span> ist die Unsicherheit/nötige Bits nach Auswahl von Attribut A.
Je kleiner <span class="math align-center">$R(S,A)$</span>, um so kleiner die <strong>verbleibende Unsicherheit</strong> bzw.
um so kleiner die Anzahl der nötigen Bits zur Darstellung der
partitionierten Trainingsmenge <strong>nach</strong> Betrachtung von Attribut <span class="math align-center">$A$</span> ...</p>
<p>=&gt; Je kleiner <span class="math align-center">$R(S,A)$</span>, um so größer der Informationsgewinn</p>
<h2 id="informationsgewinn-kriterium-zur-auswahl-von-attributen">Informationsgewinn: Kriterium zur Auswahl von Attributen</h2>
<ol>
<li>Informationsgewinn für alle Attribute berechnen</li>
<li>Nehme Attribut mit größtem Informationsgewinn als nächsten Test</li>
</ol>
<div class='columns'>
<div class='column'>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
</div>
<div class='column'>
<span class="math align-center">$H(S) = 0.92 \operatorname{Bit}$</span>
<span class="math align-center">$$
\begin{array}{rcl}
\operatorname{Gain}(S, x_1) &=& 0.92 - 0.87 = 0.05 \operatorname{Bit}\\
\operatorname{Gain}(S, x_2) &=& 0.92 - 2/6  \cdot 0 - 4/6 \cdot 1\\
                            &=& 0.25 \operatorname{Bit}\\
\operatorname{Gain}(S, x_3) &=& 0.92 - 3/6 \cdot 0.92 - 2/6 \cdot 1 - 1/6 \cdot 0\\
                            &=& 0.13 \operatorname{Bit}
\end{array}
$$</span>
</div>
</div>
<p>Informationsgewinn für <span class="math align-center">$x_2$</span> am höchsten =&gt; wähle <span class="math align-center">$x_2$</span> als nächsten Test</p>
<h2 id="entscheidungsbaumlerner-id3-quinlan-1986">Entscheidungsbaumlerner ID3 (Quinlan, 1986)</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ID3</span>(examples, attr, default):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Abbruchbedingungen</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> examples<span style="color:#f92672">.</span>isEmpty():  <span style="color:#66d9ef">return</span> default
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> examples<span style="color:#f92672">.</span>each(<span style="color:#66d9ef">class</span> <span style="color:#960050;background-color:#1e0010">== </span><span style="color:#a6e22e">A</span>):  <span style="color:#66d9ef">return</span> A  <span style="color:#75715e"># all examples have same class</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> attr<span style="color:#f92672">.</span>isEmpty():  <span style="color:#66d9ef">return</span> examples<span style="color:#f92672">.</span>MajorityValue()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Baum mit neuem Test erweitern</span>
</span></span><span style="display:flex;"><span>    test <span style="color:#f92672">=</span> MaxInformationGain(examples, attr)
</span></span><span style="display:flex;"><span>    tree <span style="color:#f92672">=</span> new DecisionTree(test)
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> examples<span style="color:#f92672">.</span>MajorityValue()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> v_i <span style="color:#f92672">in</span> test:
</span></span><span style="display:flex;"><span>        ex_i <span style="color:#f92672">=</span> examples<span style="color:#f92672">.</span>select(test <span style="color:#f92672">==</span> v_i)
</span></span><span style="display:flex;"><span>        st <span style="color:#f92672">=</span> ID3(ex_i, attr <span style="color:#f92672">-</span> test, m)
</span></span><span style="display:flex;"><span>        tree<span style="color:#f92672">.</span>addBranch(label<span style="color:#f92672">=</span>v_i, subtree<span style="color:#f92672">=</span>st)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tree</span></span></code></pre></div>
<p><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html#id_Russell2020">[Russell2020]</a>: Man erhält aus dem &quot;Learn-Decision-Tree&quot;-Algorithmus <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html#id_Russell2020">[Russell2020, S. 678, Fig. 19.5]</a>
den hier vorgestellten ID3-Algorithmus, wenn man die Funktion <span class="math align-center">$\operatorname{Importance}(a, examples)$</span>
als <span class="math align-center">$\operatorname{InformationGain}(examples, attr)$</span> implementiert/nutzt.</p>
<p><strong>Hinweis</strong>: Mit der Zeile <code>if examples.each(class == A):  return A</code> soll ausgedrückt werden, dass alle
ankommenden Trainingsbeispiele die selbe Klasse haben und dass diese dann als Ergebnis zurückgeliefert
wird. Das &quot;<code>A</code>&quot; steht im obigen Algorithmus nur symbolisch für die selbe Klasse! Es kann also auch ein
anderes Klassensymbol als &quot;<code>A</code>&quot; sein ...</p>
<h3 id="beispiel-id3">Beispiel ID3</h3>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
<ul>
<li><span class="math align-center">$x2$</span> höchsten Information Gain</li>
<li><span class="math align-center">$x2=0$</span> =&gt; Beispiele 1,2 =&gt; A</li>
<li><span class="math align-center">$x2=1$</span> =&gt; Beispiele 3,4,5,6 =&gt; Information Gain berechnen,
weiter teilen und verzweigen</li>
</ul>
<h2 id="beobachtung-hahahugoshortcode16s27hbhb-ist-bei-mehrwertigen-attributen-höher">Beobachtung: <span class="math align-center">$\operatorname{Gain}$</span> ist bei mehrwertigen Attributen höher</h2>
<ul>
<li>
<p>Faire Münze:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Fair}) = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \operatorname{Bit}$</span></li>
</ul>
</li>
<li>
<p>4-seitiger Würfel:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Dice}) = -4\cdot(0.25 \log_2 0.25) = 2 \operatorname{Bit}$</span></li>
</ul>
</li>
</ul>
<p>=&gt; <span class="math align-center">$\operatorname{Gain}$</span> ist bei mehrwertigen Attributen höher</p>
<p>Damit würden Attribute bei der Wahl bevorzugt, nur weil sie mehr Ausprägungen haben als andere.</p>
<p><em>Anmerkung</em>: Im obigen Beispiel wurde einfach die Entropie für zwei &quot;Attribute&quot; mit unterschiedlich
vielen Ausprägungen betrachtet, das ist natürlich kein <span class="math align-center">$\operatorname{Gain}(S, A)$</span>. Aber es sollte
deutlich machen, dass Merkmale mit mehr Ausprägungen bei der Berechnung des Gain für eine Trainingsmenge
einfach wegen der größeren Anzahl an Ausprägungen rechnerisch bevorzugt würden.</p>
<h2 id="c45-als-verbesserung-zu-id3">C4.5 als Verbesserung zu ID3</h2>
<p>Normierter Informationsgewinn: <span class="math align-center">$\operatorname{Gain}(S, A) \cdot \operatorname{Normalisation}(A)$</span></p>
<span class="math align-center">$$
    \operatorname{Normalisation}(A) = \frac{1}{
        \sum_{v \in \operatorname{Values}(A)} p_v \log_2 \frac{1}{p_v}
    }
$$</span>
<p>C4.5 kann zusätzlich u.a. auch noch mit kontinuierlichen Attributen umgehen, vgl.
<a href="https://en.wikipedia.org/wiki/C4.5_algorithm" rel="external" target="_blank">en.wikipedia.org/wiki/C4.5_algorithm</a>.</p>
<p>In einem <a href="http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf" rel="external" target="_blank">Paper</a>
(<a href="https://doi.org/10.1007/s10115-007-0114-2" rel="external" target="_blank">DOI 10.1007/s10115-007-0114-2</a>) wurde
der Algorithmus zu den &quot;Top 10 algorithms in data mining&quot; ausgewählt.</p>
<p>Im Wikipedia-Artikel <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="external" target="_blank">Information Gain</a>
finden Sie weitere Informationen zum &quot;Informationsgewinn&quot; (<em>Information Gain</em>).</p>
<p>Ein anderer, relativ ähnlich arbeitender Entscheidungsbaumlerner ist der
<a href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="external" target="_blank">CART (Classification And Regression Tree)</a>-Algorithmus,
wobei der Begriff &quot;CART&quot; allerdings oft auch einfach allgemein für &quot;Entscheidungsbaumlerner&quot;
genutzt wird.</p>
<p>Hierzu drei lesenswerte Blog-Einträge:</p>
<ul>
<li><a href="https://medium.com/poli-data/deep-dive-into-the-basics-of-gini-impurity-in-decision-trees-with-math-intuition-46c721d4aaec" rel="external" target="_blank">Deep dive into the basics of Gini Impurity in Decision Trees with math Intuition</a></li>
<li><a href="https://towardsdatascience.com/decision-trees-explained-d7678c43a59e" rel="external" target="_blank">Decision Trees, Explained</a></li>
<li><a href="https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38" rel="external" target="_blank">Decision Tree Algorithm With Hands-On Example</a></li>
</ul>
<h2 id="beispiele-zur-normierung-bei-c45">Beispiele zur Normierung bei C4.5</h2>
<ul>
<li>
<p>Faire Münze:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Fair}) = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \operatorname{Bit}$</span></li>
<li>Normierung: <span class="math align-center">$1/(0.5 \log_2 (1/0.5) + 0.5 \log_2 (1/0.5)) = 1/(0.5 \cdot 1 + 0.5 \cdot 1) = 1$</span></li>
<li>Normierter Informationsgewinn: <span class="math align-center">$\operatorname{Gain}(S, A) \cdot \operatorname{Normalisation}(A) = 1 \operatorname{Bit} \cdot 1 = 1 \operatorname{Bit}$</span></li>
</ul>
</li>
<li>
<p>4-seitiger Würfel:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Dice}) = -4\cdot(0.25 \log_2 0.25) = 2 \operatorname{Bit}$</span></li>
<li>Normierung: <span class="math align-center">$1/(4\cdot 0.25 \log_2 (1/0.25)) = 1/(4\cdot 0.25 \cdot 2) = 0.5$</span></li>
<li>Normierter Informationsgewinn: <span class="math align-center">$\operatorname{Gain}(S, A) \cdot \operatorname{Normalisation}(A) = 2 \operatorname{Bit} \cdot 0.5 = 1 \operatorname{Bit}$</span></li>
</ul>
</li>
</ul>
<p>=&gt; Normierung sorgt für fairen Vergleich der Attribute</p>
<p><em>Anmerkung</em>: Auch hier ist die Entropie natürlich kein <span class="math align-center">$\operatorname{Gain}(S, A)$</span>. Das Beispiel soll
nur übersichtlich deutlich machen, dass der &quot;Vorteil&quot; von Attributen mit mehr Ausprägungen durch die
Normierung in C4.5 aufgehoben wird.</p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Entscheidungsbaumlerner <strong>ID3</strong>
<ul>
<li>Nutze <em>Information Gain</em> zur Auswahl des nächsten Attributs</li>
<li>Teile die Trainingsmenge entsprechend auf (&quot;nach unten hin&quot;)</li>
</ul>
</li>
<li>Verbesserung durch Normierung des <em>Information Gain</em>: <strong>C4.5</strong></li>
</ul>


    



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit ID3.</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Entscheidungsbäume: Abschnitt 8.4</em></li> <li id='id_Mitchell2010'>[Mitchell2010] <strong>Machine Learning</strong><br>Mitchell, T., McGraw-Hill, 2010. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-0711-5467-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-0711-5467-3</a>.<br><em>ID3: Kapitel 3</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Entscheidungsbäume: Abschnitt 19.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
        </div>
      </main>
    </div>
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/clipboard.min.js?1737742242" defer></script>
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/perfect-scrollbar.min.js?1737742242" defer></script>
    <script>
      function useMathJax( config ){
        window.MathJax = Object.assign( window.MathJax || {}, {
          tex: {
            inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
            displayMath: [['\\[', '\\]'], ['$$', '$$']], 
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/mathjax/tex-mml-chtml.js?1737742242"></script>
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/theme.js?1737742242" defer></script>
  </body>
</html>
