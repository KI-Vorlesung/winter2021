<!DOCTYPE html>
<html lang="de-DE" dir="ltr" itemscope itemtype="http://schema.org/Article">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.135.0">
    <meta name="generator" content="Relearn 6.4.1">
    <meta name="description" content="Quelle: &#34;künstliche intelligenz&#34; by Gerd Altmann (geralt) on Pixabay.com (Pixabay License)
Kursbeschreibung Ausgehend von den Fragen &#34;Was ist Intelligenz?&#34; und &#34;Was ist künstliche Intelligenz?&#34; werden wir uns in diesem Modul mit verschiedenen Teilgebieten der KI beschäftigen und uns anschauen, welche Methoden und Algorithmen es gibt und wie diese funktionieren. Dabei werden wir auch das Gebiet Machine Learning berühren, aber auch andere wichtige Gebiete betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen Methoden-Baukasten zur Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)">
    <meta name="twitter:description" content="Quelle: &#34;künstliche intelligenz&#34; by Gerd Altmann (geralt) on Pixabay.com (Pixabay License)
Kursbeschreibung Ausgehend von den Fragen &#34;Was ist Intelligenz?&#34; und &#34;Was ist künstliche Intelligenz?&#34; werden wir uns in diesem Modul mit verschiedenen Teilgebieten der KI beschäftigen und uns anschauen, welche Methoden und Algorithmen es gibt und wie diese funktionieren. Dabei werden wir auch das Gebiet Machine Learning berühren, aber auch andere wichtige Gebiete betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen Methoden-Baukasten zur Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...">
    <meta property="og:url" content="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html">
    <meta property="og:title" content="IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)">
    <meta property="og:description" content="Quelle: &#34;künstliche intelligenz&#34; by Gerd Altmann (geralt) on Pixabay.com (Pixabay License)
Kursbeschreibung Ausgehend von den Fragen &#34;Was ist Intelligenz?&#34; und &#34;Was ist künstliche Intelligenz?&#34; werden wir uns in diesem Modul mit verschiedenen Teilgebieten der KI beschäftigen und uns anschauen, welche Methoden und Algorithmen es gibt und wie diese funktionieren. Dabei werden wir auch das Gebiet Machine Learning berühren, aber auch andere wichtige Gebiete betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen Methoden-Baukasten zur Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...">
    <meta property="og:locale" content="de_DE">
    <meta property="og:type" content="website">
    <meta itemprop="name" content="IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)">
    <meta itemprop="description" content="Quelle: &#34;künstliche intelligenz&#34; by Gerd Altmann (geralt) on Pixabay.com (Pixabay License)
Kursbeschreibung Ausgehend von den Fragen &#34;Was ist Intelligenz?&#34; und &#34;Was ist künstliche Intelligenz?&#34; werden wir uns in diesem Modul mit verschiedenen Teilgebieten der KI beschäftigen und uns anschauen, welche Methoden und Algorithmen es gibt und wie diese funktionieren. Dabei werden wir auch das Gebiet Machine Learning berühren, aber auch andere wichtige Gebiete betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen Methoden-Baukasten zur Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...">
    <meta itemprop="wordCount" content="1857">
    <title>IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)</title>
    <link href="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html" rel="canonical" type="text/html" title="IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)">

    

    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/images/logo.png?1737742242" rel="icon" type="image/png">

    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fontawesome-all.min.css?1737742242" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fontawesome-all.min.css?1737742242" rel="stylesheet"></noscript>
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/nucleus.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/auto-complete.css?1737742242" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/auto-complete.css?1737742242" rel="stylesheet"></noscript>
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/perfect-scrollbar.min.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fonts.css?1737742242" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fonts.css?1737742242" rel="stylesheet"></noscript>
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/theme.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/theme-auto.css?1737742242" rel="stylesheet" id="R-variant-style">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/chroma-auto.css?1737742242" rel="stylesheet" id="R-variant-chroma-style">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/variant.css?1737742242" rel="stylesheet">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/print.css?1737742242" rel="stylesheet" media="print">
    <link href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/format-print.css?1737742242" rel="stylesheet">
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/variant.js?1737742242"></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.relBasePath='.';
      window.relearn.relBaseUri='..\/..\/..\/..\/..';
      window.relearn.absBaseUri='https:\/\/www.hsbi.de\/elearning\/data\/FH-Bielefeld\/lm_data\/lm_1358898';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.index_js_url="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.search.js?1737742242";
      // variant stuff
      window.variants && variants.init( [ 'auto', 'zen-light', 'zen-dark', 'relearn-bright', 'relearn-light', 'relearn-dark' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script><style type="text/css">

 
.center {
    align-content: center;
    text-align: center;
    margin: auto;
}
.alert {
    color: #ff3333;
}
.bsp {
    padding: 0.05cm;
    border-width: 0.05cm;
    border-style: solid;
    border-color: #ddd;
    background-color: #ddd;
    border-radius: 25px;
    float: right;
}
.cbox {
    padding: 0.2cm;
    border-width: 0.1cm;
    border-style: solid;
    border-color: #4070a0;
    background-color: #f2f2f2;
    margin: auto;
    width: 60%;
    text-align: center;
    overflow: auto;
}
.blueArrow {
    color: #4070a0;
    font-family: "Courier New", "Courier", monospace;
    font-weight: bold;
}
.origin {
    background-color: #ededed;
    font-size: 0.8em;
}
.showme {
    background-color: #ededed;
    font-size: 0.8em;
}


 
.tldr {
    background: #dbe4ed;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.recap {
    
    
   margin: 4px 0px 26px 0px;
}
.bib {
    background: #dbe4ed;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.outcomes {
    background: #d9e9d5;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.quizzes {
    background: #d9e9d5;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.challenges {
    background: #ebe4d6;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.assignments {
    background: #ebe4d6;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
h1.tldr, h1.recap, h1.bib, h1.outcomes, h1.quizzes, h1.challenges, h1.assignments {
    padding: 0px;
}


 
.noJsAlert {
    padding: 20px;
    background-color: #f44336;  
    color: white;
    margin-bottom: 15px;
}


 
.embed-video-player {
    position: relative;
    padding-bottom: 56%;
    height: 0;
    overflow: hidden;
}
.youtube-player {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border:0;
}


 
#header-wrapper {
    padding:0.6rem;
}


 
#shortcuts {
    padding-top: 2.0rem;
}


 
#chapter p {
    text-align: left;
}


 
figcaption h4 {
    margin-top:-2.5rem;
}
.border1 {
    border:1px solid black;
}

 
td ul, td ol {
    margin: 0 0 1rem 0.5rem;
    padding: 0 0 0 0.5rem;
}

 
h1 { font-size:2.8rem !important;}
h2 { font-size:2.2rem; margin:1.2rem 0}
h3 { font-size:1.9rem; text-align:left !important; font-weight:400 !important;}
h4 { font-size:1.6rem}
h5 { font-size:1.3rem}
h6 { font-size:1rem}

h2 {
    width:100% !important;
    border-bottom:1px solid #5e5e5e !important;
    padding-bottom: 2px;
}
.tldr h2, .recap h2, .bib h2, .outcomes h2, .quizzes h2, .challenges h2, .assignments h2 {
    margin:0.5rem 0
}

.btn-crossreference, .btn-crossreference:hover {
    cursor: initial;
}

</style>

  </head>
  <body class="mobile-support print disableInlineCopyToClipboard" data-url="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
          </div>
          <span class="topbar-breadcrumbs highlightable">
            IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)
          </span>
          <div class="topbar-area topbar-area-end" data-area="end">
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable home" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="home">
            <header class="headline">
            </header>

<h1 id="ifm-32-po23--ifm-514-po18--inf701-künstliche-intelligenz-winter-202425">IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)</h1>

<p><a href="#R-image-611b6d38b1c47b6b1cd4d4fe554ad962" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://cdn.pixabay.com/photo/2018/09/27/09/22/artificial-intelligence-3706562_1280.jpg?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-611b6d38b1c47b6b1cd4d4fe554ad962"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://cdn.pixabay.com/photo/2018/09/27/09/22/artificial-intelligence-3706562_1280.jpg?width=60%25&height=auto"></a>
<span class='origin'>Quelle: <a href="https://pixabay.com/de/illustrations/k%c3%bcnstliche-intelligenz-netzwerk-3706562/" rel="external" target="_blank">&quot;künstliche intelligenz&quot;</a> by <a href="https://pixabay.com/de/users/geralt-9301/" rel="external" target="_blank">Gerd Altmann (geralt)</a> on Pixabay.com (<a href="https://pixabay.com/de/service/license/" rel="external" target="_blank">Pixabay License</a>)</span></p>
<h2 id="kursbeschreibung">Kursbeschreibung</h2>
<p>Ausgehend von den Fragen &quot;Was ist <em>Intelligenz</em>?&quot; und &quot;Was ist <em>künstliche</em> Intelligenz?&quot;
werden wir uns in diesem Modul mit <strong>verschiedenen Teilgebieten der KI</strong> beschäftigen und
uns anschauen, welche <strong>Methoden und Algorithmen</strong> es gibt und wie diese funktionieren. Dabei
werden wir auch das Gebiet <em>Machine Learning</em> berühren, aber auch andere wichtige Gebiete
betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen <strong>Methoden-Baukasten</strong> zur
Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die
Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...</p>
<h2 id="überblick-modulinhalte">Überblick Modulinhalte</h2>
<ol>
<li>Problemlösen
<ul>
<li>Zustände, Aktionen, Problemraum</li>
<li>Suche (blind, informiert): Breiten-, Tiefensuche, Best-First,
Branch-and-Bound, A-Stern</li>
<li>Lokale Suche: Gradientenabstieg, Genetische/Evolutionäre Algorithmen (GA/EA)</li>
<li>Spiele: Minimax, Alpha-Beta-Pruning, Heuristiken</li>
<li>Constraints: Backtracking, Heuristiken, Propagation, AC-3</li>
</ul>
</li>
<li>Maschinelles Lernen
<ul>
<li>Merkmalsvektor, Trainingsmenge, Trainingsfehler, Generalisierung</li>
<li>Entscheidungsbäume: CAL2, CAL3, ID3, C4.5</li>
<li>Neuronale Netze
<ul>
<li>Perzeptron, Lernregel</li>
<li>Feedforward Multilayer Perzeptron (MLP), Backpropagation,
Trainings- vs. Generalisierungsfehler</li>
<li>Steuerung des Trainings: Kreuzvalidierung, Regularisierung</li>
<li>Ausblick: Support-Vektor-Maschinen</li>
</ul>
</li>
<li>Naive Bayes Klassifikator</li>
</ul>
</li>
<li><del>Inferenz, Logik</del> (<strong>entfällt im W24</strong>)
<ul>
<li><del>Prädikatenlogik: Modellierung, semantische und formale Beweise,
Unifikation, Resolution</del></li>
<li><del>Ausblick: Anwendung in Prolog</del></li>
</ul>
</li>
</ol>
<h2 id="team">Team</h2>
<ul>
<li><a href="https://www.hsbi.de/minden/ueber-uns/personenverzeichnis/carsten-gips" rel="external" target="_blank">Carsten Gips</a> (HSBI, Sprechstunde nach Vereinbarung)</li>
<li><a href="http://people.tau.edu.tr/people.show/cananyildiz/de" rel="external" target="_blank">Canan Yıldız</a> (TDU)</li>
<li><a href="https://people.tau.edu.tr/people.show/fulya.yenilmez/de" rel="external" target="_blank">Fulya Yenilmez</a> (TDU)</li>
</ul>
<h2 id="kursformat">Kursformat</h2>
<p><a href="#R-image-249d1575245e6feb04fa35d1db5db7df" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/fahrplan.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-249d1575245e6feb04fa35d1db5db7df"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/fahrplan.png?width=80%25&height=auto"></a></p>
<div class="tab-panel" data-tab-group="hochschule">
  <div class="tab-nav">
    <div class="tab-nav-title">&#8203;</div>
    <button
      data-tab-item="ifm-32-gki-hsbi-po23-3-semester"
      class="tab-nav-button tab-panel-style cstyle initial active" tabindex="-1"
      onclick="switchTab('hochschule','ifm-32-gki-hsbi-po23-3-semester')"
    >
      <span class="tab-nav-text">IFM 3.2 GKI (HSBI, PO23, 3. Semester)</span>
    </button>
    <button
      data-tab-item="ifm-514-ki-hsbi-po18-5-semester"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('hochschule','ifm-514-ki-hsbi-po18-5-semester')"
    >
      <span class="tab-nav-text">IFM 5.14 KI (HSBI, PO18, 5. Semester)</span>
    </button>
    <button
      data-tab-item="inf701-ki-tdu"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('hochschule','inf701-ki-tdu')"
    >
      <span class="tab-nav-text">INF701 KI (TDU)</span>
    </button>
  </div>
  <div class="tab-content-container">
    <div
      data-tab-item="ifm-32-gki-hsbi-po23-3-semester"
      class="tab-content tab-panel-style cstyle initial active">
      <div class="tab-content-text">
<p><strong>Vorlesung (2 SWS)</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">07.10. - 24.01.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Mo, 11:00 - 12:30 Uhr (DE) (online)</td>
      </tr>
      <tr>
          <td style="text-align: left">(<em>Flipped Classroom</em>)</td>
      </tr>
  </tbody>
</table>
<p><strong>Praktikum (2 SWS)</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Praktikumsgruppe</th>
          <th style="text-align: left">07.10. - 24.01.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">G1</td>
          <td style="text-align: left">Mo, 17:00 bis 18:30 Uhr (DE) (online)</td>
      </tr>
      <tr>
          <td style="text-align: left">G2</td>
          <td style="text-align: left">Mo, 15:15 bis 16:45 Uhr (DE) (online)</td>
      </tr>
      <tr>
          <td style="text-align: left">G3</td>
          <td style="text-align: left">Di, 09:45 bis 11:15 Uhr (DE) (online)</td>
      </tr>
  </tbody>
</table>
<p>Online-Sitzungen per Zoom (<strong>Zugangsdaten siehe <a href="https://www.hsbi.de/elearning/goto.php?target=crs_1400597&client_id=FH-Bielefeld" rel="external" target="_blank">ILIAS IFM 3.2 GKI (PO23, 3. Semester)</a></strong>).
Sie <em>können</em> hierzu den Raum J101 bzw. J102 (siehe Stundenplan) nutzen.</p>
      </div>
    </div>
    <div
      data-tab-item="ifm-514-ki-hsbi-po18-5-semester"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">
<p><strong>Vorlesung (2 SWS)</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">07.10. - 24.01.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Mo, 11:00 - 12:30 Uhr (DE) (online)</td>
      </tr>
      <tr>
          <td style="text-align: left">(<em>Flipped Classroom</em>)</td>
      </tr>
  </tbody>
</table>
<p><strong>Praktikum (2 SWS)</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Praktikumsgruppe</th>
          <th style="text-align: left">07.10. - 24.01.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">G1</td>
          <td style="text-align: left">Mi, 11:30 bis 13:00 Uhr (DE) (online)</td>
      </tr>
      <tr>
          <td style="text-align: left">G2</td>
          <td style="text-align: left">Mi, 14:00 bis 15:30 Uhr (DE) (online)</td>
      </tr>
      <tr>
          <td style="text-align: left">G3</td>
          <td style="text-align: left">Fr, 11:30 bis 13:00 Uhr (DE) (online)</td>
      </tr>
  </tbody>
</table>
<p>Online-Sitzungen per Zoom (<strong>Zugangsdaten siehe <a href="https://www.hsbi.de/elearning/goto.php?target=crs_1400604&client_id=FH-Bielefeld" rel="external" target="_blank">ILIAS IFM 5.14 KI (PO18, 5. Semester)</a></strong>).
Sie <em>können</em> hierzu den Raum J101 bzw. J102 (siehe Stundenplan) nutzen.</p>
      </div>
    </div>
    <div
      data-tab-item="inf701-ki-tdu"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">
<p><strong>Vorlesung (2 SWS)</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">30.09. - 25.10.</th>
          <th style="text-align: left">28.10. - 15.01.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Mo, 12:00 - 13:30 Uhr (TR)</td>
          <td style="text-align: left">Mo, 13:00 - 14:30 Uhr (TR)</td>
      </tr>
      <tr>
          <td style="text-align: left">online</td>
          <td style="text-align: left">online</td>
      </tr>
  </tbody>
</table>
<p>Durchführung als <em>Flipped Classroom</em>: Sitzungen per Zoom (<strong>Zugangsdaten siehe <a href="https://classroom.google.com/c/NzE4Mzk0NDE5ODEz?cjc=fhzfku3" rel="external" target="_blank">Google Classroom</a></strong>)</p>
<p><strong>Übung (2 SWS)</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Übungsgruppe</th>
          <th style="text-align: left">30.09. - 15.01.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">G1 / G2</td>
          <td style="text-align: left">wird bekanntgegeben</td>
      </tr>
      <tr>
          <td style="text-align: left">G3 / G4</td>
          <td style="text-align: left">wird bekanntgegeben</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">online</td>
      </tr>
  </tbody>
</table>
<p>Sitzungen per Google Meet (<strong>Zugangsdaten siehe <a href="https://classroom.google.com/c/NzE4Mzk0NDE5ODEz?cjc=fhzfku3" rel="external" target="_blank">Google Classroom</a></strong>)</p>
      </div>
    </div>
  </div>
</div>
<h2 id="fahrplan">Fahrplan</h2>
<div class="tab-panel" data-tab-group="hochschule">
  <div class="tab-nav">
    <div class="tab-nav-title">&#8203;</div>
    <button
      data-tab-item="ifm-32-gki-hsbi-po23-3-semester"
      class="tab-nav-button tab-panel-style cstyle initial active" tabindex="-1"
      onclick="switchTab('hochschule','ifm-32-gki-hsbi-po23-3-semester')"
    >
      <span class="tab-nav-text">IFM 3.2 GKI (HSBI, PO23, 3. Semester)</span>
    </button>
    <button
      data-tab-item="ifm-514-ki-hsbi-po18-5-semester"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('hochschule','ifm-514-ki-hsbi-po18-5-semester')"
    >
      <span class="tab-nav-text">IFM 5.14 KI (HSBI, PO18, 5. Semester)</span>
    </button>
    <button
      data-tab-item="inf701-ki-tdu"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('hochschule','inf701-ki-tdu')"
    >
      <span class="tab-nav-text">INF701 KI (TDU)</span>
    </button>
  </div>
  <div class="tab-content-container">
    <div
      data-tab-item="ifm-32-gki-hsbi-po23-3-semester"
      class="tab-content tab-panel-style cstyle initial active">
      <div class="tab-content-text">
<div class="box notices cstyle note">
  <div class="box-label">
    <i class="fa-fw fas fa-exclamation-circle"></i> News
  </div>
  <div class="box-content">
<div class="expand">
  <input type="checkbox" id="R-expand-c249e5b5ec030df73decb432bdb81123" aria-controls="R-expandcontent-c249e5b5ec030df73decb432bdb81123">
  <label class="expand-label" for="R-expand-c249e5b5ec030df73decb432bdb81123">
    <i class="expander-icon fa-fw fas fa-chevron-down"></i>
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 24.01.25
  </label>
  <div id="R-expandcontent-c249e5b5ec030df73decb432bdb81123" class="expand-content">
<p><strong>Planung Klausur GKI (04.02.25)</strong></p>
<p>Die Klausur in GKI wird am Dienstag, den 04.02.25 von <strong>10:00 bis ca. 11:30 Uhr</strong>
im B40 stattfinden.</p>
  </div>
</div>
<div class="expand">
  <input type="checkbox" id="R-expand-1356a696566e4a4454ae74d461b76bce" aria-controls="R-expandcontent-1356a696566e4a4454ae74d461b76bce">
  <label class="expand-label" for="R-expand-1356a696566e4a4454ae74d461b76bce">
    <i class="expander-icon fa-fw fas fa-chevron-down"></i>
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 09.12.24
  </label>
  <div id="R-expandcontent-1356a696566e4a4454ae74d461b76bce" class="expand-content">
<p>Am 16.12. wird es in der Vorlesung eine Einführung ins Deep Learning geben sowie die
bereits angekündigte offene Sprechstunde.</p>
<p>Das Thema Deep Learning ist nicht prüfungsrelevant (aber trotzdem spannend ;).</p>
  </div>
</div>
<div class="expand">
  <input type="checkbox" id="R-expand-9adc58c6b8880c09a499685336a2e33b" aria-controls="R-expandcontent-9adc58c6b8880c09a499685336a2e33b">
  <label class="expand-label" for="R-expand-9adc58c6b8880c09a499685336a2e33b">
    <i class="expander-icon fa-fw fas fa-chevron-down"></i>
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 04.12.24
  </label>
  <div id="R-expandcontent-9adc58c6b8880c09a499685336a2e33b" class="expand-content">
<p>Da die Projektwoche (16.-20.12.2024) mangels Interesse nicht stattfinden wird, biete ich eine
zusätzliche Sprechstunde im Vorlesungsslot am 16.12. an.</p>
  </div>
</div>
<div class="expand">
  <input type="checkbox" id="R-expand-966e0e312b3ca1f2ff3f0758d2f69ca4" aria-controls="R-expandcontent-966e0e312b3ca1f2ff3f0758d2f69ca4">
  <label class="expand-label" for="R-expand-966e0e312b3ca1f2ff3f0758d2f69ca4">
    <i class="expander-icon fa-fw fas fa-chevron-down"></i>
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 29.11.24
  </label>
  <div id="R-expandcontent-966e0e312b3ca1f2ff3f0758d2f69ca4" class="expand-content">
<p><strong>Projektwoche</strong>: Vom 16.-20.12.2024 findet unsere gemeinsame Projektwoche des ersten und
dritten Semesters statt. Sie können in einem Team an spannenden Aufgaben arbeiten. In den
teilnehmenden Veranstaltungen entfallen deshalb die Vorlesungen und Übungen - wir nehmen
mit &quot;Grundlagen der KI&quot; (GKI) an der Projektwoche teil :-)</p>
<p>Wenn Sie in Ihr Projekt nachweislich Inhalte aus &quot;Grundlagen der KI&quot; (GKI) einbringen, zählt
das wie ein zusätzliches Übungsblatt. Sie haben also eine zusätzliche Chance für das Testat
(6 aus 11 Blättern) ...</p>
<p>Melden Sie sich bis zum 04.12. unter <a href="https://www.hsbi.de/elearning/goto.php?target=crs_1460449" rel="external" target="_blank">https://www.hsbi.de/elearning/goto.php?target=crs_1460449</a>
an. Dort finden Sie auch weitere Informationen zum Ablauf.</p>
  </div>
</div>
  </div>
</div>
<p>Hier finden Sie einen abonnierbaren <a href="https://calendar.google.com/calendar/ical/552fdc6c19e64eda7b36b2d16a88bf4b7e593af2c520afbe1aeeb0bb4f43107d%40group.calendar.google.com/public/basic.ics" rel="external" target="_blank">Google Kalender IFM 3.2 GKI (PO23, 3. Semester)</a> mit allen Terminen der Veranstaltung zum Einbinden in Ihre Kalender-App.</p>
<p>Abgabe der Übungsblätter jeweils <strong>Montag bis 11:00 Uhr</strong> im <a href="https://www.hsbi.de/elearning/goto.php?target=exc_1420535&client_id=FH-Bielefeld" rel="external" target="_blank">ILIAS</a>. Vorstellung der Lösung im jeweiligen Praktikum in der Abgabewoche.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Monat</th>
          <th style="text-align: left">Woche</th>
          <th style="text-align: left">Vorlesung</th>
          <th style="text-align: left">Lead</th>
          <th style="text-align: left">Abgabe Aufgabenblatt</th>
          <th style="text-align: left">Vorstellung Praktikum</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Oktober</td>
          <td style="text-align: left">41</td>
          <td style="text-align: left">07.: <a href="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#%C3%BCberblick-modulinhalte" rel="external" target="_blank">Orga</a> (<em>Zoom</em>); <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html">Einführung KI</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html">Problemlösen</a>; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn01-perceptron.html">Perzeptron</a></td>
          <td style="text-align: left">Carsten, Canan</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">42</td>
          <td style="text-align: left">14.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression.html">Lineare Regression</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">14.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-perceptron.html">Blatt: Perzeptron</a></td>
          <td style="text-align: left">14. / 15.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">43</td>
          <td style="text-align: left">21.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression.html">Logistische Regression</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">44</td>
          <td style="text-align: left">28.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn04-overfitting.html">Overfitting</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp.html">Multilayer Perceptron</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">28.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-regression.html">Blatt: Regression</a></td>
          <td style="text-align: left">28. / 29.</td>
      </tr>
      <tr>
          <td style="text-align: left">November</td>
          <td style="text-align: left">45</td>
          <td style="text-align: left">04.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn06-backprop.html">Backpropagation</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">04.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-mlp.html">Blatt: MLP</a></td>
          <td style="text-align: left">04. / 05.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">46</td>
          <td style="text-align: left">11.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing.html">Training &amp; Testing</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn08-testing.html">Performanzanalyse</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">11.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-backprop.html">Blatt: Backpropagation</a></td>
          <td style="text-align: left">11. / 12.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">47</td>
          <td style="text-align: left">18.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2.html">CAL2</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl3-pruning.html">Pruning</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl4-cal3.html">CAL3</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy.html">Entropie</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html">ID3 und C4.5</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">48</td>
          <td style="text-align: left">25.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html">Tiefensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Breitensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html">Branch-and-Bound</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html">Best First</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A-Stern</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">25.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html">Blatt: DTL</a></td>
          <td style="text-align: left">25. / 26.</td>
      </tr>
      <tr>
          <td style="text-align: left">Dezember</td>
          <td style="text-align: left">49</td>
          <td style="text-align: left">02.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html">Gradientensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing.html">Simulated Annealing</a>; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro.html">Intro EA/GA</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga.html">Genetische Algorithmen</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">02.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html">Blatt: Suche</a></td>
          <td style="text-align: left">02. / 03.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">50</td>
          <td style="text-align: left">09.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro.html">Optimale Spiele</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax.html">Games mit Minimax</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html">Minimax und Heuristiken</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html">Alpha-Beta-Pruning</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">09.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html">Blatt: EA/GA</a></td>
          <td style="text-align: left">09. / 10.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">51</td>
          <td style="text-align: left">16.: <del>Projektwoche Semester 1+3</del> Intro Deep Learning und offene Sprechstunde</td>
          <td style="text-align: left">Canan, Carsten</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">52</td>
          <td style="text-align: left">23.: <em>Weihnachtspause</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">01</td>
          <td style="text-align: left">30.: <em>Weihnachtspause</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">Januar</td>
          <td style="text-align: left">02</td>
          <td style="text-align: left">06.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro.html">Einführung Constraints</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch.html">Lösen von diskreten CSP</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html">CSP und Heuristiken</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html">Kantenkonsistenz und AC-3</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">06.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html">Blatt: Games</a></td>
          <td style="text-align: left">06. / 07.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">03</td>
          <td style="text-align: left">13.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb1-probability.html">Wahrscheinlichkeitstheorie</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb2-naivebayes.html">Naive Bayes</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">13.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html">Blatt: CSP</a></td>
          <td style="text-align: left">13. / 14.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">04</td>
          <td style="text-align: left">20.: Rückblick (<em>Zoom</em>), <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html">Prüfungsvorbereitung HSBI</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">20.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nb.html">Blatt: Naive Bayes</a></td>
          <td style="text-align: left">20. / 21.</td>
      </tr>
      <tr>
          <td style="text-align: left"><em>(Prüfungsphase I)</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Klausur</strong>: Di, 04. Feb 2025, 10-18 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher)</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"><em>(Prüfungsphase II)</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Klausur</strong>: Di, 01. Apr 2025, 10-16 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher)</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
      </div>
    </div>
    <div
      data-tab-item="ifm-514-ki-hsbi-po18-5-semester"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">
<div class="box notices cstyle note">
  <div class="box-label">
    <i class="fa-fw fas fa-exclamation-circle"></i> News
  </div>
  <div class="box-content">
<div class="expand">
  <input type="checkbox" id="R-expand-ca5c48cffd9f8f0a186b44f3ec2c28c7" aria-controls="R-expandcontent-ca5c48cffd9f8f0a186b44f3ec2c28c7">
  <label class="expand-label" for="R-expand-ca5c48cffd9f8f0a186b44f3ec2c28c7">
    <i class="expander-icon fa-fw fas fa-chevron-down"></i>
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 24.01.25
  </label>
  <div id="R-expandcontent-ca5c48cffd9f8f0a186b44f3ec2c28c7" class="expand-content">
<p><strong>Planung Klausur KI (04.02.25)</strong></p>
<p>Die Klausur in KI wird am Dienstag, den 04.02.25 von <strong>12:00 bis ca. 13:30 Uhr</strong>
im B40 stattfinden.</p>
  </div>
</div>
<div class="expand">
  <input type="checkbox" id="R-expand-b9ad7b1ccb868fe17950bd1f6f81baba" aria-controls="R-expandcontent-b9ad7b1ccb868fe17950bd1f6f81baba">
  <label class="expand-label" for="R-expand-b9ad7b1ccb868fe17950bd1f6f81baba">
    <i class="expander-icon fa-fw fas fa-chevron-down"></i>
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 09.12.24
  </label>
  <div id="R-expandcontent-b9ad7b1ccb868fe17950bd1f6f81baba" class="expand-content">
<p>Am 16.12. wird es in der Vorlesung eine Einführung ins Deep Learning geben sowie die
bereits angekündigte offene Sprechstunde.</p>
<p>Das Thema Deep Learning ist nicht prüfungsrelevant (aber trotzdem spannend ;).</p>
  </div>
</div>
  </div>
</div>
<p>Hier finden Sie einen abonnierbaren <a href="https://calendar.google.com/calendar/ical/5ad0fbba66a7823a0687b9ed373dcb3bdc1cc8d87139f755e7188face897f129%40group.calendar.google.com/public/basic.ics" rel="external" target="_blank">Google Kalender IFM 5.14 KI (PO18, 5. Semester)</a> mit allen Terminen der Veranstaltung zum Einbinden in Ihre Kalender-App.</p>
<p>Abgabe der Übungsblätter jeweils <strong>Mittwoch bis 11:00 Uhr</strong> <a href="https://www.hsbi.de/elearning/goto.php?target=exc_1420536&client_id=FH-Bielefeld" rel="external" target="_blank">im ILIAS</a>. Vorstellung der Lösung im jeweiligen Praktikum in der Abgabewoche.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Monat</th>
          <th style="text-align: left">Woche</th>
          <th style="text-align: left">Vorlesung</th>
          <th style="text-align: left">Lead</th>
          <th style="text-align: left">Abgabe Aufgabenblatt</th>
          <th style="text-align: left">Vorstellung Praktikum</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Oktober</td>
          <td style="text-align: left">41</td>
          <td style="text-align: left">07.: <a href="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#%C3%BCberblick-modulinhalte" rel="external" target="_blank">Orga</a> (<em>Zoom</em>); <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html">Einführung KI</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html">Problemlösen</a>; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn01-perceptron.html">Perzeptron</a></td>
          <td style="text-align: left">Carsten, Canan</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">42</td>
          <td style="text-align: left">14.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression.html">Lineare Regression</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">16.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-perceptron.html">Blatt: Perzeptron</a></td>
          <td style="text-align: left">16. / 18.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">43</td>
          <td style="text-align: left">21.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression.html">Logistische Regression</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">44</td>
          <td style="text-align: left">28.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn04-overfitting.html">Overfitting</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp.html">Multilayer Perceptron</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">November</td>
          <td style="text-align: left">45</td>
          <td style="text-align: left">04.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn06-backprop.html">Backpropagation</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">06.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-regression.html">Blatt: Regression</a></td>
          <td style="text-align: left">06. / 08.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">46</td>
          <td style="text-align: left">11.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing.html">Training &amp; Testing</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn08-testing.html">Performanzanalyse</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left">13.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-mlp.html">Blatt: MLP</a></td>
          <td style="text-align: left">13. / 15.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">47</td>
          <td style="text-align: left">18.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2.html">CAL2</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl3-pruning.html">Pruning</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl4-cal3.html">CAL3</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy.html">Entropie</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html">ID3 und C4.5</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">20.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-backprop.html">Blatt: Backpropagation</a></td>
          <td style="text-align: left">20. / 22.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">48</td>
          <td style="text-align: left">25.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html">Tiefensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Breitensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html">Branch-and-Bound</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html">Best First</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A-Stern</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">27.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html">Blatt: DTL</a></td>
          <td style="text-align: left">27. / 29.</td>
      </tr>
      <tr>
          <td style="text-align: left">Dezember</td>
          <td style="text-align: left">49</td>
          <td style="text-align: left">02.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html">Gradientensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing.html">Simulated Annealing</a>; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro.html">Intro EA/GA</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga.html">Genetische Algorithmen</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">04.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html">Blatt: Suche</a></td>
          <td style="text-align: left">04. / 06.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">50</td>
          <td style="text-align: left">09.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro.html">Optimale Spiele</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax.html">Games mit Minimax</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html">Minimax und Heuristiken</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html">Alpha-Beta-Pruning</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">11.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html">Blatt: EA/GA</a></td>
          <td style="text-align: left">11. / 13.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">51</td>
          <td style="text-align: left">16.: <del>Projektwoche Semester 1+3</del> Intro Deep Learning und offene Sprechstunde</td>
          <td style="text-align: left">Canan, Carsten</td>
          <td style="text-align: left">18.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html">Blatt: Games</a></td>
          <td style="text-align: left">18. / 20.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">52</td>
          <td style="text-align: left">23.: <em>Weihnachtspause</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">01</td>
          <td style="text-align: left">30.: <em>Weihnachtspause</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">Januar</td>
          <td style="text-align: left">02</td>
          <td style="text-align: left">06.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro.html">Einführung Constraints</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch.html">Lösen von diskreten CSP</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html">CSP und Heuristiken</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html">Kantenkonsistenz und AC-3</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">03</td>
          <td style="text-align: left">13.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb1-probability.html">Wahrscheinlichkeitstheorie</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb2-naivebayes.html">Naive Bayes</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">15.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html">Blatt: CSP</a></td>
          <td style="text-align: left">15. / 17.</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">04</td>
          <td style="text-align: left">20.: Rückblick (<em>Zoom</em>), <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html">Prüfungsvorbereitung HSBI</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left">22.: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nb.html">Blatt: Naive Bayes</a></td>
          <td style="text-align: left">22. / 24.</td>
      </tr>
      <tr>
          <td style="text-align: left"><em>(Prüfungsphase I)</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Klausur</strong>: Di, 04. Feb 2025, 10-18 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher)</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left"><em>(Prüfungsphase II)</em></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Klausur</strong>: Di, 01. Apr 2025, 10-16 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher)</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
      </div>
    </div>
    <div
      data-tab-item="inf701-ki-tdu"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">
<table>
  <thead>
      <tr>
          <th style="text-align: left">KW</th>
          <th style="text-align: left">Monat</th>
          <th style="text-align: left">Tag</th>
          <th style="text-align: left">Vorlesung</th>
          <th style="text-align: left">Lead</th>
          <th style="text-align: left">Abgabe Übung</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">40</td>
          <td style="text-align: left">Sep</td>
          <td style="text-align: left">30.</td>
          <td style="text-align: left"><a href="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#%C3%BCberblick-modulinhalte" rel="external" target="_blank">Orga</a> (<em>Zoom</em>); <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html">Einführung KI</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html">Problemlösen</a></td>
          <td style="text-align: left">Canan, Carsten</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">41</td>
          <td style="text-align: left">Okt</td>
          <td style="text-align: left">07. (<strong>12:30 - 13:30</strong> Uhr TR)</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn01-perceptron.html">Perzeptron</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">42</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">14.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression.html">Lineare Regression</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-perceptron.html">Blatt: Perzeptron</a></td>
      </tr>
      <tr>
          <td style="text-align: left">43</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">21.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression.html">Logistische Regression</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">44</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">28.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn04-overfitting.html">Overfitting</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp.html">Multilayer Perceptron</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-regression.html">Blatt: Regression</a></td>
      </tr>
      <tr>
          <td style="text-align: left">45</td>
          <td style="text-align: left">Nov</td>
          <td style="text-align: left">04.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn06-backprop.html">Backpropagation</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-mlp.html">Blatt: MLP</a></td>
      </tr>
      <tr>
          <td style="text-align: left">46</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">11.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing.html">Training &amp; Testing</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn08-testing.html">Performanzanalyse</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-backprop.html">Blatt: Backpropagation</a></td>
      </tr>
      <tr>
          <td style="text-align: left">47</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">18.</td>
          <td style="text-align: left"><strong>Zwischenprüfung</strong></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">48</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">25.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html">Tiefensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Breitensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html">Branch-and-Bound</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html">Best First</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A-Stern</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">49</td>
          <td style="text-align: left">Dez</td>
          <td style="text-align: left">02.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html">Gradientensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing.html">Simulated Annealing</a>; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro.html">Intro EA/GA</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga.html">Genetische Algorithmen</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html">Blatt: Suche</a></td>
      </tr>
      <tr>
          <td style="text-align: left">50</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">09.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro.html">Optimale Spiele</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax.html">Games mit Minimax</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html">Minimax und Heuristiken</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html">Alpha-Beta-Pruning</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html">Blatt: EA/GA</a></td>
      </tr>
      <tr>
          <td style="text-align: left">51</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">16.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn10-cnn.html">Vorschau Deep Learning (CNN, RNN)</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html">Blatt: Games</a></td>
      </tr>
      <tr>
          <td style="text-align: left">52</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">23. (<strong>Google Meet</strong>)</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-tdu.html">Prüfungsvorbereitung TDU</a></td>
          <td style="text-align: left">Canan</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">01</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">30. (<strong>KEINE Sprechstunde</strong>)</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2.html">CAL2</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl3-pruning.html">Pruning</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl4-cal3.html">CAL3</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy.html">Entropie</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html">ID3 und C4.5</a></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">02</td>
          <td style="text-align: left">Jan</td>
          <td style="text-align: left">06.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro.html">Einführung Constraints</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch.html">Lösen von diskreten CSP</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html">CSP und Heuristiken</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html">Kantenkonsistenz und AC-3</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html">Blatt: DTL</a></td>
      </tr>
      <tr>
          <td style="text-align: left">03</td>
          <td style="text-align: left"></td>
          <td style="text-align: left">13.</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb1-probability.html">Wahrscheinlichkeitstheorie</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb2-naivebayes.html">Naive Bayes</a></td>
          <td style="text-align: left">Carsten</td>
          <td style="text-align: left"><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html">Blatt: CSP</a></td>
      </tr>
  </tbody>
</table>
      </div>
    </div>
  </div>
</div>
<h2 id="prüfungsform-note-und-credits">Prüfungsform, Note und Credits</h2>
<div class="tab-panel" data-tab-group="hochschule">
  <div class="tab-nav">
    <div class="tab-nav-title">&#8203;</div>
    <button
      data-tab-item="ifm-32-gki-hsbi-po23-3-semester"
      class="tab-nav-button tab-panel-style cstyle initial active" tabindex="-1"
      onclick="switchTab('hochschule','ifm-32-gki-hsbi-po23-3-semester')"
    >
      <span class="tab-nav-text">IFM 3.2 GKI (HSBI, PO23, 3. Semester)</span>
    </button>
    <button
      data-tab-item="ifm-514-ki-hsbi-po18-5-semester"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('hochschule','ifm-514-ki-hsbi-po18-5-semester')"
    >
      <span class="tab-nav-text">IFM 5.14 KI (HSBI, PO18, 5. Semester)</span>
    </button>
    <button
      data-tab-item="inf701-ki-tdu"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('hochschule','inf701-ki-tdu')"
    >
      <span class="tab-nav-text">INF701 KI (TDU)</span>
    </button>
  </div>
  <div class="tab-content-container">
    <div
      data-tab-item="ifm-32-gki-hsbi-po23-3-semester"
      class="tab-content tab-panel-style cstyle initial active">
      <div class="tab-content-text">
<p><strong>Klausur plus Testat</strong>, 5 ECTS</p>
<ul>
<li>
<p><strong>Testat</strong>: Vergabe der Credit-Points</p>
<p>Kriterien: Mindestens 6 der 10 Aufgabenblätter erfolgreich bearbeitet.</p>
<p>(&quot;erfolgreich bearbeitet&quot;: Bearbeitung individuell (also in 1er Teams),
je mindestens 60% bearbeitet, fristgerechte Abgabe der Lösungen im ILIAS,
Vorstellung der Lösungen im Praktikum)</p>
</li>
<li>
<p><strong>Klausur</strong>: =&gt; Modulnote</p>
<p>Schriftliche Prüfung (&quot;<strong>Klausur</strong>&quot;) am Ende des Semesters (in beiden
Prüfungszeiträumen; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html">Prüfungsvorbereitung HSBI</a>).</p>
</li>
</ul>
<p>Die nächste Klausur für &quot;Grundlagen der KI&quot; wird am <strong>Dienstag, 04. Februar 2025</strong>
angeboten. Die Klausur wird als digitale Klausur auf dem Prüfungs-ILIAS der HSBI in
Präsenz vor Ort in <strong>Minden im Raum B40</strong> durchgeführt. Die Prüfung für GKI beginnt
um <strong>10:00 Uhr</strong> und <strong>dauert 90 Minuten</strong>. Ein DIN-A4-Zettel ist als Hilfsmittel
zugelassen. Der geprüfte Stoff bezieht sich auf den zuletzt durchgeführten Kurs
(Winter 2024/25). Weitere Informationen siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html">Prüfungsvorbereitung HSBI</a>.</p>
      </div>
    </div>
    <div
      data-tab-item="ifm-514-ki-hsbi-po18-5-semester"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">
<p><strong>Klausur plus Testat</strong>, 5 ECTS</p>
<ul>
<li>
<p><strong>Testat</strong>: Vergabe der Credit-Points</p>
<p>Kriterien: Mindestens 6 der 10 Aufgabenblätter erfolgreich bearbeitet.</p>
<p>(&quot;erfolgreich bearbeitet&quot;: Bearbeitung individuell (also in 1er Teams),
je mindestens 60% bearbeitet, fristgerechte Abgabe der Lösungen im ILIAS,
Vorstellung der Lösungen im Praktikum)</p>
</li>
<li>
<p><strong>Klausur</strong>: =&gt; Modulnote</p>
<p>Schriftliche Prüfung (&quot;<strong>Klausur</strong>&quot;) am Ende des Semesters (in beiden
Prüfungszeiträumen; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html">Prüfungsvorbereitung HSBI</a>).</p>
</li>
</ul>
<p>Die nächste Klausur für &quot;Künstliche Intelligenz&quot; wird am <strong>Dienstag, 04. Februar 2025</strong>
angeboten. Die Klausur wird als digitale Klausur auf dem Prüfungs-ILIAS der HSBI in
Präsenz vor Ort in <strong>Minden im Raum B40</strong> durchgeführt. Die Prüfung für KI beginnt
um <strong>12:00 Uhr</strong> und <strong>dauert 90 Minuten</strong>. Ein DIN-A4-Zettel ist als Hilfsmittel
zugelassen. Der geprüfte Stoff bezieht sich auf den zuletzt durchgeführten Kurs
(Winter 2024/25). Weitere Informationen siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html">Prüfungsvorbereitung HSBI</a>.</p>
      </div>
    </div>
    <div
      data-tab-item="inf701-ki-tdu"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">
<table>
  <thead>
      <tr>
          <th style="text-align: left">Prüfung</th>
          <th style="text-align: left">Gewicht</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Zwischenprüfung</td>
          <td style="text-align: left"><strong>40 %</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">Endprüfung</td>
          <td style="text-align: left"><strong>60 %</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">Übung</td>
          <td style="text-align: left"><strong>10 % Bonus für Endprüfung</strong></td>
      </tr>
  </tbody>
</table>
<p>Wenn in der Endprüfung die 40 Punkte Mindestgrenze erreicht wird (<strong>Prüfungsnote ≥40</strong>),
werden <strong>10 % der Übungspunkte als Bonus</strong> zu der Prüfungsnote hinzugefügt.</p>
<p>Für die Vergabe von Übungspunkten ist eine <strong>erfolgreiche Teilnahme an der Übung</strong> erforderlich.
<strong>Für Details siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-tdu.html">Prüfung &amp; Noten @TDU</a></strong>.</p>
      </div>
    </div>
  </div>
</div>
<h2 id="materialien">Materialien</h2>
<ol>
<li><a href="http://aima.cs.berkeley.edu/" rel="external" target="_blank">&quot;<strong>Artificial Intelligence: A Modern Approach</strong>&quot;</a> (<em>AIMA</em>).
Russell, S. und Norvig, P., Pearson, 2020.
ISBN <a href="https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993" rel="external" target="_blank">978-0134610993</a>.</li>
<li>&quot;Introduction to Artificial Intelligence&quot;.
Ertel, W., Springer, 2017.
ISBN <a href="https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4" rel="external" target="_blank">978-3-319-58487-4</a>.
DOI <a href="https://doi.org/10.1007/978-3-319-58487-4" rel="external" target="_blank">10.1007/978-3-319-58487-4</a>.</li>
<li>&quot;An Introduction to Machine Learning&quot;.
Kubat, M., Springer, 2017.
ISBN <a href="https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-63913-0" rel="external" target="_blank">978-3-319-63913-0</a>.
DOI <a href="https://doi.org/10.1007/978-3-319-63913-0" rel="external" target="_blank">10.1007/978-3-319-63913-0</a>.</li>
</ol>
<h2 id="förderungen-und-kooperationen">Förderungen und Kooperationen</h2>
<h3 id="kooperation-zw-hsbi-und-tdu">Kooperation zw. HSBI und TDU</h3>
<p>Über das Projekt <a href="https://www.hsbi.de/en/digitalmobil" rel="external" target="_blank">&quot;Digital Mobil @ FH Bielefeld&quot;</a> der Fachhochschule Bielefeld (HSBI) ist
im Sommer 2020 eine Kooperation mit der Türkisch-Deutschen Universität in Istanbul (TDU)
im Modul &quot;Künstliche Intelligenz&quot; gestartet.</p>
<p>Wir werden in diesem Semester die Vorlesungen und auch die Übungen/Praktika wieder im
Co-Teaching durchführen. In den Zoom-Sitzungen nehmen deshalb alle Studierenden gemeinsam
(TDU und HSBI) teil.</p>
<h3 id="kooperation-mit-dem-digikos-projekt">Kooperation mit dem DigikoS-Projekt</h3>
<p>Diese Vorlesung wurde zudem vom Projekt <a href="https://www.digikos.de" rel="external" target="_blank">&quot;Digitalbaukasten für kompetenzorientiertes Selbststudium&quot;</a>
(<em>DigikoS</em>) unterstützt. Ein vom DigikoS-Projekt ausgebildeter Digital Learning Scout hat
insbesondere die Koordination der digitalen Gruppenarbeiten, des Peer-Feedbacks und der
Postersessions in ILIAS technisch und inhaltlich begleitet. DigikoS wird als Verbundprojekt
von der Stiftung Innovation in der Hochschullehre gefördert.</p>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)</h1>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="einführung-ki">Einführung KI</h1>

<p>Was ist Intelligenz? Was ist künstliche Intelligenz? Woran kann man das erkennen?
Wie kann man eine Welt (ein Problem) modellieren, um es dann anschließend lösen zu können?</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html">Intro: Was ist Künstliche Intelligenz?</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html">Problemlösen</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Einführung KI</h1>
<article class="default">
<h1>Intro: Was ist Künstliche Intelligenz?</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>KI ist ein altes und modernes Forschungsgebiet, welches periodisch Hype-Zeiten erlebt hat und sich aktuell
wieder in einer Hoch-Phase befindet. Wer heute &quot;KI&quot; sagt, meint meist Maschinelles Lernen oder (noch genauer)
eine Form von <em>Deep Learning</em>. Dabei gibt es in der KI viele weitere Gebiete: Suche (Problemlösen), Spiele,
Constraintprobleme, Entscheidungsbäume, ..., um nur einige zu nennen.</p>
<p>Der <em>Turing-Test</em> (Alan Turin, 1950) hat gewissermaßen die modernen Zweige der KI begründet, u.a.
<em>Wissensrepräsentation</em>, <em>Logisches Schließen</em>, <em>Maschinelles Lernen</em>, <em>Verarbeitung natürlicher Sprache</em>,
<em>Computer Vision</em> und <em>Robotik</em>. Dabei ist zwischen <em>starker KI</em> und <em>schwacher KI</em> zu unterscheiden.</p>
<p>Häufig werden die Gebiete in einem Diagramm eingeordnet, wobei die x-Achse Verhalten vs. Denken und
die y-Achse Rational vs. Menschlich aufspannen. So kann man beispielsweise Logik dem rationalen Denken
zuordnen oder die Erforschung kognitiver Prozesse dem Quadranten menschliches Denken.</p>
<p>Wenn man sich die Geschichte der KI anschaut, beobachtet man bei fast allen Themen, dass sie in der
Vergangenheit eine Hype-Phase erlebt haben und dabei die oft stark überzogenen Erwartungen enttäuscht
haben und danach meist nur wenig Beachtung erfuhren. Nach einer Weile kamen die Themen wieder
&quot;auf die Tagesordnung&quot;, diesmal mit vernünftigen Erwartungen.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/oETxokgYdDk' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Einführung</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/80e8eaac54e2b4bb11488efc73672cccbec3bb9660187613385cf34e0428a82048d6371cebec02a2bb678882fb53fab49d9bab56ed6fcfe8ac6ddbad6a740bfc' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Einführung</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K1) Aspekte, die zur Intelligenz gerechnet werden</li> <li>(K1) Turing-Test: Aufbau, Gebiete, Kritik</li> <li>(K1) Gebiete der KI sowie deren Zielsetzung</li> <li>(K1) Starke vs. schwache KI</li> <li>(K1) Wichtige Strömungen in der KI und ihre zeitliche Einordnung</li></ul>
  </div>
</div>




    <h2 id="was-ist-künstliche-intelligenz">Was ist (künstliche) Intelligenz?</h2>
<p><a href="#R-image-bbc9ba3462f0ade259d2341c01c3c657" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://live.staticflickr.com/2889/10151827605_911e35be10_c_d.jpg?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-bbc9ba3462f0ade259d2341c01c3c657"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://live.staticflickr.com/2889/10151827605_911e35be10_c_d.jpg?width=80%25&height=auto"></a></p>
<p><span class='origin'>Quelle: <a href="https://www.flickr.com/photos/80267257@N05/10151827605" rel="external" target="_blank">AvB - RoboCup 2013 - Eindhoven</a> by <a href="https://www.flickr.com/photos/80267257@N05" rel="external" target="_blank">RoboCup2013</a> on Flickr.com (<a href="https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=rich" rel="external" target="_blank">CC BY 2.0</a>)</span></p>
<ul>
<li>Was ist (künstliche) Intelligenz?</li>
<li>Ist <a href="https://en.wikipedia.org/wiki/Data_(Star_Trek)" rel="external" target="_blank">Commander Data</a> intelligent?</li>
<li>Woran erkennen Sie das?</li>
</ul>
<h2 id="definition-intelligenz">Definition Intelligenz</h2>
<blockquote>
<p>Intelligenz (von lat. <em>intellegere</em> &quot;verstehen&quot;, wörtlich &quot;wählen zwischen ...&quot;
von lat. <em>inter</em> &quot;zwischen&quot; und <em>legere</em> &quot;lesen, wählen&quot;) ist in der
Psychologie ein Sammelbegriff für die <strong>kognitive Leistungsfähigkeit</strong> des
Menschen. Da einzelne kognitive Fähigkeiten unterschiedlich stark ausgeprägt
sein können und keine Einigkeit besteht, wie diese zu bestimmen und zu
unterscheiden sind, gibt es <strong>keine allgemeingültige Definition der
Intelligenz</strong>.</p>
<p> <span class='origin'>Quelle: <a href="https://de.wikipedia.org/wiki/Intelligenz" rel="external" target="_blank">&quot;Intelligenz&quot;</a> by <a href="https://de.wikipedia.org/wiki/Benutzer:Cumtempore" rel="external" target="_blank">Cumtempore</a> and <a href="https://xtools.wmflabs.org/articleinfo-authorship/de.wikipedia.org/Intelligenz?uselang=de" rel="external" target="_blank">others</a> on Wikipedia (<a href="https://creativecommons.org/licenses/by-sa/3.0/legalcode" rel="external" target="_blank">CC BY-SA 3.0</a>)</span></p>
</blockquote>
<p>Das ist spannend: Es gibt keine allgemeingültige Definition für den Begriff &quot;Intelligenz&quot;.
Damit wird es schwierig, auch &quot;Künstliche Intelligenz&quot; zu definieren ...</p>
<p>Aber wir können aus dieser Definition auf Wikipedia mitnehmen, dass es um kognitive
Fähigkeiten geht. Verstehen und im weiteren Sinne Denken sind bereits im Begriff enthalten
und damit auch Teil der kognitiven Fähigkeiten.</p>
<p>Schauen wir uns nun noch die Definition von &quot;kognitiven Fähigkeiten&quot; genauer an.</p>
<blockquote>
<p>Zu den <strong>kognitiven Fähigkeiten</strong> eines Menschen zählen die
<strong>Wahrnehmung</strong>, die Aufmerksamkeit, die Erinnerung, das <strong>Lernen</strong>, das
<strong>Problemlösen</strong>, die Kreativität, das <strong>Planen</strong>, die Orientierung, die
Imagination, die <strong>Argumentation</strong>, die Introspektion, der Wille, das
Glauben und einige mehr. Auch Emotionen haben einen wesentlichen kognitiven
Anteil.</p>
<p> <span class='origin'>Quelle: <a href="https://de.wikipedia.org/wiki/Kognition" rel="external" target="_blank">&quot;Kognition&quot;</a> by <a href="https://de.wikipedia.org/wiki/User:Arbraxan" rel="external" target="_blank">Arbraxan</a> and <a href="https://xtools.wmflabs.org/articleinfo-authorship/de.wikipedia.org/Kognition?uselang=de" rel="external" target="_blank">others</a> on Wikipedia (<a href="https://creativecommons.org/licenses/by-sa/3.0/legalcode" rel="external" target="_blank">CC BY-SA 3.0</a>)</span></p>
</blockquote>
<p>Zu den kognitiven Fähigkeiten und damit zur Intelligenz zählen also eine Reihe von
Fähigkeiten, die man Menschen im allgemeinen zuschreibt. Lernen und Problemlösen
und Planen sind Dinge, die vermutlich jeder direkt mit dem Begriff Intelligenz
verbindet. Interessanterweise gehören auf auch Aufmerksamkeit und Wahrnehmung und
Orientierung mit dazu -- Fähigkeiten, die beispielsweise in der Robotik sehr
wichtig sind. Kreativität und Vorstellung zählen aber auch mit in den Bereich der
kognitiven Fähigkeiten und damit zum Begriff Intelligenz. In der KI werden diese
Gebiete zunehmend interessant, etwa beim Komponieren von Musik und beim Erzeugen
von Bildern oder Texten. Mit Emotionen beschäftigt sich die KI-Forschung aktuell
nur am Rande, etwa bei der Erkennung von Emotionen in Texten. Andere Gebiete der
kognitiven Fähigkeiten wie Glaube und Wille spielen derzeit keine Rolle in der KI.</p>
<h2 id="versuch-einer-definition-für-ki">Versuch einer Definition für &quot;KI&quot;</h2>
<blockquote>
<p>Ziel der KI ist es, Maschinen zu entwickeln, die sich verhalten, als
verfügten sie über Intelligenz.</p>
<p> -- John McCarthy (1955)</p>
</blockquote>
<p>Einwand: <a href="https://en.wikipedia.org/wiki/Braitenberg_vehicle" rel="external" target="_blank">Braitenberg-Vehikel</a> zeigen
so etwas wie &quot;intelligentes&quot; Verhalten, sind aber noch lange nicht intelligent! Bezieht
sich nur auf <em>Verhalten</em>!</p>
<blockquote>
<p>KI ist die Fähigkeit digitaler Computer oder computergesteuerter Roboter,
Aufgaben zu lösen, die normalerweise mit den höheren intellektuellen
Verarbeitungsfähigkeiten von Menschen in Verbindung gebracht werden ...</p>
<p> -- Encyclopedia Britannica</p>
</blockquote>
<p>Einwand: Dann wäre aber auch Auswendig-Lernen oder das Multiplizieren langer
Zahlen als intelligent zu betrachten! Bezieht sich vor allem auf &quot;<em>Denken</em>&quot;!</p>
<blockquote>
<p>Artificial Intelligence is the study of how to make computers do things at
which, at the moment, people are better.</p>
<p> -- Elaine Rich (&quot;Artificial Intelligence&quot;, McGraw-Hill, 1983)</p>
</blockquote>
<p><span class='origin'>Quelle: nach <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html#id_Ertel2017">[Ertel2017, pp. 1-3]</a></span></p>
<p>Dazu gehört auch</p>
<ul>
<li>Anpassung an sich verändernde Situationen</li>
<li>Erkennung von Bildern und Gesichtern und Emotionen</li>
<li>Erkennung von Sprache</li>
<li>Umgang mit kontextbehafteten, unvollständigen Informationen</li>
<li>Ausräumen von Geschirrspülern ;-)</li>
<li>Über Emotionen und Empathie verfügen</li>
</ul>
<div class="box notices cstyle info">
  <div class="box-label">
    <i class="fa-fw fas fa-info-circle"></i> KI-Grundverordnung der EU
  </div>
  <div class="box-content">
<p>Die EU hat am 13. Juni 2024 die sogenannte &quot;KI-Verordnung&quot; verabschiedet (&quot;VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES&quot;,
<a href="https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689" rel="external" target="_blank">Document 32024R1689: Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz) (Text von Bedeutung für den EWR)</a>).</p>
<p>Dort finden Sie unter Artikel 3 &quot;Begriffsbestimmungen&quot; unter Absatz 1 eine Begriffsdefinition.
Ein &quot;KI-System&quot; wird darin als ein maschinengestütztes System definiert, in irgendeiner Art für
einen autonomen Betrieb ausgelegt ist und eine gewisse Anpassungsfähigkeit haben kann. Zusätzlich
soll das &quot;KI-System&quot; aus den Eingaben Vorhersagen und Entscheidungen zu treffen oder auch Inhalte
erzeugen und mit der physischen oder digitalen Umwelt interagieren können.
<span class='origin'>Quelle: <a href="https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689" rel="external" target="_blank">VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES</a>, Art. 3 Abs. 1</span></p>
<p>Interessant ist, dass dabei nicht explizit auf Softwaresysteme eingegangen wird. Später im
Text finden sich Hinweise, dass sich ein KI-System vermutlich als Software repräsentiert,
auch kommen Modelle und Daten vor. Bei den Modellen kommt der Begriff des Lernens vor, in
allen derzeit üblichen Varianten (überwachtes Lernen, unüberwachtes Lernen, Reinforcement
Learning). Große Teile des Dokuments beschäftigen sich mit weitreichenden Bestimmungen für
Akteure, die ein KI-System zur Verfügung stellen wollen.</p>
<p>Lesen Sie selbst: <a href="https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689" rel="external" target="_blank">VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES</a>.</p>
  </div>
</div>
<h2 id="alan-turing-1950-turing-test-begründet-zweige-der-ki">Alan Turing 1950: Turing-Test (begründet Zweige der KI)</h2>
<ul>
<li>Wann verhält sich eine Maschine intelligent?</li>
</ul>
<p><a href="#R-image-16b71d102dfe0e005a8b3818bc08e80f" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/e/e4/Turing_Test_version_3.png?width=40%25&height=auto" style=" height: auto; width: 40%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-16b71d102dfe0e005a8b3818bc08e80f"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/e/e4/Turing_Test_version_3.png?width=40%25&height=auto"></a></p>
<p><span class='origin'>Quelle: <a href="https://commons.wikimedia.org/wiki/File:Turing_Test_version_3.png" rel="external" target="_blank">Turing Test version 3.png</a> by <a href="https://commons.wikimedia.org/wiki/User:Bilby" rel="external" target="_blank">Bilby</a> on Wikimedia Commons (<a href="https://en.wikipedia.org/wiki/en:public_domain" rel="external" target="_blank">Public Domain</a>)</span></p>
<p>Zum Bestehen des Turing-Tests ist (u.a.) erforderlich:</p>
<ul>
<li><em>Wissensrepräsentation</em>: Speichern des gesammelten Wissens <strong>=&gt;</strong> <strong>Wissensbasierte Systeme</strong></li>
<li><em>Logisches Schließen</em>: Beantworten von Fragen mit Hilfe des vorhandenen Wissens <strong>=&gt;</strong> <strong>Logik, Resolution</strong></li>
<li><em>Maschinelles Lernen</em>: Anpassung an veränderliches Umfeld <strong>=&gt;</strong> <strong>Musteranalyse und Mustererkennung und Mustervorhersage</strong></li>
<li><em>Verarbeitung natürlicher Sprache</em>: Erfolgreiche Kommunikation, beispielsweise in Englisch <strong>=&gt;</strong> <strong>NLP</strong></li>
</ul>
<p>&quot;Totaler Turing-Test&quot;: zusätzlich <strong>Computer Vision</strong> (Erkennen von Objekten) und
<strong>Robotik</strong> (Manipulation von Objekten)</p>
<p>Damit begründet der Turing-Test die Gebiete der KI.</p>
<p><strong>Problem</strong>: Der Turing-Test ist nicht reproduzierbar, nicht konstruktiv und
nicht mathematisch analysierbar ... Außerdem wird durch den Turing-Test im Wesentlichen
nur Funktionalität geprüft, nicht ob Intention oder Bewusstsein vorhanden ist.</p>
<h2 id="starke-vs-schwache-ki">Starke vs. schwache KI</h2>
<h3 id="schwache-ki">&quot;Schwache KI&quot;</h3>
<ul>
<li>Simulation intelligenten Verhaltens</li>
<li>Lösung konkreter Probleme</li>
<li>Adaptives Verhalten (&quot;Lernen&quot;)</li>
<li>Umgang mit Unsicherheit und unvollständigen Informationen</li>
</ul>
<h3 id="starke-ki">&quot;Starke KI&quot;</h3>
<ul>
<li>Eigenschaften der &quot;schwachen KI&quot;</li>
<li>Intelligenz nach menschlichen Maßstäben (auf &quot;Augenhöhe&quot;)</li>
<li>Bewusstsein</li>
<li>Emotionen (?)</li>
<li>Empathie</li>
</ul>
<div class="box notices cstyle note">
  <div class="box-label">
    <i class="fa-fw fas fa-exclamation-circle"></i> Frage
  </div>
  <div class="box-content">
<p>Wie würden Sie Systeme wie ChatGPT einordnen? Woran machen Sie das fest?</p>
  </div>
</div>
<h2 id="typische-ansätze-in-der-ki">Typische Ansätze in der KI</h2>
<p><a href="#R-image-dab329eb5741bc13630ae9dea5a72f27" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview/dimensionen-ki.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-dab329eb5741bc13630ae9dea5a72f27"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview/dimensionen-ki.png?width=60%25&height=auto"></a></p>
<p>Untersuchung von</p>
<ul>
<li>Verhalten vs. Denken</li>
<li>Rational vs. menschlich</li>
</ul>
<p>Motivation dabei</p>
<ul>
<li>Menschliche Intelligenz verstehen</li>
<li>Intelligente/intelligent wirkende Systeme bauen</li>
</ul>
<p>Damit erhält man vier Kombinationen:</p>
<ul>
<li>Menschliches Denken</li>
<li>Rationales Denken</li>
<li>Rationales Verhalten</li>
<li>Menschliches Verhalten</li>
</ul>
<h3 id="menschliches-denken-kognitive-modellierung">Menschliches Denken: Kognitive Modellierung</h3>
<ul>
<li>Welche kognitiven Fähigkeiten sind für intelligentes Verhalten nötig?
Wie laufen Denkprozesse im Gehirn ab?</li>
<li>Erfordert Theorien über interne Aktivitäten des Gehirns</li>
<li>Ansätze:
<ul>
<li>top-down: Vorhersage und Test von menschlichem Verhalten</li>
<li>bottom-up: Auswertung neurobiologischer Daten</li>
</ul>
</li>
<li>Wissenschaftszweige: Kognitionswissenschaft (Verbindung der
Computermodelle der KI mit den Experimenten und Theorien der
Psychologie), Neurobiologie/-informatik</li>
</ul>
<div class="box notices cstyle info">
  <div class="box-label">
    <i class="fa-fw fas fa-info-circle"></i> Neuronale bzw. Konnektionistische KI
  </div>
  <div class="box-content">
<p>Die Schule der sogenannten &quot;<strong>Neuronalen bzw. Konnektionistischen KI</strong>&quot; verfolgt den Ansatz,
die biologischen Prozesse im Gehirn zu verstehen und nachzubauen (bottom-up Ansatz) und auf
reale Probleme anzuwenden.</p>
<p>Dank massiver Rechenleistung, riesigen Datenmengen und geeigneten Modellen (Deep Learning)
kann diese Tradition aktuell große Erfolge vorzeigen.</p>
  </div>
</div>
<h3 id="rationales-denken-aristoteles-was-sind-korrekte-argumente-und-denkprozesse--wie-sollen-wir-denken">Rationales Denken: Aristoteles: Was sind korrekte Argumente und Denkprozesse? =&gt; Wie sollen wir denken?</h3>
<p>Beispiel:</p>
<pre><code>Fakt: Sokrates ist ein Mensch.
Fakt: Alle Menschen sind sterblich.
Folgerung: Sokrates ist sterblich.
</code></pre>
<ul>
<li>Formalisierte Problembeschreibung</li>
<li>Problemlösung durch <em>logische Prozesse</em></li>
<li>Verbindung von moderner KI zur Mathematik und Philosophie</li>
</ul>
<div class="box notices cstyle info">
  <div class="box-label">
    <i class="fa-fw fas fa-info-circle"></i> Symbolische KI
  </div>
  <div class="box-content">
<p>Die Schule der sogenannten &quot;<strong>Symbolische KI</strong>&quot; verfolgt den top-down-Ansatz, mit Hilfe
formaler Beweise Schlüsse zu ziehen und damit Fragen zu beantworten bzw. Probleme zu lösen.
Dabei wird die betrachtete &quot;Welt&quot;, also Gedanken, Konzepte und Beziehungen zwischen Objekten
durch Symbole und Formeln repräsentiert, ähnlich der Art und Weise, wie Menschen logisch
denken und kommunizieren.</p>
<p>Das Hauptproblem dieser Tradition liegt im Aufwand bei der geeigneten Formalisierung der
realen Welt.</p>
  </div>
</div>
<h3 id="rationales-verhalten-das-richtige-tun">Rationales Verhalten: Das &quot;Richtige&quot; tun</h3>
<ul>
<li>
<p>Das &quot;Richtige&quot;: Verhalten zum Erzielen des besten (erwarteten)
Ergebnisses (unter Berücksichtigung der verfügbaren Informationen)</p>
<p>Ein System ist rational, wenn es das seinen Kenntnissen nach &quot;Richtige&quot;
macht.</p>
</li>
<li>
<p>Denken ist nicht unbedingt notwendig (zb. Reflexe können auch rationales
Verhalten sein)</p>
</li>
<li>
<p>Interessant: &quot;richtige&quot; Handlung unter unvollständigen/unsicheren
Informationen</p>
</li>
</ul>
<div class="box notices cstyle info">
  <div class="box-label">
    <i class="fa-fw fas fa-info-circle"></i> Statistische KI
  </div>
  <div class="box-content">
<p>Die Schule der sogenannten &quot;<strong>Statistischen KI</strong>&quot; verfolgt einen Ansatz, der sich stark auf
statistische Methoden und Modelle stützt, um Muster in Daten zu erkennen und Entscheidungen zu
treffen, also um aus großen Datenmengen Erkenntnisse zu gewinnen und Vorhersagen zu treffen.</p>
<p>Aus der Analyse von Datenpunkten und deren Merkmalen werden Wahrscheinlichkeiten für bestimmte
Ereignisse berechnet, beispielsweise in Regressionsanalysen, Klassifizierungsverfahren oder
Zeitreihenanalysen.</p>
<p>Diese Tradition spielt eine zentrale Rolle in zahlreichen Anwendungsbereichen wie
Gesundheitswesen, Finanzen, Marketing und vielen weiteren.</p>
  </div>
</div>
<h3 id="menschliches-verhalten-na-ja-sie-wissen-schon--">Menschliches Verhalten: Na ja, Sie wissen schon ;-)</h3>
<h2 id="modelle-lernen-und-vorhersagen">Modelle, Lernen und Vorhersagen</h2>
<p>In der Informatik allgemein und auch in der KI versuchen wir, Probleme der realen Welt mit
Hilfe von künstlichen Systemen (Algorithmen, Software) zu lösen. Dafür brauchen wir zunächst
ein abstraktes mathematisches <strong>Modell</strong> der Welt, in der wir uns bewegen. Das Modell sollte
alle für das zu lösende Problem relevanten Aspekte der Welt repräsentieren - und möglichst
nicht mehr als diese, um unnötige Komplexität zu vermeiden. Es kommt häufig vor, dass selbst
die relevanten Aspekte zu umfangreich oder teilweise sogar unbekannt sind und nicht
vollständig dargestellt werden können. Modelle stellen also eine Abstraktion der echten Welt
dar und sind <em>verlustbehaftet</em>. Es gibt viele verschiedene Modelle.</p>
<p><em>Beispiel</em>: Wir möchten von Bielefeld nach Minden fahren. Neben den offensichtlichen
Parametern (Womit wollen wir fahren? Wo genau ist der Startpunkt, wo genau der Zielpunkt? Wann
wollen wir fahren?) spielen in der realen Welt unendlich viele Aspekte eine Rolle: Farben,
Gerüche, Licht, Beschaffenheit der einzelnen Straßen, exakte Positionen auf der Straße/im Ort
... Sind diese wirklich relevant für dieses Problem? Am Ende wird es wichtig sein, eine
abstrakte Darstellung zu finden, die irgendwie die Städte und Dörfer repräsentiert und die
Verbindungen dazwischen. Und vermutlich muss ich wissen, wie lang die Strecken jeweils sind
(oder wie lange ich brauche oder wieviel Geld mich das Abfahren kostet). Es scheint also so zu
sein, dass eine Abstraktion des Problems als <em>Graph</em> sinnvoll ist: Die Knoten entsprechen den
Orten, die Kanten den Straßen (oder Bahnlinien o.ä.). An den Kanten sind Kosten annotiert
(Kilometer, Zeit, ...). Damit ignorieren wir die Komplexität der realen Welt und fokussieren
uns auf die Aspekte, die zur Lösung des Problems wichtig sind. Behalten Sie im Gedächtnis,
dass unser Modell verlustbehaftet ist und wir damit tatsächlich nur das Wegeproblem lösen
können! Wenn wir Wege vergessen haben oder falsch bewertet haben, wird unser Algorithmus
später möglicherweise nicht die gewünschte Lösung finden! Wir schauen uns das Thema
Modellierung am Beispiel des Problemlösens und insbesondere für Suchprobleme in der Lektion
<a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html">Problemlösen</a> noch genauer an.</p>
<p>Ein Modell kann Parameter haben. Im obigen Beispiel wären dies die Werte an den Kanten. Es
kann sein, dass diese Werte nicht im Vorfeld bekannt sind, sondern aus einem Datensatz
extrahiert werden müssen. Dies nennt man <strong>Lernen</strong>: Das Modell wird (besser gesagt: die
Parameter des Modells werden) an das Problem angepasst. Dafür gibt es unterschiedliche
Algorithmen. In der Regel benötigt man ein Ziel für den Adaptionsprozess: eine sogenannte
Ziel- oder Kostenfunktion. Anpassen der Modellparameter mit Hilfe von Daten und einer
Zielfunktion bedeutet auch, dass man das Ziel möglicherweise nie zu 100% erreicht, sondern nur
in die Nähe kommt. Je nach Problem kann man auch nur eine Modellfamilie vorgeben und den
konkreten Aufbau des Modells im Trainingsprozess erarbeiten lassen.</p>
<p><em>Wichtig</em>: Lernen bietet sich immer dann an, wenn eine analytische Lösung nicht möglich ist
(fehlende Informationen, Komplexität des Problems). Das bedeutet im Umkehrschluss aber auch:
Wenn eine analytische Lösung bekannt ist (oder zu finden ist), dann gibt es keinen Grund für
den Einsatz von adaptiven Systemen!</p>
<p>Mit dem Modell der Welt kann nun das Problem gelöst werden. Dazu wird das Modell mit Daten
versorgt (im obigen Beispiel: Start und Ziel) und ein passender Algorithmus kann auf dem
Modell die Lösung berechnen. Dies kann eine <strong>Vorhersage</strong> sein, welchen Weg ich nehmen soll,
wie lange es dauern wird, welchen Spielzug ich als nächstes machen sollte, ob in einem Bild
eine Katze zu sehen ist, ... Es könnte aber auch im Fall von sogenannten <em>generativen
Modellen</em> ein erzeugter Text oder ein erzeugtes Bild sein.</p>
<p><em>Hinweis</em>: In manchen Quellen wird dieser Vorgang auch &quot;Inferenz&quot; genannt. Da dieser Begriff
aus der Logik stammt und mit bestimmten Prozessen zur Schlussfolgerung verbunden ist, möchte
ich in diesem Skript diesen Begriff nicht für das Generieren einer Vorhersage nutzen.</p>
<h2 id="modellkomplexität">Modellkomplexität</h2>
<p>In der KI werden sehr unterschiedliche Modelle betrachtet, die auch eine sehr unterschiedliche
Komplexität aufweisen.</p>
<p>Besonders einfache Modelle sind Modelle, die für einen Input direkt einen Output berechnen.
Für jeden Input wird eine feste Berechnung durchgeführt, es erfolgt kein Backtracking und es
gibt keine (inneren) Zustände. Dies ähnelt dem reflexartigen Verhalten in biologischen
Vorbildern, weshalb diese Modell oft auch &quot;<em>reflex-based models</em>&quot; genannt werden. In diese
Kategorie fallen beispielsweise <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression.html">lineare Regression</a> und das
<a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn01-perceptron.html">Perzeptron</a>, aber auch Modelle mit vielen Parametern wie ein
<a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp.html">Multilagen-Perzeptron (MLP)</a> oder die daraus abgeleiteten Deep Neural
Networks.</p>
<p>In der nächst komplexeren Stufe haben die Modelle einen internen Zustand (&quot;<em>state-based
models</em>&quot;). Darüber wird ein Zustand der betrachteten Welt modelliert. Zwischen den Zuständen
gibt es Übergänge (sogenannte Aktionen), so dass sich hier ein Graph aufspannt (der sogenannte
Problemgraph, vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html">Problemlösen</a>). In diese Klasse fallen die
typischen Suchprobleme (wie <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Breitensuche</a>,
<a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html">Tiefensuche</a>, <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A*</a>), aber auch
<a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games.html">Spiele</a> mit Zufallskomponente oder mit gegnerischen Mitspielern.</p>
<p>Noch eine Stufe komplexer sind Modelle mit Variablen (&quot;<em>variable-based models</em>&quot;). Während es
bei den zustandsbasierten Modellen immer (auch) um den Weg zwischen den Zuständen geht und
damit um eine prozedurale Beschreibung, wie von einem Startzustand zu einem Zielzustand zu
gelangen ist, steht bei Modellen mit Variablen nur die Lösung im Vordergrund: Das Modell
enthält verschiedene Variablen, denen ein passender Wert aus einem Wertebereich zugeordnet
werden muss. Wie diese Belegung entsteht, ist am Ende nicht mehr so interessant. Denken Sie
beispielsweise an ein Sudoku oder die Erstellung eines Stundenplans. Die Variablen sind
entsprechend die einzelnen Felder, gesucht ist eine insgesamt korrekte Belegung aller Felder.
In diese Klasse fallen <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp.html">Constraint Satisfaction Probleme (CSP)</a>, aber auch
Bayes'sche Netze und die sogenannte <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html">&ldquo;lokale Suche&rdquo;</a>.</p>
<p>Auf der höchsten Komplexitätsstufe stehen logische Modelle. Hier wird Wissen über die Welt in
Form von Fakten und Regeln modelliert, und über eine entsprechende Anfrage wird daraus mit
Hilfe von formal definierten Beweisen eine korrekte Antwort generiert. Dies nennt man auch
&quot;<em>Inferenz</em>&quot;. Hier kommt beispielsweise das Prädikatenkalkül zum Einsatz oder die
Programmiersprache Prolog.</p>
<h2 id="kurzer-geschichtsüberblick----wichtigste-meilensteine">Kurzer Geschichtsüberblick -- Wichtigste Meilensteine</h2>
<ul>
<li>1943: McCulloch/Pitts: binäres Modell eines Neurons</li>
<li>1950: Turing-Test</li>
<li>1956: Dartmouth Workshop (Minsky, McCarthy, ...) -- McCarthy schlägt den Begriff &quot;Artificial Intelligence&quot; vor</li>
<li>1960er: Symbolische KI (Theorembeweiser), Blockwelt, LISP</li>
<li>1970er: Wissensbasierte System (Expertensysteme)</li>
<li>1980er: zunächst kommerzieller Erfolg der Expertensysteme, später
Niedergang (&quot;KI-Eiszeit&quot;); Zuwendung zu Neuronalen Netzen</li>
<li>1990er: Formalisierung der KI-Methoden, Einführung probabilistischer
Methoden (Bayes'sches Schließen), verstärkte mathematische Fundierung</li>
<li>heute: friedliche Koexistenz verschiedener Paradigmen und Methoden;
Rückkehr zu &quot;Human-Level AI&quot; (McCarthy, Minsky, Nilsson, Winston); Rückkehr
zu Neuronalen Netzen (CNN/RNN)</li>
</ul>
<p>Siehe auch Abbildung 1.3 in <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html#id_Ertel2017">[Ertel2017, S.8]</a> ...</p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Definition von &quot;Intelligenz&quot; nicht ganz einfach</li>
<li>Dimensionen: Denken vs. Verhalten, menschlich vs. rational</li>
<li>Ziele der KI: Verständnis menschlicher Fähigkeiten, Übertragen auf künstliche Systeme</li>
</ul>
<p>Schauen Sie sich zur Einführung auch gern das YouTube-Video
<a href="https://youtu.be/J8Eh7RqggsU" rel="external" target="_blank">Overview Artificial Intelligence Course | Stanford CS221</a>
an. (Vorsicht: Das ist recht lang.)</p>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106585&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest KI Einführung (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Kapitel 1: Introduction</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Kapitel 1: Introduction</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Problemlösen</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Um ein Problem lösen zu können, muss es zunächst geeignet dargestellt werden. In der KI
betrachten wir Zustände einer Welt, auf die Aktionen angewendet werden können und die die
betrachtete Welt in den/einen Folgezustand bringen. Hier muss unterschieden werden zwischen
deterministischen und stochastischen Welten, ebenso spielt die Beobachtbarkeit durch den
Agenten (die die Welt betrachtende und durch die Aktionen auf die Welt einwirkende Instanz)
eine Rolle: Kann der Agent die Welt komplett beobachten, nur einen Teil davon oder gar nichts?
Es spielt auch eine Rolle, ob es diskrete Zustände gibt, oder ob man mit kontinuierlichen
Variablen zu tun hat. Gibt es nur einen Agenten oder können mehrere Agenten beteiligt sein ...
(In dieser Veranstaltung werden wir uns auf deterministische und voll beobachtbare Welten
mit diskreten Zuständen konzentrieren.)</p>
<p>Dies alles muss bei der Modellierung betrachtet werden. Es empfiehlt sich, die Zustände und
die Aktionen so abstrakt wie möglich zu beschreiben. Anderenfalls kann später die Lösung des
Problems zumindest stark erschwert werden.</p>
<p>Durch das wiederholte Anwenden der Aktionen auf den Startzustand bzw. auf die sich daraus
ergebenden Folgezustände wird der Zustandsraum aufgebaut. Dabei ist zu beachten, dass Aktionen
Vorbedingungen haben können, d.h. unter Umständen nicht auf alle Zustände angewendet werden
können. Die entstehende Struktur (Zustandsraum) kann man formal als Graph repräsentieren: Die
Zustände werden durch die Knoten und die Aktionen als (gerichtete) Kanten im Graph dargestellt
(=&gt; Problemgraph).</p>
<p>Das Problemlösen entspricht nun einer Suche im Problemgraphen: Man sucht einen Weg von einem
Startknoten zu einem Zielknoten, d.h. eine Folge von Aktionen, die den Start- in den Zielzustand
überführen. Der Weg entspricht dann der Lösung des Problems. Normalerweise will man eine bestimmte
Qualität der Lösungen haben: Es sollen die kürzesten Wege (also die mit den wenigsten
Zwischenstationen/Knoten) oder die billigsten Wege (die Summe der an den Kanten annotierten Gewichte
soll minimal sein) gefunden werden.</p>
<p>Zur Suche kann man bei den in dieser Veranstaltung betrachteten deterministischen Problemen mit
diskreten Zuständen den einfachen &quot;Tree-Search&quot;-Algorithmus (Benennung in Anlehnung an <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#id_Russell2020">[Russell2020]</a>)
einsetzen, der allerdings Wiederholungen und Schleifen zulässt. Mit zwei Erweiterungen wird daraus
der &quot;Graph-Search&quot;-Algorithmus (Benennung in Anlehnung an <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#id_Russell2020">[Russell2020]</a>), der die wiederholte
Untersuchung von bereits besuchten Knoten vermeidet. In beiden Algorithmen wird eine zentrale
Datenstruktur eingesetzt (im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#id_Russell2020">[Russell2020]</a> auch &quot;Frontier&quot; genannt), die die als Nächstes zu
untersuchenden Knoten hält und die damit die Grenze zwischen dem bereits untersuchten Teil des Graphen
und dem unbekannten Teil des Graphen bildet. Je nach Art der Datenstruktur und je nach den betrachteten
Kosten ergeben sich eine Reihe unterschiedlicher Suchalgorithmen, die wir in einer späteren Sitzung
betrachten.</p>
<p>Die Suchverfahren können im Hinblick auf Optimalität, Vollständigkeit und Komplexität beurteilt
werden.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/IhUmUUzR9lQ' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Problemlösen</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/6754160d1977fb73c1d7f2052a1153fd073dc151853d4143c6f84622ffdb785baa6205e877ad448e1ba00fddd1c6528615c9dcb83528d32015e30a04201028f9' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Problemlösen</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Definition Problem: Begriffe Zustand, Aktion, Zustandsraum, Problemgraph, Suchbaum</li> <li>(K2) Problemlösen als Suche nach Wegen im Problemgraph =&gt; Suchbaum</li> <li>(K2) Unterschied zw. Tree-Search und Graph-Search</li></ul>
  </div>
</div>




    <h2 id="motivation-roboter-in-einer-bibliothek">Motivation: Roboter in einer Bibliothek</h2>
<p><a href="#R-image-35a49c923707cc9f323e47e9093b67b5" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving/problem.png?width=40%25&height=auto" style=" height: auto; width: 40%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-35a49c923707cc9f323e47e9093b67b5"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving/problem.png?width=40%25&height=auto"></a></p>
<div class='columns'>
<div class='column'>
<p><strong>Aktionen:</strong></p>
<ul>
<li>Right (R)</li>
<li>Left (L)</li>
<li>Take (T)</li>
<li>Drop (D)</li>
</ul>
</div>
<div class='column'>
<p><strong>Wahrnehmungen:</strong></p>
<ul>
<li>In welchem Raum bin ich?</li>
<li>Habe ich das Buch?</li>
</ul>
</div>
</div>
<p><strong>Aufgabe:</strong> Das Buch aus der Bibliothek holen und in den Kopiererraum bringen.</p>
<p>Bemerkungen zur Umwelt:</p>
<ul>
<li>Beobachtbarkeit der Umwelt kann variieren: &quot;voll beobachtbar&quot; bis zu &quot;unbeobachtbar&quot;</li>
<li>Umwelt kann &quot;deterministisch&quot; oder &quot;stochastisch&quot; sein: Führt eine Aktion in
einem Zustand immer zum selben Folgezustand?</li>
<li>Wann erfolgt die Rückmeldung an den Agenten über die Auswirkung der
Aktionen? Sofort (&quot;sequentiell&quot;) oder erst am Ende einer Aktionsfolge
(&quot;episodisch&quot;)?</li>
<li>Wird die Umwelt nur durch die Aktionen des Agenten verändert (&quot;statisch&quot;)?
Oder verändert sich die Umwelt zwischen den Aktionen eines Agenten,
beispielsweise durch andere Agenten (&quot;dynamisch&quot;)?</li>
<li>Gibt es diskrete Zustände (wie im Beispiel)?</li>
</ul>
<h2 id="zustände-der-bibliotheks-welt">Zustände der Bibliotheks-Welt</h2>
<p><a href="#R-image-059ba5327fdda7d73086dd07ed76d7f7" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving/states.png?width=55%25&height=auto" style=" height: auto; width: 55%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-059ba5327fdda7d73086dd07ed76d7f7"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving/states.png?width=55%25&height=auto"></a></p>
<p><strong>Problem:</strong> Gegeben einen Startzustand, wie komme ich zum Ziel?</p>
<ul>
<li>Welche Aktionen können in einem Zustand (zb. Nr. 4) angewendet werden?</li>
<li>Welche Aktionen können in den Folgezuständen angewendet werden?</li>
</ul>
<p>Ergebnis:</p>
<ul>
<li>Zustandsraum: Menge aller von den Startzuständen aus erreichbaren Zustände</li>
<li>Problemgraph: Repräsentation der Zustände und Aktionen als Knoten und (gerichtete) Kanten</li>
</ul>
<h2 id="suche-im-problemgraphen">Suche im Problemgraphen</h2>
<p><a href="#R-image-b0cec68baafc61bb0eb639121c992ff1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving/state-space.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-b0cec68baafc61bb0eb639121c992ff1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving/state-space.png?width=60%25&height=auto"></a></p>
<ul>
<li>Durch die Suche im Problemgraphen wird ein Suchbaum aufgespannt</li>
<li>Varianten: Zustände können in einem Pfad wiederholt vorkommen vs.
Wiederholungen werden ausgeschlossen</li>
</ul>
<h2 id="definition-zustand-und-aktion">Definition Zustand und Aktion</h2>
<dl>
<dt><strong>Zustand:</strong></dt>
<dd>(Formale) Beschreibung eines Zustandes der Welt</dd>
<dt><strong>Aktion:</strong></dt>
<dd>
<p>(Formale) Beschreibung einer durch Agenten ausführbaren Aktion</p>
<ul>
<li>Anwendbar auf bestimmte Zustände</li>
<li>Überführt Welt in neuen Zustand (&quot;Nachfolge-Zustand&quot;)</li>
</ul>
</dd>
</dl>
<p><span class='alert'><strong>Geeignete Abstraktionen wählen für Zustände und Aktionen!</strong></span></p>
<p><strong>Anmerkung:</strong>
<a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020]</a> unterscheidet zw. Aktionen und Transitionsmodell; hier nur Aktionen!
D.h. die Aktionen und das Übergangsmodell aus dem <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020]</a> werden direkt zusammen
betrachtet. Bei den hier diskutierten Problemen ist das ohne Nachteile möglich, es
wird lediglich etwas Flexibilität genommen bzw. Komplexität vermieden (je nach Sichtweise :-) ...</p>
<h2 id="definition-problem">Definition Problem</h2>
<p>Ein Problem besteht aus:</p>
<dl>
<dt><strong>Startzustände</strong></dt>
<dd>Menge <span class="math align-center">$S_A \subset S$</span></dd>
<dt><strong>Aktionen</strong></dt>
<dd>Menge von Funktionen <span class="math align-center">$\operatorname{op}: S \to S$</span></dd>
<dt><strong>Zustandsraum</strong></dt>
<dd>Menge aller Zustände <span class="math align-center">$S$</span>, die durch (wiederholte) Anwendung von Aktionen
von den Startzuständen aus erreichbar sind</dd>
<dt><strong>Zieltest</strong></dt>
<dd>Funktion <span class="math align-center">$\operatorname{goal}: S \to \{0,1\}$</span></dd>
<dt><strong>Zielzustände</strong></dt>
<dd>Menge <span class="math align-center">$S_E \subseteq S$</span> mit <span class="math align-center">$\forall x \in S_E : \operatorname{goal}(x)=1$</span></dd>
<dt><strong>Kosten</strong></dt>
<dd>
<p><strong>Gesamtkosten</strong>: <span class="math align-center">$f(n) = g(n) + h(n)$</span></p>
<ul>
<li><span class="math align-center">$n \in S$</span> auf dem aktuellen Weg erreichter Knoten</li>
<li><span class="math align-center">$g(n)$</span> tatsächliche Kosten für den Weg vom Start bis zu Knoten <span class="math align-center">$n$</span></li>
<li><span class="math align-center">$h(n)$</span> geschätzte Restkosten für den Weg von Knoten <span class="math align-center">$n$</span> zum Ziel</li>
</ul>
</dd>
</dl>
<h2 id="hinweis-unterschied-zustand-und-knoten-bzw-zustandsraum-und-problemgraph">Hinweis: Unterschied Zustand und Knoten bzw. Zustandsraum und Problemgraph</h2>
<ul>
<li>Zustände und Aktionen kann man als einen Graph darstellen: <strong>Problemgraph</strong>
<ul>
<li>Zustände werden als Knoten im Graphen abgebildet</li>
<li>Aktionen werden als (gerichtete) Kanten im Graphen abgebildet</li>
</ul>
</li>
<li>Unterscheidung &quot;Zustand&quot; und &quot;Knoten&quot;:
<ul>
<li>Zustand: Beschreibung/Modellierung eines Zustandes der Welt</li>
<li>Knoten: Datenstruktur, Bestandteil des Graphen,
<em>symbolisiert</em> einen Zustand</li>
</ul>
</li>
</ul>
<p>Das bedeutet, dass der Problemgraph eine Repräsentation des Zustandsraumes ist.</p>
<p>Die beiden Begriffe werden normalerweise synonym verwendet, sofern eindeutig ist,
was gemeint ist.</p>
<h2 id="definition-problemlösen">Definition Problemlösen</h2>
<dl>
<dt>Problemlösen</dt>
<dd>
<p>Wegesuche im Graph vom Startknoten zu einem Zielknoten</p>
<ul>
<li>Spannt den <strong>Suchbaum</strong> auf</li>
</ul>
</dd>
<dt><strong>Lösung</strong></dt>
<dd>
<p>Folge von Aktionen, die Start- in Zielzustand überführen</p>
<p>Ergebnis des Problemlösens</p>
</dd>
</dl>
<h2 id="suche-einfache-basisvariante">Suche: Einfache Basisvariante</h2>
<ol>
<li>Füge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein</li>
<li>Entnehme Knoten aus der Datenstruktur:
<ul>
<li>Knoten ist gesuchtes Element: Abbruch, melde &quot;<em>gefunden</em>&quot;</li>
<li>Expandiere alle Nachfolger des Knotens und füge diese in die
Datenstruktur ein</li>
</ul>
</li>
<li>Falls die Datenstruktur leer ist: Abbruch, melde &quot;<em>nicht gefunden</em>&quot;</li>
<li>Gehe zu Schritt 2</li>
</ol>
<p>Für die in dieser Veranstaltung betrachteten deterministischen Probleme mit
diskreten Zuständen ist diese Basisvariante der Suche eine Art generischer
Suchalgorithmus: Durch die Variation der eingesetzten Datenstruktur und durch
die Betrachtung unterschiedlicher Kosten erhält man die in den nächsten
Sitzungen betrachteten verschiedenen klassischen Suchalgorithmen.</p>
<p><strong>Anmerkung</strong>: Für Handsimulation besserer Überblick, wenn statt der Knoten
immer <strong>partielle Wege</strong> in Datenstruktur gespeichert werden!</p>
<p><strong>Anmerkung</strong>: Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020, Abschnitt 3.3.3, S.92]</a> wird ein Algorithmus
mit den vorgestellten Eigenschaften als &quot;<strong>tree-like search</strong>&quot; bezeichnet.
In Anlehnung an <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020]</a> wird diese Basisvariante der Suche in dieser
Lehrveranstaltung kurz als &quot;Tree-Search&quot;-Algorithmus bezeichnet.</p>
<p><strong>Anmerkung</strong>: Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020]</a> wird für die Datenstruktur, mit der die
Suche arbeitet, auch &quot;<em>Frontier</em>&quot; genannt. Hier werden alle Knoten gehalten,
die in einem der nächsten Schritte betrachtet werden sollen, d.h. diese Knoten
bilden die Grenze zwischen dem bereits untersuchten Teil des Graphen und dem
noch unbekannten Teil des Graphen (deshalb auch &quot;Frontier&quot;).</p>
<h2 id="erweiterung-der-suche-vermeiden-von-wiederholungen">Erweiterung der Suche: Vermeiden von Wiederholungen</h2>
<ol>
<li>Füge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein</li>
<li>Entnehme Knoten aus der Datenstruktur:
<ul>
<li>Knoten ist gesuchtes Element: Abbruch, melde &quot;<em>gefunden</em>&quot;</li>
<li>Markiere aktuellen Knoten, und</li>
<li>Expandiere alle Nachfolger des Knotens und füge alle unmarkierten
Nachfolger, die noch nicht in der Datenstruktur sind, in die
Datenstruktur ein</li>
</ul>
</li>
<li>Falls die Datenstruktur leer ist: Abbruch, melde &quot;<em>nicht gefunden</em>&quot;</li>
<li>Gehe zu Schritt 2</li>
</ol>
<p>Dieser Algorithmus ist eine Erweiterung der einfachen Basisvariante der
Suche:</p>
<ol>
<li>Man markiert bereits besuchte (expandierte) Knoten und besucht diese nie wieder
(man würde diese bei einer Expansion nicht wieder in die Datenstruktur aufnehmen).</li>
<li>Außerdem vermeidet man, dass ein Knoten mehrfach in der Datenstruktur vorkommt: Dies
würde bedeuten, dass man hier verschiedene Wege vom Start zu diesem Knoten in der
Datenstruktur hat, die dann auch alle weiter untersucht werden müssten. In der Regel
reicht aber ein Weg vom Start zu einem Zwischenknoten (meist wird der kürzeste
genommen, dazu in einer späteren Sitzung mehr).</li>
</ol>
<p><strong>Anmerkung</strong>: Für Handsimulation besserer Überblick, wenn statt der Knoten
immer <strong>partielle Wege</strong> in Datenstruktur gespeichert werden!</p>
<p><strong>Anmerkung</strong>: Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020, Abschnitt 3.3.3, S.92]</a> wird ein Algorithmus
mit den vorgestellten Eigenschaften als &quot;<strong>graph search</strong>&quot; bezeichnet.
In Anlehnung an <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html#id_Russell2020">[Russell2020]</a> wird diese erweiterter Variante der Suche in dieser
Lehrveranstaltung kurz als &quot;Graph-Search&quot;-Algorithmus bezeichnet.</p>
<h2 id="bewertung-von-suchalgorithmen">Bewertung von Suchalgorithmen</h2>
<dl>
<dt><strong>Vollständigkeit</strong></dt>
<dd>Findet der Algorithmus eine Lösung, wenn es eine gibt?</dd>
<dt><strong>Optimalität</strong></dt>
<dd>Findet der Algorithmus die beste Lösung?</dd>
<dt><strong>Zeitkomplexität</strong></dt>
<dd>Wie lange dauert es eine Lösung zu finden?</dd>
<dt><strong>Speicherkomplexität</strong></dt>
<dd>Wieviel Speicher benötigt die Suche?</dd>
</dl>
<p><strong>Größen zur Bewertung:</strong></p>
<ul>
<li><strong>b</strong>: Verzweigungsfaktor</li>
<li><strong>d</strong>: Ebene (Tiefe) des höchsten Lösungsknotens</li>
<li><strong>m</strong>: Länge des längsten Pfades</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>
<p>Begriffe &quot;Problem&quot;, &quot;Zustand&quot;, &quot;Aktion&quot;, &quot;Zustandsraum&quot;, &quot;Problemgraph&quot;, &quot;Suchbaum&quot;</p>
</li>
<li>
<p>Problemlösen: Suche in Graphen nach Weg vom Start zum Ziel</p>
<ul>
<li>Suche spannt einen Suchbaum auf</li>
<li>Unterschiedliche Kostenfunktionen möglich</li>
<li>Suchalgorithmen: Einfache Basisvariante, Erweiterung mit Vermeidung von Redundanzen</li>
<li>Beurteilung der Suchverfahren: Optimalität, Vollständigkeit, Komplexität</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106586&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Problemlösen (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p>Drei Elben und drei Orks befinden sich an einem Ufer eines Flusses und wollen diesen überqueren. Es steht dazu ein Pferd zur Verfügung, welches maximal zwei Wesen tragen kann. Das Pferd kann den Fluss nicht allein überqueren.</p>
<p>Gesucht ist eine Möglichkeit, alle Elben und Orks über den Fluss zu bringen. Dabei darf zu keiner Zeit an keinem Ufer die Anzahl der sich dort befindlichen Orks größer sein als die der dort wartenden Elben, da es sonst zu Konflikten zwischen beiden Gruppen kommt.</p>
<ol>
<li>Formalisieren Sie das Problem (Zustände, Aktionen, Start- und Endzustand).</li>
<li>Skizzieren Sie den Problemgraph.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Problemlösen: Kapitel 3.1 - 3.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="entscheidungsbäume-decision-tree-learner---dtl">Entscheidungsbäume (Decision Tree Learner - DTL)</h1>

<p>Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden)
Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum
dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es
eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern
sind die Klassen zugeordnet.</p>
<p>Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für
jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang
aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt,
d.h. üblicherweise die Klasse.</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html">Machine Learning 101</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2.html">CAL2</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl3-pruning.html">Pruning</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl4-cal3.html">CAL3</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy.html">Entropie</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html">ID3 und C4.5</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Entscheidungsbäume (Decision Tree Learner - DTL)</h1>
<article class="default">
<h1>Machine Learning 101</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Lernen wird in der KI oft als Verhaltensänderung (eines Systems) aufgefasst. Dabei soll eine
Gütefunktion optimiert werden.</p>
<p>Je nach verfügbarem Feedback eines &quot;Lehrers&quot; werden typischerweise drei Arten von Lernen
unterschieden: Überwachtes Lernen, Unüberwachtes Lernen, Reinforcement Lernen. Dabei stellt
der Lehrer beim überwachten Lernen Trainingsbeispiele plus eine Vorgabe (Klasse, Funktionswert)
zur Verfügung, während beim unüberwachten Lernen nur die Trainingsbeispiele bereitgestellt
werden und der Algorithmus selbst Zusammenhänge in den Daten erkennen soll. Beim Reinforcement
Learning erfolgt das Feedback am Ende einer Kette von Aktionen, d.h. der Algorithmus muss
diese Bewertung auf die einzelnen Aktionen zurückrechnen.</p>
<p>Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden)
Funktion möglichst nahe kommt. Eine konsistente Hypothese erklärt die Trainingsdaten, eine
generalisierende Hypothese kann auch unbekannte Daten (die aus der selben Quelle stammen, also
zum selben Problem gehören) korrekt bewerten. Es wird unterschieden zwischen Klassifikation
(einige wenige diskrete Label/Klassen, die den Trainingsbeispielen zugeordnet sind) und
Regression (Lernen eines Funktionsverlaufs).</p>
<p>Merkmalsvektoren gruppieren Eigenschaften des Problems bzw. der Objekte, d.h. jedes Objekt
kann über einen Merkmalsvektor beschrieben werden. Trainingsdaten sind ausgewählte Beispielobjekte
(durch Merkmalsvektoren beschrieben) plus die Vorgabe (Klasse oder Funktionswert) vom Lehrer.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/FliWEXQZhsw' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Machine Learning 101</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/c871a589c1e95782173b9a3c1efbb79ac38dfb4d871df8b39fd5851562036619df4d6b31cec0b630c1c88ff928bbcb2429ea194e2cc795720fce214367009242' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Machine Learning 101</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K1) Definition und Arten des Lernens</li> <li>(K2) Überwachtes Lernen: Lernen durch Beobachten (mit Lehrer)</li> <li>(K2) Merkmalsvektoren, Eigenschaften, Ausprägung, Objekte, Trainingsmenge</li></ul>
  </div>
</div>




    <h2 id="was-ist-lernen">Was ist Lernen?</h2>
<blockquote>
<p>Verhaltensänderung eines Agenten in Richtung der Optimierung eines
Gütefunktionals (Bewertungsfunktion) durch Erfahrung.</p>
</blockquote>
<h2 id="warum-lernen">Warum Lernen?</h2>
<ul>
<li>Nicht alle Situationen vorhersehbar</li>
<li>Nicht alle Details modellierbar</li>
<li>Lösung oder Lösungsweg unbekannt, nicht explizit programmierbar</li>
<li>Data Mining: Entdeckung neuen Wissens durch Analyse der Daten</li>
<li>Selbstanpassende Programme</li>
</ul>
<p>=&gt; Lernen wichtige Eigenschaft lebender Wesen :-)</p>
<h2 id="learning-agent">Learning Agent</h2>
<p><a href="#R-image-082c7ca01e066e40bb5f192e6d48d8e8" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/learning.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-082c7ca01e066e40bb5f192e6d48d8e8"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/learning.png?width=80%25&height=auto"></a></p>
<h2 id="feedback-während-des-lernens">Feedback während des Lernens</h2>
<ul>
<li>
<p><strong>Überwachtes Lernen</strong></p>
<ul>
<li>Lernen durch Beobachtung</li>
<li>Vorgabe von Beispielen: Ein- und Ausgabewerte</li>
</ul>
<p>=&gt; Regression, <span class='alert'>Klassifikation</span></p>
</li>
<li>
<p><strong>Unüberwachtes Lernen</strong></p>
<ul>
<li>Erkennen von Mustern in den Inputdaten, Clustering</li>
<li>Kein Feedback (!)</li>
</ul>
</li>
<li>
<p><strong>Reinforcement Lernen</strong></p>
<ul>
<li>Bewertung der Aktionen des Agenten am Ende einer Aktionsfolge</li>
</ul>
</li>
</ul>
<p><strong>Beispiel Kleinkind</strong>: Lernen von Klassen/Konzepten durch Beispiele</p>
<ul>
<li>Zuerst ist alles &quot;Katze&quot; (Übergeneralisierung)</li>
<li>Differenzierung durch Feedback der Umwelt; Erkennung unterschiedlicher Ausprägungen</li>
</ul>
<h2 id="beispiel-kreditrisiko">Beispiel: Kreditrisiko</h2>
<ul>
<li>
<p>Bankkunde beantragt Kredit</p>
</li>
<li>
<p>Soll er aus Sicht der Bank den Kredit bekommen?</p>
</li>
<li>
<p>Bankangestellter betrachtet (relevante) Merkmale des Kunden:</p>
<ul>
<li>Alter, Einkommen, sozialer Status</li>
<li>Kundenhistorie bei der Bank</li>
<li>Höhe des Kredits</li>
</ul>
</li>
<li>
<p>Bewertung des Kreditrisikos:</p>
<ul>
<li><strong>Klassifikation</strong>: Guter oder schlechter Kunde (Binäre Entscheidung: 2 Klassen)</li>
<li><strong>Regression</strong>: Vorhersage Gewinn/Verlust für die Bank (Höhe des Gewinns/Verlusts interessant)</li>
</ul>
</li>
</ul>
<h2 id="beispiel-autoreparatur">Beispiel: Autoreparatur</h2>
<ul>
<li>
<p><strong>Gegeben</strong>: Eigenschaften eines Autos</p>
<p>=&gt; Eigenschaften: Ausprägungen der Merkmale</p>
</li>
<li>
<p><strong>Gesucht</strong>: Diagnose und Reparaturanleitung</p>
<p>=&gt; Hypothese über den Merkmalen (Funktion <span class="math align-center">$\operatorname{h}$</span>)</p>
</li>
</ul>
<h2 id="lernen-durch-beobachten-lernen-einer-funktion-hahahugoshortcode11s1hbhb">Lernen durch Beobachten: Lernen einer Funktion <span class="math align-center">$\operatorname{f}$</span></h2>
<p>Funktionsapproximation: Lernen einer Funktion <span class="math align-center">$\operatorname{f}$</span> anhand von Beispielen</p>
<ul>
<li>
<p>Ein Beispiel ist ein Tupel <span class="math align-center">$(\mathbf{x}, \operatorname{f}(\mathbf{x}))$</span>, etwa
<span class="math align-center">$$
    (\mathbf{x}, \operatorname{f}(\mathbf{x})) = \left(\begin{array}{ccc}
    O & O & X \\
    . & X & . \\
    X & . & .
    \end{array}, +1\right)
    $$</span></p>
</li>
<li>
<p>Aufgabe: Baue Hypothese <span class="math align-center">$\operatorname{h}$</span> auf, so dass <span class="math align-center">$\operatorname{h} \approx \operatorname{f}$</span>.</p>
<ul>
<li>Benutze dazu Menge von Beispielen =&gt; <span class='alert'><strong>Trainingsdaten</strong></span>.</li>
</ul>
</li>
<li>
<p>Ziele:</p>
<ol>
<li><strong>Konsistente Hypothese</strong>: Übereinstimmung bei Trainingsdaten</li>
<li><strong>Generalisierende Hypothese</strong>: Korrekte Vorhersage bei
unbekannten Daten</li>
</ol>
</li>
</ul>
<p><em>Anmerkung</em>: Stark vereinfachtes Modell realen Lernens!</p>
<h2 id="konstruieren-einer-konsistenten-hypothese">Konstruieren einer konsistenten Hypothese</h2>
<p><a href="#R-image-584a2037a65d4722554ab77ee7bfc3d1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams1.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-584a2037a65d4722554ab77ee7bfc3d1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams1.png?width=60%25&height=auto"></a></p>
<p>Welcher Zusammenhang ist hier dargestellt? Offenbar eine Art Funktionsverlauf ...
Wir haben für einige x-Werte die zugehörigen y-Werte vorgegeben.</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-653581bb61aacf184839f27aa50e1248" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams2.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-653581bb61aacf184839f27aa50e1248"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams2.png?width=60%25&height=auto"></a></p>
<p>Die einfachste Approximation wäre eine lineare Funktion. Allerdings werden hierbei
einige Werte mehr oder weniger stark nicht korrekt widergegeben, d.h. man hat einen
relativ hohen (Trainings-) Fehler.</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt-1">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-f0ee7a7e616e9eb2a1516292b46a18e1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams3.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f0ee7a7e616e9eb2a1516292b46a18e1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams3.png?width=60%25&height=auto"></a></p>
<p>Die Hyperbel erklärt die Trainingsdaten bis auf den einen Punkt sehr gut.
Die Frage ist, ob dieser eine Punkt zum zu lernenden Zusammenhang gehört
oder ein Ausreißer ist, den man gefahrlos ignorieren kann?</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt-2">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-de83e68a19d2a3cef14469df05ab7536" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams4.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-de83e68a19d2a3cef14469df05ab7536"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams4.png?width=60%25&height=auto"></a></p>
<p>Die grüne Hypothese ist von allen bisher gezeigten die komplexeste, erklärt
aber alle Datenpunkte. D.h. hier wäre der Trainingsfehler Null. Zwischen den
Trainingsdaten zeigt das Modell eine &quot;glatte&quot; Approximation, d.h. es wird auch
neue Daten, die es beim Training nicht gesehen hat, relativ gut erklären.
(Dabei liegt freilich die Annahme zugrunde, dass alle relevanten Daten in der
Trainingsmenge vorhanden sind, d.h. dass es insbesondere zwischen den Datenpunkten
keine Ausreißer o.ä. gibt.)</p>
<h2 id="konstruieren-einer-konsistenten-hypothese-cnt-3">Konstruieren einer konsistenten Hypothese (cnt.)</h2>
<p><a href="#R-image-e1156df8090325306925a290899f72c1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams5.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e1156df8090325306925a290899f72c1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics/occams5.png?width=60%25&height=auto"></a></p>
<p>Diese Hypothese erklärt ebenfalls sämtliche Trainingsdaten. Allerdings schwingt
die Funktion zwischen den Daten stark hin und her. Vermutlich entspricht dies
nicht dem zu lernenden Funktionsverlauf. Der Trainingsfehler wäre wie bei der
deutlich einfacheren Hypthese aus dem letzten Schritt Null. Der Generalisierungsfehler
(sprich die Abweichung, wenn man das Modell nach Daten zwischen den Trainingspunkten
fragt) dürfte erheblich höher liegen.</p>
<p>D.h. hier hat das Modell einfach die Trainingsdaten auswendig gelernt, aber nicht
den Zusammenhang zwischen den Daten! Dies ist in der Regel unerwünscht!</p>
<h2 id="occams-razor">Occam's Razor</h2>
<p><strong>Bevorzuge die einfachste konsistente Hypothese!</strong></p>
<ol>
<li>Wenn es mehrere mögliche Erklärungen für einen Sachverhalt gibt, ist die
einfachste Erklärung allen anderen vorzuziehen.</li>
<li>Eine Erklärung ist &quot;einfach&quot;, wenn sie möglichst wenige Variablen und
Annahmen enthält und wenn diese in klaren logischen Beziehungen zueinander
stehen, aus denen der zu erklärende Sachverhalt logisch folgt.</li>
</ol>
<h2 id="trainingsdaten-und-merkmalsvektoren">Trainingsdaten und Merkmalsvektoren</h2>
<p>Lehrer gibt Beispiele vor: Eingabe <span class="math align-center">$\mathbf{x}$</span> und passende Ausgabe <span class="math align-center">$\operatorname{f}(\mathbf{x})$</span></p>
<ul>
<li>
<p>Ausgabe: typischerweise Skalar (Funktionswert oder Klasse)
=&gt; Beispiel: Bewertung eines Spielstandes bei TicTacToe</p>
</li>
<li>
<p>Eingabe: (Beschreibung des) Objekt(s) oder Situation, die zur Ausgabe gehört
=&gt; Beispiel: Spielstand bei TicTacToe</p>
</li>
</ul>
<p><strong>Merkmalsvektoren</strong>:</p>
<ul>
<li>Zusammenfassen der <span class='alert'>relevanten Merkmale</span> zu <span class='alert'>Vektoren</span></li>
</ul>
<h2 id="beispiel-schwimmen-im-see">Beispiel: Schwimmen im See</h2>
<p>Beschreibung der Faktoren, wann ich im See schwimmen möchte:</p>
<ol>
<li>Scheint die Sonne?</li>
<li>Wie warm ist das Wasser?</li>
<li>Wie warm ist die Luft?</li>
</ol>
<ul>
<li>Trainingsbeispiel:
<ul>
<li>Eingabe: Merkmalsvektor <code>(sonnig, warm, warm)</code></li>
<li>Ausgabe: Klasse <code>ja</code></li>
</ul>
</li>
</ul>
<p>Dabei wird davon ausgegangen, dass jeder Faktor (jedes Merkmal) an einer bestimmten
Stelle im Merkmalsvektor aufgeführt ist. Beispielsweise gehört das <code>sonnig</code> zur
Frage &quot;Scheint die Sonne&quot;, <code>warm</code> jeweils zur Wasser- und zur Lufttemperatur.</p>
<p>Damit hat man in einem Vektor eine Situation komplett beschrieben, d.h. einen Zustand
der Welt mit den relevanten Dingen beschrieben. Diesem Zustand kann man beispielsweise
ein Label (Klasse) verpassen, hier in diesem Fall &quot;ja, in dieser Welt möchte ich
schwimmen&quot;.</p>
<p>Die Trainingsmenge baut sich dann beim überwachten Lernen aus vielen solcher Paare
(Merkmalsvektor, Klasse) auf, und die Algorithmen sollen diese Zuordnung lernen, d.h.
ein Modell für diese Daten erzeugen, welches die Daten gut erklärt und darüber hinaus
für neue Daten aus der selben Datenquelle gute Vorhersagen macht.</p>
<h2 id="trainingsdaten----merkmalsvektoren">Trainingsdaten -- Merkmalsvektoren</h2>
<p><strong>Generell</strong>: Merkmalsvektor für Objekt <span class="math align-center">$v$</span>:
<span class="math align-center">$$
    \mathbf{x}(v) = (x_1, x_2, \ldots, x_n)
$$</span></p>
<ul>
<li><span class="math align-center">$n$</span> Merkmale (Attribute)</li>
<li>Attribut <span class="math align-center">$x_t$</span> hat <span class="math align-center">$m_t$</span> mögliche Ausprägungen</li>
<li>Ausprägung von <span class="math align-center">$v$</span> bzgl. <span class="math align-center">$x_t$</span>: <span class="math align-center">$\quad x_t(v) = i \quad$</span> (mit <span class="math align-center">$i = 1 \ldots m_t$</span>)</li>
</ul>
<p><em>Anmerkung</em>: Stellen Sie sich den Merkmalsvektor vielleicht wie
einen Konstruktor einer Klasse <code>x</code> vor: Die einzelnen Attribute <span class="math align-center">$x_t$</span> sind
die Parameter, aus denen der Merkmalsvektor aufgebaut ist/wird. Jedes der
Attribute hat einen Typ und damit eine bestimmte Anzahl erlaubter Werte
(&quot;Ausprägungen&quot;) ...</p>
<p><strong>Trainingsbeispiel</strong>:</p>
<ul>
<li>Tupel aus Merkmalsvektor und zugehöriger Klasse: <span class="math align-center">$\left(\mathbf{x}(v), k\right)$</span></li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>
<p>Lernen ist Verhaltensänderung, Ziel: Optimierung einer Gütefunktion</p>
<ul>
<li>Aufbau einer Hypothese, die beobachtete Daten erklären soll</li>
<li>Arten: Überwachtes Lernen, Unüberwachtes Lernen, Reinforcement Lernen</li>
</ul>
</li>
<li>
<p>Merkmalsvektoren gruppieren Eigenschaften des Problems bzw. der Objekte</p>
</li>
<li>
<p>Trainingsdaten: Beispielobjekte (durch Merkmalsvektoren beschrieben) plus Vorgabe vom Lehrer</p>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106589&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Intro ML (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Modellierung</strong></p>
<p>Sie stehen vor der Entscheidung, ob Sie sich zur Vorbereitung auf die
Flipped-Classroom-Sitzung noch das Skript anschauen. Welche Attribute
benötigen Sie, um die Situation zu beschreiben?</p>
<p><strong>Metriken für Klassifikatoren</strong></p>
<p>Es ist wieder Wahlkampf: Zwei Kandidaten O und M bewerben sich um die Kanzlerschaft. Die folgende Tabelle zeigt die Präferenzen von sieben Wählern.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left">Alter</th>
          <th style="text-align: left">Einkommen</th>
          <th style="text-align: left">Bildung</th>
          <th style="text-align: left">Kandidat</th>
          <th style="text-align: left">Vorhersage</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Abitur</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">O</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left"><span class="math align-center">$< 35$</span></td>
          <td style="text-align: left">niedrig</td>
          <td style="text-align: left">Master</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">O</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Bachelor</td>
          <td style="text-align: left">M</td>
          <td style="text-align: left">M</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">niedrig</td>
          <td style="text-align: left">Abitur</td>
          <td style="text-align: left">M</td>
          <td style="text-align: left">M</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left"><span class="math align-center">$\ge 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Master</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">O</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left"><span class="math align-center">$< 35$</span></td>
          <td style="text-align: left">hoch</td>
          <td style="text-align: left">Bachelor</td>
          <td style="text-align: left">O</td>
          <td style="text-align: left">M</td>
      </tr>
      <tr>
          <td style="text-align: left">7</td>
          <td style="text-align: left"><span class="math align-center">$< 35$</span></td>
          <td style="text-align: left">niedrig</td>
          <td style="text-align: left">Abitur</td>
          <td style="text-align: left">M</td>
          <td style="text-align: left">O</td>
      </tr>
  </tbody>
</table>
<p>Auf diesem Datensatz wurde ein Klassifikator trainiert, die Trainingsergebnisse sind in der Tabelle unter &quot;Vorhersage&quot; angegeben.</p>
<p>Bewerten Sie den Klassifikator.</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Lernen: Abschnitte 19.1 und 19.2</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>CAL2</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden
dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal
bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.</p>
<p>Einen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden
Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten.
Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die
Klasse.</p>
<p>Den Baum kann man mit dem Algorithmus CAL2 schrittweise aufbauen. Man startet mit &quot;Nichtwissen&quot; (symbolisiert
mit einem &quot;*&quot;) und iteriert durch alle Trainingsbeispiele, bis der Baum sich nicht mehr verändert. Wenn
der Baum auf ein Beispiel einen &quot;*&quot; ausgibt, dann ersetzt man diesen &quot;*&quot; mit der Klasse des eben betrachteten
Beispiels. Wenn der Baum bei einem Beispiel die passende Klasse ausgibt, macht man mit dem nächsten Beispiel
weiter. Wenn der Baum bei einem Beispiel eine andere Klasse ausgibt, muss das Klassensymbol im Baum (an
der Stelle, wo das Objekt gelandet ist) durch den nächsten Test ersetzt werden: Hierzu nimmt man das nächste,
auf diesem konkreten Pfad noch nicht verwendete Merkmal. CAL2 kann nur mit diskreten Attributen und disjunkten
Klassen einen fehlerfreien Baum erzeugen.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/bR_QVYtPRx8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL2</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/66bfcc3ba546355d5a4d41394912380e4641fc8498e8f257a98c602c11dd6ff33eb7f2ddb4fdebd433be30e9fcf91f99aaf1a484b46d2f34feac63a6e777a177' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL2</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Entscheidungsbaumlerner CAL2</li></ul>
  </div>
</div>




    <h2 id="entscheidungsbäume-klassifikation">Entscheidungsbäume: Klassifikation</h2>
<p><a href="#R-image-fdf278f6f66b1fa5e8fc7de2d9fb7123" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2/xor-decision-tree.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fdf278f6f66b1fa5e8fc7de2d9fb7123"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2/xor-decision-tree.png?width=80%25&height=auto"></a></p>
<ul>
<li>Attribute als Knoten im Baum</li>
<li>Ausprägungen als Test (Ausgang, Verzweigung)</li>
<li>Klasse (Funktionswert) als Blatt</li>
</ul>
<p>Erinnern Sie sich an das Beispiel mit der Auto-Reparatur aus der letzten Sitzung.</p>
<p>Die relevanten Eigenschaften (Merkmale) eines Autos würden als Knoten im Baum
repräsentiert. Beispiel: &quot;Motor startet&quot; oder &quot;Farbe&quot;.</p>
<p>Jedes Merkmal hat eine Anzahl von möglichen Ausprägungen, diese entsprechen den
Verzweigungen am Knoten. Beispiel: &quot;startet&quot;, &quot;startet nicht&quot; oder &quot;rot&quot;, &quot;weiß&quot;, &quot;silber&quot;, ... .</p>
<p>Entsprechend kann man durch Abarbeiten des Entscheidungsbaumes am Ende zu einer
Diagnose gelangen (Klasse).</p>
<p>Eine andere Sichtweise ist die Nutzung als Checkliste für eine Reparatur ...</p>
<h2 id="definition-entscheidungsbaum">Definition Entscheidungsbaum</h2>
<ul>
<li>
<p>Erinnerung: <strong>Merkmalsvektor</strong> für Objekt <span class="math align-center">$v$</span>:
<span class="math align-center">$$
        \mathbf{x}(v) = (x_1, x_2, \ldots, x_n)
    $$</span></p>
<ul>
<li><span class="math align-center">$n$</span> Merkmale (Attribute)</li>
<li>Attribut <span class="math align-center">$x_t$</span> hat <span class="math align-center">$m_t$</span> mögliche Ausprägungen</li>
<li>Ausprägung von <span class="math align-center">$v$</span> bzgl. <span class="math align-center">$x_t$</span>: <span class="math align-center">$\quad x_t(v) = i \quad$</span> (mit <span class="math align-center">$i = 1 \ldots m_t$</span>)</li>
</ul>
</li>
<li>
<p><strong>Alphabet</strong> für Baum:
<span class="math align-center">$$
        \lbrace x_t | t=1,\ldots,n \rbrace \cup \lbrace \kappa | \kappa = \ast,A,B,C,\ldots \rbrace \cup \lbrace (,) \rbrace
    $$</span></p>
</li>
<li>
<p><strong>Entscheidungsbaum</strong> <span class="math align-center">$\alpha$</span>:
<span class="math align-center">$$
        \alpha = \left\lbrace  \begin{array}{ll}
            \kappa  & \text{Terminalsymbole: } \kappa = \ast,A,B, \ldots\\
            x_t(\alpha_1, \alpha_2, \ldots, \alpha_{m_t}) & x_t \text{ Testattribut mit } m_t \text{ Ausprägungen}
        \end{array}\right.
    $$</span></p>
</li>
</ul>
<p><em>Anmerkung</em>: Stellen Sie sich die linearisierte Schreibweise wieder
wie den (verschachtelten) Aufruf von Konstruktoren vor. Es gibt die
Oberklasse <code>Baum</code>, von der für jedes Attribut eine Klasse abgeleitet
wird. D.h. der Konstruktor für eine Attributklasse erzeugt letztlich
ein Objekt vom Obertyp <code>Baum</code>. Außerdem sind die Terminalsymbole <code>A</code>,
<code>B</code>, ... Objekte vom Typ <code>Blatt</code>, welches eine Unterklasse von <code>Baum</code>
ist ...</p>
<p>Dabei wird die Anzahl der möglichen Ausprägungen für ein Attribut
berücksichtigt: Jede Ausprägung hat einen Parameter im Konstruktor.
Damit werden die Unterbäume beim Erzeugen des Knotens übergeben.</p>
<h2 id="induktion-von-entscheidungsbäumen-cal2">Induktion von Entscheidungsbäumen: CAL2</h2>
<ol>
<li>
<p>Anfangsschritt: <span class="math align-center">$\alpha^{(0)} = \ast$</span> (totales Unwissen)</p>
</li>
<li>
<p><span class="math align-center">$n$</span>-ter Lernschritt: Objekt <span class="math align-center">$v$</span> mit Klasse <span class="math align-center">$k$</span>, Baum <span class="math align-center">$\alpha^{(n-1)}$</span>
gibt <span class="math align-center">$\kappa$</span> aus</p>
<ul>
<li><span class="math align-center">$\kappa = \ast$</span>: ersetze <span class="math align-center">$\ast$</span> durch <span class="math align-center">$k$</span></li>
<li><span class="math align-center">$\kappa = k$</span>: keine Aktion nötig</li>
<li><span class="math align-center">$\kappa \neq k$</span>: Fehler
<ul>
<li>Ersetze <span class="math align-center">$\kappa$</span> mit neuem Test: <span class="math align-center">$\kappa \gets x_{t+1}(\ast, \ldots, \ast, k, \ast, \ldots, \ast)$</span></li>
<li><span class="math align-center">$x_{t+1}$</span>: nächstes Attribut, auf dem aktuellen Pfad noch nicht verwendet</li>
<li>Symbol <span class="math align-center">$k$</span> an Position <span class="math align-center">$i$</span> wenn <span class="math align-center">$x_{t+1}(v) = i$</span></li>
</ul>
</li>
</ul>
</li>
</ol>
<p><span class="math align-center">$\alpha^{(n)}$</span> bezeichnet den Baum im <span class="math align-center">$n$</span>-ten Lernschritt.</p>
<p>CAL2 ist ein <strong>Meta-Algorithmus</strong>: Es ist ein Algorithmus, um einen Algorithmus
zu lernen :-)</p>
<h2 id="beispiel-mit-cal2">Beispiel mit CAL2</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">4</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">3</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
<p><strong>Ergebnis</strong>: <span class="math align-center">$x_1(x_2(A, B), x_2(A, B))$</span></p>
<p><em>Anmerkung</em>: Denken Sie an die Analogie von oben. <span class="math align-center">$x_1$</span> kann als
Konstruktor einer Klasse <code>x1</code> betrachtet werden, die eine Unterklasse
von <code>Baum</code> ist. Durch den Aufruf des Konstruktors wird als ein <code>Baum</code>
erzeugt.</p>
<p>Es gibt in <span class="math align-center">$x_1$</span> zwei mögliche Ausprägungen, d.h. der Baum hat in
diesem Knoten zwei alternative Ausgänge. Diese Unterbäume werden
dem Konstruktor von <code>x1</code> direkt beim Aufruf übergeben (müssen also
Referenzen vom Typ <code>Baum</code> sein).</p>
<h2 id="cal2-bemerkungen">CAL2: Bemerkungen</h2>
<ul>
<li>
<p>Nur für diskrete Merkmale und disjunkte Klassen</p>
</li>
<li>
<p>Zyklischer Durchlauf durch Trainingsmenge</p>
</li>
<li>
<p>Abbruch:</p>
<ul>
<li>Alle Trainingsobjekte richtig klassifiziert
=&gt; Kein Fehler in einem kompletten Durchlauf</li>
<li>(Differenzierung nötig, aber alle Merkmale verbraucht)</li>
<li>(Lernschrittzahl überschritten)</li>
</ul>
</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Darstellung der Hypothese als Entscheidungsbaum</li>
<li>CAL2: diskrete Attribute, disjunkte Klassen</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106575&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest CAL2 (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Modellierung</strong></p>
<p>Sie stehen vor der Entscheidung, ob Sie sich zur Vorbereitung auf die
Flipped-Classroom-Sitzung noch das Skript anschauen.</p>
<p>Zeichnen Sie einen Entscheidungsbaum, der Ihnen bei der Entscheidung hilft.</p>
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit CAL2.</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
<p><strong>Handsimulation CAL2</strong></p>
<p>Zeigen Sie mit einer Handsimulation, wie CAL2 mit dem folgenden
Trainingsdatensatz schrittweise einen Entscheidungsbaum generiert.
Nutzen Sie die linearisierte Schreibweise.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Beispiel</th>
          <th style="text-align: center"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: center"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: center"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: center">Klasse</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">1</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">2</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">2</td>
      </tr>
      <tr>
          <td style="text-align: center">3</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">4</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">5</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">a</td>
          <td style="text-align: center">c</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">6</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">b</td>
          <td style="text-align: center">2</td>
      </tr>
  </tbody>
</table>
<p>Welchen Entscheidungsbaum würde CAL2 lernen, wenn dem Trainingsdatensatz
der Vektor <span class="math align-center">$((a,a,b), 2)$</span> als Beispiel Nr. 7 hinzugefügt werden würde?</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Unger1981'>[Unger1981] <strong>Lernfähige Klassifizierungssysteme (Classifier Systems Which Are Able to Learn)</strong><br>Unger, S. und Wysotzki, F., Akademie-Verlag, 1981.<br><em>Der Vollständigkeit halber aufgeführt (Werk ist leider vergriffen und wird nicht mehr verlegt)</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Pruning</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Pruning ist das Entfernen redundanter und irrelevanter Tests (Merkmale).</p>
<p>Irrelevante Merkmale spielen keine Rolle bei der Klassifikation, an jedem Ausgang
eines irrelevanten Merkmals findet sich exakt der selbe Baum. Diese Tests kann man
einfach entfernen und durch einen ihrer Teilbäume ersetzen; dadurch ändert sich
nicht die Klassifikation des Baumes.</p>
<p>Bei redundanten Tests sind alle Ausgänge bis auf einen noch mit &quot;Nichtwissen&quot; (&quot;*&quot;)
markiert. Hier kann man den Test durch den einen bekannten Ausgang ersetzen, wodurch
sich die Klassifikation ändert. Allerdings wird der Klassifikationsfehler nicht größer,
da man ja vorher nur für eine Ausprägung des redundanten Merkmals einen Baum hatte und
für die anderen jeweils mit &quot;*&quot; antworten musste (d.h. hier stets einen Fehler gemacht
hatte).</p>
<p>Über die Transformationsregel kann man einfach die Reihenfolge von Tests im Baum ändern.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/LKt9F2kGYdk' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Pruning</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/badf517191aa92377bf6ca9e63f90e8083d64de43f85b230b336cbf2b56e805d45063cf0974a6292ee39cf010aef11e87d3cf7ff9c9bd7e7c0a64f61128504e2' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Pruning</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Pruning: Entfernen bedingt irrelevanter Tests</li> <li>(K3) Pruning: Entfernen bedingt redundanter Tests</li> <li>(K3) Umformen von Entscheidungsbäumen mit Transformationsregel</li></ul>
  </div>
</div>




    <h2 id="pruning-bedingt-irrelevante-attribute">Pruning: Bedingt irrelevante Attribute</h2>
<p><strong>Baum</strong>: <span class="math align-center">$\alpha = x_1(x_2(A, B),  x_2(A, B),  x_2(A, B))$</span></p>
<p><span class="math align-center">$x_1$</span> ist <span class='alert'>bedingt irrelevant</span>
=&gt; Vereinfachung: <span class="math align-center">$\alpha = x_2(A, B)$</span></p>
<p><strong>Allgemein</strong>:</p>
<ul>
<li>Sei <span class="math align-center">$\tilde{x}$</span> Weg zu Nichtendknoten <span class="math align-center">$x_t$</span></li>
<li>Baum dort <span class="math align-center">$\alpha/\tilde{x} = x_t(\alpha_1, \ldots, \alpha_{m_t})$</span></li>
<li><span class="math align-center">$x_t$</span> ist <span class='alert'><strong>bedingt irrelevant</strong></span> unter der Bedingung
<span class="math align-center">$\tilde{x}$</span>, wenn <span class="math align-center">$\alpha_1 = \alpha_2 = \ldots = \alpha_{m_t}$</span></li>
<li><strong>Vereinfachung</strong>: Ersetze in <span class="math align-center">$\alpha/\tilde{x}$</span> den Test <span class="math align-center">$x_t$</span> durch <span class="math align-center">$\alpha_1$</span></li>
</ul>
<p><em>Anmerkung</em>:
Der durch das Entfernen von bedingt irrelevanten Attributen entstandene Baum
hat <strong>exakt</strong> die selbe Aussage (Klassifikation) wie der Baum vor dem Pruning.</p>
<p><strong>Anmerkung</strong>:
<span class="math align-center">$x_1$</span> im obigen Beispiel ist sogar <span class='alert'><strong>global</strong> irrelevant</span>, da es sich hier
um die Wurzel des Baumes handelt. Der Weg <span class="math align-center">$\tilde{x}$</span> ist in diesem Fall der leere
Weg ...</p>
<h2 id="pruning-bedingt-redundante-attribute">Pruning: Bedingt redundante Attribute</h2>
<p><strong>Baum</strong>: <span class="math align-center">$\alpha = x_1(\ast,  \ast,  x_2(A, B))$</span></p>
<p><span class="math align-center">$x_1$</span> ist <span class='alert'>bedingt redundant</span>
=&gt; Vereinfachung: <span class="math align-center">$\alpha = x_2(A, B)$</span></p>
<p><strong>Allgemein</strong>:</p>
<ul>
<li>Sei <span class="math align-center">$\tilde{x}$</span> Weg zu Nichtendknoten <span class="math align-center">$x_t$</span></li>
<li>Baum dort <span class="math align-center">$\alpha/\tilde{x} = x_t(\ast, \ldots, \ast, \alpha_i, \ast, \ldots, \ast)$</span> (mit <span class="math align-center">$\alpha_i \neq \ast$</span>)</li>
<li><span class="math align-center">$x_t$</span> ist <span class='alert'><strong>bedingt redundant</strong></span> unter der Bedingung <span class="math align-center">$\tilde{x}$</span></li>
<li><strong>Vereinfachung</strong>: Ersetze in <span class="math align-center">$\alpha/\tilde{x}$</span> den Test <span class="math align-center">$x_t$</span> durch <span class="math align-center">$\alpha_i$</span></li>
</ul>
<p><em>Anmerkung</em>:
Der durch das Entfernen von bedingt redundanten Attributen entstandene Baum
hat eine etwas andere Klassifikation als der Baum vor dem Pruning. Wo vorher
ein <code>*</code> ausgegeben wurde, wird nach dem Pruning u.U. ein Klassensymbol
ausgegeben. Der Klassifikationsfehler erhöht sich aber <strong>nicht</strong>, da hier ein
<code>*</code> wie ein falsches Klassensymbol zu werten ist.</p>
<p><strong>Anmerkung</strong>:
<span class="math align-center">$x_1$</span> im obigen Beispiel ist sogar <span class='alert'><strong>global</strong> redundant</span>, da es sich
hier um die Wurzel des Baumes handelt. Der Weg <span class="math align-center">$\tilde{x}$</span> ist in diesem Fall
der leere Weg ...</p>
<h2 id="allgemeine-transformationsregel">Allgemeine Transformationsregel</h2>
<span class="math align-center">$$
    x_1(x_2(a, b),  x_2(c, d))  \Leftrightarrow  x_2(x_1(a, c),  x_1(b, d))
$$</span>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Pruning: Entfernen bedingt redundanter und irrelevanter Tests</li>
<li>Transformationsregel zum Umbauen von Entscheidungsbäumen</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106577&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Pruning (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Entscheidungsbäume: Abschnitt 8.4</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Entscheidungsbäume: Abschnitt 19.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>CAL3</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>
CAL3 ist eine einfache Erweiterung von CAL2 für nicht-disjunkte (überlappende) Klassen. Statt
beim Baumaufbau bei einer Fehlklassifikation sofort zu verzweigen, werden hier zunächst die
im entsprechenden Pfad aufgelaufenen Klassensymbole gezählt. Wenn ausreichend viele davon
gesehen wurden (Schwelle <span class="math align-center">$S_1$</span>), wird eine Entscheidung getroffen: Wenn eine Klasse in diesem
temporären Blatt dominiert (ihre Häufigkeit über einer Schwelle <span class="math align-center">$S_2$</span> liegt), dann entscheidet
man sich in diesem Blatt fest für diese Klasse. Ansonsten (die Häufigkeit aller Klassen in
dem Blatt liegt unter <span class="math align-center">$S_2$</span>) nimmt man analog zu CAL2 den nächsten, auf diesem Pfad noch nicht
verwendeten Test hinzu.
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/9Wj51XvuntM' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL3</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/6acb93574f25ff341b5a09487fc153ea28252e12d3960342bc7d05a463e56b338f53f366338229df44f5c486400465fddf58e727fd8f9cc56904dd67c7c8ecb8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CAL3</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Meta-Algorithmus CAL3 für überlappende Klassen</li></ul>
  </div>
</div>




    <h2 id="cal3-erweiterung-von-cal2-für-nicht-disjunkte-klassen">CAL3: Erweiterung von CAL2 für nicht-disjunkte Klassen</h2>
<ol>
<li>
<p>Anfangsschritt: <span class="math align-center">$\alpha^{(0)} = \ast$</span> (totales Unwissen)</p>
</li>
<li>
<p><span class="math align-center">$n$</span>-ter Lernschritt: Objekt <span class="math align-center">$v$</span> mit Klasse <span class="math align-center">$k$</span></p>
<ul>
<li>
<p>Rückweisung (Endknoten mit <span class="math align-center">$\ast$</span>):
Ersetze <span class="math align-center">$\ast$</span> durch Vereinigungsklasse <span class="math align-center">$/k1/$</span></p>
</li>
<li>
<p>Endknoten mit Vereinigungsklasse:</p>
<ul>
<li>Zähler für <span class="math align-center">$k$</span> erhöhen, bzw.</li>
<li><span class="math align-center">$k$</span> mit Anzahl <span class="math align-center">$1$</span> in Vereinigungsklasse einfügen</li>
</ul>
</li>
</ul>
<p>Falls nun die Summe aller Klassen am Endknoten größer/gleich <span class="math align-center">$S_1$</span> (Statistikschwelle):</p>
<ul>
<li>
<p>Für <strong>genau eine</strong> Klasse gilt: <span class="math align-center">$P(k | \tilde{x}) \ge S_2$</span>:
=&gt; Abschluss: Ersetze Vereinigungsklasse durch <span class="math align-center">$k$</span> (für immer!)</p>
</li>
<li>
<p>Für <strong>alle</strong> Klassen gilt: <span class="math align-center">$P(k | \tilde{x}) < S_2$</span>:
=&gt; Differenzierung: Ersetze Vereinigungsklasse durch neuen
Test: <span class="math align-center">$\kappa \gets x_{t+1}(\ast, \ldots, \ast, /k1/, \ast, \ldots, \ast)$</span></p>
<p><span class="math align-center">$x_{t+1}$</span>: nächstes Attribut, auf dem aktuellen Pfad <span class="math align-center">$\tilde{x}$</span>
noch nicht verwendet
Symbol <span class="math align-center">$k$</span> mit Anzahl 1 an Position <span class="math align-center">$i$</span> wenn <span class="math align-center">$x_{t+1}(v) = i$</span></p>
</li>
</ul>
</li>
</ol>
<h2 id="beispiel-mit-cal3">Beispiel mit CAL3</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
<ul>
<li><span class="math align-center">$S_1 = 4, S_2 = 0.7$</span></li>
</ul>
<p><strong>Ergebnis</strong>: <span class="math align-center">$x_1(A,  x_2(B, A))$</span></p>
<p>Trainingsfehler: <span class="math align-center">$1/5 = 0.2 < 1-S_2 = 1-0.7 = 0.3$</span></p>
<p><strong>Hinweis</strong>: Bei nicht überlappenden Klassen erzeugt CAL3 u.U. andere Bäume als CAL2 ...</p>
<h2 id="cal3-abbruchbedingungen-und-parameter">CAL3: Abbruchbedingungen und Parameter</h2>
<ul>
<li>
<p><strong>Parameter</strong>:</p>
<ul>
<li><span class="math align-center">$S_1$</span>: Statistikschwelle, problemabhängig wählen</li>
<li><span class="math align-center">$S_2$</span>: <span class="math align-center">$0.5 < S_2 \le 1.0$</span></li>
<li>Klassifikationsfehler kleiner als <span class="math align-center">$1-S_2$</span>
<ul>
<li>kleiner Fehler =&gt; großer Baum</li>
<li>großer Fehler =&gt; kleiner Baum</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Abbruch</strong>:</p>
<ul>
<li>Alle Trainingsobjekte richtig klassifiziert
=&gt; Kein Fehler in einem kompletten Durchlauf</li>
<li>Alle Endknoten mit eindeutigen Klassensymbolen belegt</li>
<li>Differenzierung nötig, aber alle Merkmale verbraucht</li>
<li>Lernschrittzahl überschritten</li>
</ul>
</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>CAL3: Erweiterung von CAL2 für überlappende Klassen
<ul>
<li>Parameter <span class="math align-center">$S_1$</span> (Anzahl Objekte bis Entscheidung), <span class="math align-center">$S_2$</span> (Dominanz?)</li>
<li>Trainingsfehler wg. überlappender Klassen!</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106576&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest CAL3 (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit CAL3 (<span class="math align-center">$S_1=4, S_2=0.6$</span>).</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Unger1981'>[Unger1981] <strong>Lernfähige Klassifizierungssysteme (Classifier Systems Which Are Able to Learn)</strong><br>Unger, S. und Wysotzki, F., Akademie-Verlag, 1981.<br><em>Der Vollständigkeit halber aufgeführt (Werk ist leider vergriffen und wird nicht mehr verlegt)</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Entropie</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Die Entropie kann als Maß für den Informationsgehalt einer Trainingsmenge betrachtet werden:
Wieviele Ja/Nein-Entscheidungen sind nötig, um die Daten fehlerfrei zu repräsentieren?</p>
<p>Nach der Wahl eines Attributs kann die verbleibende mittlere Entropie berechnet werden. Damit
hat man ein Kriterium für die Auswahl von Attributen beim Aufbau von Entscheidungsbäumen:
Nimm das Attribut, welches einen möglichst hohen Informationsgehalt hat. Oder andersherum:
Wähle das Attribut, bei dem die verbleibende mittlere Entropie der Trainingsmenge nach der
Wahl des Attributs am kleinsten ist.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/4IZYA5EWO1k' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Entropie</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/6c84f8e181911aa89818cac70de40087fcab1209f4e0264f77da811289a5420fd284bc89464822690ff7906a735c778bda490949bf69091a5420885cc5cdad69' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Entropie</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Berechnung der Entropie und des Information Gain</li></ul>
  </div>
</div>




    <h2 id="wie-attribute-wählen">Wie Attribute wählen?</h2>
<h3 id="erinnerung-cal2cal3">Erinnerung: CAL2/CAL3</h3>
<ul>
<li>Zyklische Iteration durch die Trainingsmenge</li>
<li>Ausschließlich aktuelles Objekt betrachtet</li>
<li><span class='alert'>Reihenfolge</span> der &quot;richtigen&quot; Attributwahl bei Verzweigung unklar</li>
</ul>
<p>=&gt; Betrachte stattdessen die komplette Trainingsmenge!</p>
<h3 id="relevanz--informationsgehalt">Relevanz =&gt; Informationsgehalt</h3>
<ul>
<li>Shannon/Weaver (1949): <span class='alert'><strong>Entropie</strong></span>
<ul>
<li>Maß für die Unsicherheit einer Zufallsvariablen</li>
<li>Anzahl der Bits zur Darstellung der Ergebnisse eines Zufallsexperiments</li>
</ul>
</li>
</ul>
<h3 id="beispiele">Beispiele</h3>
<ul>
<li>Münze, die immer auf dem Rand landet: keine Unsicherheit, 0 Bit</li>
<li>Faire Münze: Kopf oder Zahl: Entropie 1 Bit</li>
<li>Fairer 4-seitiger Würfel: 4 mögliche Ausgänge: Entropie 2 Bit</li>
<li>Münze, die zu 99% auf einer Seite landet: Entropie nahe Null</li>
</ul>
<p>=&gt; Anzahl der Ja/Nein-Fragen, um zur gleichen Information zu kommen</p>
<h2 id="definition-der-entropie-hahahugoshortcode14s0hbhb-für-zufallsvariable-hahahugoshortcode14s1hbhb">Definition der Entropie <span class="math align-center">$H(V)$</span> für Zufallsvariable <span class="math align-center">$V$</span></h2>
<ul>
<li>Zufallsvariable <span class="math align-center">$V$</span> =&gt; mögliche Werte <span class="math align-center">$v_k$</span></li>
<li>Wahrscheinlichkeit für <span class="math align-center">$v_k$</span> sei <span class="math align-center">$p_k = P(v_k)$</span></li>
</ul>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p><span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<p><span class='alert'>Hinweis</span>:
<span class="math align-center">$\log_2 x = \frac{\log_{10} x}{\log_{10} 2} = \frac{\log x}{\log 2}$</span></p>
</div>
<div class='column'>
<p><a href="#R-image-04d8f80904ded657778dd9ed0f123215" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy/log_range.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-04d8f80904ded657778dd9ed0f123215"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy/log_range.png?width=auto&height=auto"></a></p>
</div>
</div>
<ul>
<li>Nur eine Klasse: <span class="math align-center">$\log_2 1 = 0$</span> =&gt; <span class="math align-center">$H(V) = 0$</span> Bit</li>
<li>Zwei Klassen, gleichwahrscheinlich: <span class="math align-center">$\log_2 0.5 = -1$</span> =&gt; <span class="math align-center">$H(V) = 1$</span> Bit</li>
</ul>
<h2 id="beispiele-entropie-faire-münze">Beispiele Entropie: faire Münze</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>Entropie: <span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<ul>
<li><span class="math align-center">$v_1 = \operatorname{Kopf},  v_2 = \operatorname{Zahl}$</span></li>
<li><span class="math align-center">$p_1 = 0.5,  p_2 = 0.5$</span></li>
<li><span class="math align-center">$H(\operatorname{Fair}) = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1$</span> Bit</li>
</ul>
</div>
<div class='column'>
<span class="math align-center">$\log_2 0.5 = -1$</span>
</div>
</div>
<h2 id="beispiele-entropie-unfaire-münze">Beispiele Entropie: unfaire Münze</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>Entropie: <span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<ul>
<li>
<span class="math align-center">$v_1 = \operatorname{Kopf},  v_2 = \operatorname{Zahl}$</span>
</li>
<li>
<span class="math align-center">$p_1 = 0.99,  p_2 = 0.01$</span>
</li>
<li>
<span class="math align-center">$H(\operatorname{UnFair}) = -(0.99 \log_2 0.99 + 0.01 \log_2 0.01)$</span>
<p><span class="math align-center">$H(\operatorname{UnFair}) \approx 0.08$</span> Bit</p>
</li>
</ul>
</div>
<div class='column'>
<span class="math align-center">$\log_2 0.01 \approx -6.64$</span>
<span class="math align-center">$\log_2 0.99 \approx -0,014$</span>
</div>
</div>
<h2 id="beispiele-entropie-4-seitiger-würfel">Beispiele Entropie: 4-seitiger Würfel</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>Entropie: <span class="math align-center">$H(V) = -\sum_k p_k \log_2 p_k$</span></p>
</span></span>
</div>
<div class='columns'>
<div class='column'>
<ul>
<li><span class="math align-center">$v_1 = 1,  v_2 = 2,   v_3 = 3,   v_4 = 4$</span></li>
<li><span class="math align-center">$p_1 = p_2 = p_3 = p_4 = 0.25$</span></li>
<li><span class="math align-center">$H(\operatorname{Wuerfel}) = -4\cdot(0.25 \log_2 0.25) = 2$</span> Bit</li>
</ul>
</div>
<div class='column'>
<span class="math align-center">$\log_2 0.25 = -2$</span>
</div>
</div>
<h2 id="entropie-der-trainingsmenge-häufigkeit-der-klassen-zählen">Entropie der Trainingsmenge: Häufigkeit der Klassen zählen</h2>
<div class='columns'>
<div class='column'>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
</div>
<div class='column'>
<ul>
<li>Anzahl Klasse <span class="math align-center">$A$</span>: 4</li>
<li>Anzahl Klasse <span class="math align-center">$B$</span>: 2</li>
<li>Gesamtzahl Beispiele: 6</li>
</ul>
<p>Wahrscheinlichkeit für <span class="math align-center">$A$</span>: <span class="math align-center">$p_A = 4/6 = 0.667$</span></p>
<p>Wahrscheinlichkeit für <span class="math align-center">$B$</span>: <span class="math align-center">$p_B = 2/6 = 0.333$</span></p>
</div>
</div>
<span class="math align-center">$$
\begin{array}{rcl}
    H(S) &=& -\sum_k p_k \log_2 p_k\\
         &=& -(4/6 \cdot \log_2 4/6 + 2/6 \cdot \log_2 2/6)\\
         &=& -(-0.39 -0.53) = 0.92 \operatorname{Bit}
\end{array}
$$</span>
<h2 id="mittlere-entropie-nach-betrachtung-von-attribut-hahahugoshortcode14s40hbhb">Mittlere Entropie nach Betrachtung von Attribut <span class="math align-center">$A$</span></h2>
<span class="math align-center">$$
    R(S, A) = \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
$$</span>
<ul>
<li>
<p>Auswahl von Attribut <span class="math align-center">$A$</span> partitioniert die Trainingsmenge:
Je Ausprägung <span class="math align-center">$v$</span> von <span class="math align-center">$A$</span> erhält man eine Submenge <span class="math align-center">$S_v$</span></p>
</li>
<li>
<p><span class="math align-center">$R(S, A)$</span> berechnet die mittlere Entropie der Trainingsmenge, nachdem
Attribut <span class="math align-center">$A$</span> ausgewählt wurde: Unsicherheit/nötige Bits nach Auswahl von
Attribut <span class="math align-center">$A$</span></p>
</li>
</ul>
<h2 id="entropie-der-trainingsmenge-nach-attributwahl">Entropie der Trainingsmenge nach Attributwahl</h2>
<div class='columns'>
<div class='column'>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
</div>
<div class='column'>
<ul>
<li>Sei Attribut <span class="math align-center">$x_1$</span> ausgewählt</li>
<li><span class="math align-center">$x_1$</span> partitioniert die Trainingsmenge
<ul>
<li><span class="math align-center">$x_1=0$</span> liefert <span class="math align-center">$S_0 = \lbrace 1,3,5,6 \rbrace$</span></li>
<li><span class="math align-center">$x_1=1$</span> liefert <span class="math align-center">$S_1 = \lbrace 2,4 \rbrace$</span></li>
<li>Häufigkeit für <span class="math align-center">$x_1=0$</span>: <span class="math align-center">$4/6$</span></li>
<li>Häufigkeit für <span class="math align-center">$x_1=1$</span>: <span class="math align-center">$2/6$</span></li>
<li>Gesamtzahl Beispiele: 6</li>
</ul>
</li>
</ul>
</div>
</div>
<span class="math align-center">$$
\begin{array}{rcl}
    R(S, A) &=& \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)\\
         &=& 4/6 \cdot H(\lbrace 1,3,5,6 \rbrace) + 2/6 \cdot H(\lbrace 2,4 \rbrace)\\
         &=& 4/6\cdot(-3/4 \cdot \log_2 3/4 - 1/4 \cdot \log_2 1/4) +\\
         && 2/6\cdot(-1/2 \cdot \log_2 1/2 - 1/2 \cdot \log_2 1/2)\\
         &=& 0.54 + 0.33 = 0.87 \operatorname{Bit}
\end{array}
$$</span>
<h2 id="ausblick-gini-impurity">Ausblick: Gini Impurity</h2>
<p>Wir haben hier die <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="external" target="_blank">Entropie</a>
als Maß für den Informationsgehalt einer Trainingsmenge genutzt. <span class="math align-center">$R(S,A)$</span> als die mittlere
Entropie nach Betrachtung von Attribut <span class="math align-center">$A$</span> wird von typischen Entscheidungsbaumverfahren
wie ID3 und C4.5 genutzt, um bei einer Verzweigung das nächste möglichst aussagekräftige
Merkmal auszuwählen.</p>
<p>In anderen Entscheidungsbaumlernern wird stattdessen die
<a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="external" target="_blank">Gini Impurity</a>
zur Bestimmung des Informationsgehalts eingesetzt (u.a. CART). Dieses Maß sagt aus,
wie oft man ein zufällig gezogenes Element des Datensatzes falsch klassifizieren
würde, wenn man es mit einer zufälligen Klasse basierend auf der Verteilung der
Klassen im Datensatz labeln würde.</p>
<p>Hierzu drei lesenswerte Blog-Einträge:</p>
<ul>
<li><a href="https://medium.com/poli-data/deep-dive-into-the-basics-of-gini-impurity-in-decision-trees-with-math-intuition-46c721d4aaec" rel="external" target="_blank">Deep dive into the basics of Gini Impurity in Decision Trees with math Intuition</a></li>
<li><a href="https://towardsdatascience.com/decision-trees-explained-d7678c43a59e" rel="external" target="_blank">Decision Trees, Explained</a></li>
<li><a href="https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38" rel="external" target="_blank">Decision Tree Algorithm With Hands-On Example</a></li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Begriff und Berechnung der Entropie: Maß für die Unsicherheit</li>
<li>Begriff und Berechnung des Informationsgewinns
<ul>
<li>Entropie für eine Trainingsmenge</li>
<li>Mittlere Entropie nach Wahl eines Attributs</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106578&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Entropie (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Entropie einer Trainingsmenge</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Geben Sie die Entropie <span class="math align-center">$H(S)$</span> der Trainingsmenge an.</li>
<li>Berechnen Sie <span class="math align-center">$R(H,A)$</span> (die mittlere Entropie der Trainingsmenge, nachdem Attribut <span class="math align-center">$A$</span> gesehen wurde) für die einzelnen Attribute.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Entscheidungsbäume: Abschnitt 8.4</em></li> <li id='id_Mitchell2010'>[Mitchell2010] <strong>Machine Learning</strong><br>Mitchell, T., McGraw-Hill, 2010. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-0711-5467-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-0711-5467-3</a>.<br><em>ID3: Kapitel 3</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Entscheidungsbäume: Abschnitt 19.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>ID3 und C4.5</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Der Entscheidungsbaum-Lernalgorithmus <strong>ID3</strong> nutzt den Informationsgehalt für die Entscheidung
bei der Attributwahl: Nimm das Attribut, welches einen möglichst hohen Informationsgehalt hat.
Oder andersherum: Wähle das Attribut, bei dem die verbleibende mittlere Entropie der Trainingsmenge
nach der Wahl des Attributs am kleinsten ist. Oder noch anders formuliert: Nimm das Attribut, bei
dem die Differenz zwischen der Entropie der Trainingsmenge (vor der Wahl des Attributs) und der
verbleibenden mittleren Entropie (nach der Wahl des Attributs) am größten ist (die Differenz nennt
man auch &quot;<em>Information Gain</em>&quot;). Die Trainingsmenge wird entsprechend der Ausprägung in Bezug auf
das eben gewählte Merkmal aufgeteilt und an die Kinder des Knotens weiter gereicht; dort wird der
Baum rekursiv weiter aufgebaut.</p>
<p>Durch eine Normierung des <em>Information Gain</em> kann eine Verbesserung in Bezug auf mehrwertige
Attribute erreicht werden, dies führt zum Algorithmus <strong>C4.5</strong>.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/Yo1cmeS6BK8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL ID3 und C4.5</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/aa69406cfdf0ce8b2b614dd475926b56c8025239c5b4458ec7741c7733c6077fd192e4db6f58c8a3e39b7b895c2ddedf83327640326bfbedc2617c4f75bc59bd' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL ID3 und C4.5</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Entscheidungsbaumalgorithmen ID3 und C4.5</li></ul>
  </div>
</div>




    <h2 id="wie-attribute-wählen">Wie Attribute wählen?</h2>
<p>Erinnerung: CAL2/CAL3</p>
<ul>
<li>Zyklische Iteration durch die Trainingsmenge</li>
<li>Ausschließlich aktuelles Objekt betrachtet</li>
<li><span class='alert'>Reihenfolge</span> der &quot;richtigen&quot; Attributwahl bei Verzweigung unklar</li>
</ul>
<p>=&gt; Betrachte stattdessen die <strong>komplette</strong> Trainingsmenge!</p>
<h2 id="erinnerung-entropie-maß-für-die-unsicherheit">Erinnerung Entropie: Maß für die Unsicherheit</h2>
<ul>
<li>
<p>Entropie <span class="math align-center">$H(S)$</span> der Trainingsmenge <span class="math align-center">$S$</span>: relative Häufigkeit der Klassen zählen</p>
</li>
<li>
<p>Mittlere Entropie nach Betrachtung von Attribut <span class="math align-center">$A$</span></p>
<span class="math align-center">$$
        R(S, A) = \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
    $$</span>
</li>
<li>
<p>Informationsgewinn durch Betrachtung von Attribut <span class="math align-center">$A$</span></p>
<span class="math align-center">$$
    \begin{array}{rcl}
        \operatorname{Gain}(S, A) &=& H(S) - R(S, A)\\[5pt]
                                &=& H(S) - \sum_{v \in \operatorname{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
    \end{array}
    $$</span>
</li>
</ul>
<p><span class="math align-center">$R(S,A)$</span> ist die Unsicherheit/nötige Bits nach Auswahl von Attribut A.
Je kleiner <span class="math align-center">$R(S,A)$</span>, um so kleiner die <strong>verbleibende Unsicherheit</strong> bzw.
um so kleiner die Anzahl der nötigen Bits zur Darstellung der
partitionierten Trainingsmenge <strong>nach</strong> Betrachtung von Attribut <span class="math align-center">$A$</span> ...</p>
<p>=&gt; Je kleiner <span class="math align-center">$R(S,A)$</span>, um so größer der Informationsgewinn</p>
<h2 id="informationsgewinn-kriterium-zur-auswahl-von-attributen">Informationsgewinn: Kriterium zur Auswahl von Attributen</h2>
<ol>
<li>Informationsgewinn für alle Attribute berechnen</li>
<li>Nehme Attribut mit größtem Informationsgewinn als nächsten Test</li>
</ol>
<div class='columns'>
<div class='column'>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
</div>
<div class='column'>
<span class="math align-center">$H(S) = 0.92 \operatorname{Bit}$</span>
<span class="math align-center">$$
\begin{array}{rcl}
\operatorname{Gain}(S, x_1) &=& 0.92 - 0.87 = 0.05 \operatorname{Bit}\\
\operatorname{Gain}(S, x_2) &=& 0.92 - 2/6  \cdot 0 - 4/6 \cdot 1\\
                            &=& 0.25 \operatorname{Bit}\\
\operatorname{Gain}(S, x_3) &=& 0.92 - 3/6 \cdot 0.92 - 2/6 \cdot 1 - 1/6 \cdot 0\\
                            &=& 0.13 \operatorname{Bit}
\end{array}
$$</span>
</div>
</div>
<p>Informationsgewinn für <span class="math align-center">$x_2$</span> am höchsten =&gt; wähle <span class="math align-center">$x_2$</span> als nächsten Test</p>
<h2 id="entscheidungsbaumlerner-id3-quinlan-1986">Entscheidungsbaumlerner ID3 (Quinlan, 1986)</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ID3</span>(examples, attr, default):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Abbruchbedingungen</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> examples<span style="color:#f92672">.</span>isEmpty():  <span style="color:#66d9ef">return</span> default
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> examples<span style="color:#f92672">.</span>each(<span style="color:#66d9ef">class</span> <span style="color:#960050;background-color:#1e0010">== </span><span style="color:#a6e22e">A</span>):  <span style="color:#66d9ef">return</span> A  <span style="color:#75715e"># all examples have same class</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> attr<span style="color:#f92672">.</span>isEmpty():  <span style="color:#66d9ef">return</span> examples<span style="color:#f92672">.</span>MajorityValue()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Baum mit neuem Test erweitern</span>
</span></span><span style="display:flex;"><span>    test <span style="color:#f92672">=</span> MaxInformationGain(examples, attr)
</span></span><span style="display:flex;"><span>    tree <span style="color:#f92672">=</span> new DecisionTree(test)
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> examples<span style="color:#f92672">.</span>MajorityValue()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> v_i <span style="color:#f92672">in</span> test:
</span></span><span style="display:flex;"><span>        ex_i <span style="color:#f92672">=</span> examples<span style="color:#f92672">.</span>select(test <span style="color:#f92672">==</span> v_i)
</span></span><span style="display:flex;"><span>        st <span style="color:#f92672">=</span> ID3(ex_i, attr <span style="color:#f92672">-</span> test, m)
</span></span><span style="display:flex;"><span>        tree<span style="color:#f92672">.</span>addBranch(label<span style="color:#f92672">=</span>v_i, subtree<span style="color:#f92672">=</span>st)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tree</span></span></code></pre></div>
<p><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html#id_Russell2020">[Russell2020]</a>: Man erhält aus dem &quot;Learn-Decision-Tree&quot;-Algorithmus <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html#id_Russell2020">[Russell2020, S. 678, Fig. 19.5]</a>
den hier vorgestellten ID3-Algorithmus, wenn man die Funktion <span class="math align-center">$\operatorname{Importance}(a, examples)$</span>
als <span class="math align-center">$\operatorname{InformationGain}(examples, attr)$</span> implementiert/nutzt.</p>
<p><strong>Hinweis</strong>: Mit der Zeile <code>if examples.each(class == A):  return A</code> soll ausgedrückt werden, dass alle
ankommenden Trainingsbeispiele die selbe Klasse haben und dass diese dann als Ergebnis zurückgeliefert
wird. Das &quot;<code>A</code>&quot; steht im obigen Algorithmus nur symbolisch für die selbe Klasse! Es kann also auch ein
anderes Klassensymbol als &quot;<code>A</code>&quot; sein ...</p>
<h3 id="beispiel-id3">Beispiel ID3</h3>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nr.</th>
          <th style="text-align: left"><span class="math align-center">$x_1$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_2$</span></th>
          <th style="text-align: left"><span class="math align-center">$x_3$</span></th>
          <th style="text-align: left"><span class="math align-center">$k$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">3</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">A</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">B</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">A</td>
      </tr>
  </tbody>
</table>
<ul>
<li><span class="math align-center">$x2$</span> höchsten Information Gain</li>
<li><span class="math align-center">$x2=0$</span> =&gt; Beispiele 1,2 =&gt; A</li>
<li><span class="math align-center">$x2=1$</span> =&gt; Beispiele 3,4,5,6 =&gt; Information Gain berechnen,
weiter teilen und verzweigen</li>
</ul>
<h2 id="beobachtung-hahahugoshortcode16s27hbhb-ist-bei-mehrwertigen-attributen-höher">Beobachtung: <span class="math align-center">$\operatorname{Gain}$</span> ist bei mehrwertigen Attributen höher</h2>
<ul>
<li>
<p>Faire Münze:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Fair}) = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \operatorname{Bit}$</span></li>
</ul>
</li>
<li>
<p>4-seitiger Würfel:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Dice}) = -4\cdot(0.25 \log_2 0.25) = 2 \operatorname{Bit}$</span></li>
</ul>
</li>
</ul>
<p>=&gt; <span class="math align-center">$\operatorname{Gain}$</span> ist bei mehrwertigen Attributen höher</p>
<p>Damit würden Attribute bei der Wahl bevorzugt, nur weil sie mehr Ausprägungen haben als andere.</p>
<p><em>Anmerkung</em>: Im obigen Beispiel wurde einfach die Entropie für zwei &quot;Attribute&quot; mit unterschiedlich
vielen Ausprägungen betrachtet, das ist natürlich kein <span class="math align-center">$\operatorname{Gain}(S, A)$</span>. Aber es sollte
deutlich machen, dass Merkmale mit mehr Ausprägungen bei der Berechnung des Gain für eine Trainingsmenge
einfach wegen der größeren Anzahl an Ausprägungen rechnerisch bevorzugt würden.</p>
<h2 id="c45-als-verbesserung-zu-id3">C4.5 als Verbesserung zu ID3</h2>
<p>Normierter Informationsgewinn: <span class="math align-center">$\operatorname{Gain}(S, A) \cdot \operatorname{Normalisation}(A)$</span></p>
<span class="math align-center">$$
    \operatorname{Normalisation}(A) = \frac{1}{
        \sum_{v \in \operatorname{Values}(A)} p_v \log_2 \frac{1}{p_v}
    }
$$</span>
<p>C4.5 kann zusätzlich u.a. auch noch mit kontinuierlichen Attributen umgehen, vgl.
<a href="https://en.wikipedia.org/wiki/C4.5_algorithm" rel="external" target="_blank">en.wikipedia.org/wiki/C4.5_algorithm</a>.</p>
<p>In einem <a href="http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf" rel="external" target="_blank">Paper</a>
(<a href="https://doi.org/10.1007/s10115-007-0114-2" rel="external" target="_blank">DOI 10.1007/s10115-007-0114-2</a>) wurde
der Algorithmus zu den &quot;Top 10 algorithms in data mining&quot; ausgewählt.</p>
<p>Im Wikipedia-Artikel <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="external" target="_blank">Information Gain</a>
finden Sie weitere Informationen zum &quot;Informationsgewinn&quot; (<em>Information Gain</em>).</p>
<p>Ein anderer, relativ ähnlich arbeitender Entscheidungsbaumlerner ist der
<a href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="external" target="_blank">CART (Classification And Regression Tree)</a>-Algorithmus,
wobei der Begriff &quot;CART&quot; allerdings oft auch einfach allgemein für &quot;Entscheidungsbaumlerner&quot;
genutzt wird.</p>
<p>Hierzu drei lesenswerte Blog-Einträge:</p>
<ul>
<li><a href="https://medium.com/poli-data/deep-dive-into-the-basics-of-gini-impurity-in-decision-trees-with-math-intuition-46c721d4aaec" rel="external" target="_blank">Deep dive into the basics of Gini Impurity in Decision Trees with math Intuition</a></li>
<li><a href="https://towardsdatascience.com/decision-trees-explained-d7678c43a59e" rel="external" target="_blank">Decision Trees, Explained</a></li>
<li><a href="https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38" rel="external" target="_blank">Decision Tree Algorithm With Hands-On Example</a></li>
</ul>
<h2 id="beispiele-zur-normierung-bei-c45">Beispiele zur Normierung bei C4.5</h2>
<ul>
<li>
<p>Faire Münze:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Fair}) = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \operatorname{Bit}$</span></li>
<li>Normierung: <span class="math align-center">$1/(0.5 \log_2 (1/0.5) + 0.5 \log_2 (1/0.5)) = 1/(0.5 \cdot 1 + 0.5 \cdot 1) = 1$</span></li>
<li>Normierter Informationsgewinn: <span class="math align-center">$\operatorname{Gain}(S, A) \cdot \operatorname{Normalisation}(A) = 1 \operatorname{Bit} \cdot 1 = 1 \operatorname{Bit}$</span></li>
</ul>
</li>
<li>
<p>4-seitiger Würfel:</p>
<ul>
<li>Entropie = <span class="math align-center">$H(\operatorname{Dice}) = -4\cdot(0.25 \log_2 0.25) = 2 \operatorname{Bit}$</span></li>
<li>Normierung: <span class="math align-center">$1/(4\cdot 0.25 \log_2 (1/0.25)) = 1/(4\cdot 0.25 \cdot 2) = 0.5$</span></li>
<li>Normierter Informationsgewinn: <span class="math align-center">$\operatorname{Gain}(S, A) \cdot \operatorname{Normalisation}(A) = 2 \operatorname{Bit} \cdot 0.5 = 1 \operatorname{Bit}$</span></li>
</ul>
</li>
</ul>
<p>=&gt; Normierung sorgt für fairen Vergleich der Attribute</p>
<p><em>Anmerkung</em>: Auch hier ist die Entropie natürlich kein <span class="math align-center">$\operatorname{Gain}(S, A)$</span>. Das Beispiel soll
nur übersichtlich deutlich machen, dass der &quot;Vorteil&quot; von Attributen mit mehr Ausprägungen durch die
Normierung in C4.5 aufgehoben wird.</p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Entscheidungsbaumlerner <strong>ID3</strong>
<ul>
<li>Nutze <em>Information Gain</em> zur Auswahl des nächsten Attributs</li>
<li>Teile die Trainingsmenge entsprechend auf (&quot;nach unten hin&quot;)</li>
</ul>
</li>
<li>Verbesserung durch Normierung des <em>Information Gain</em>: <strong>C4.5</strong></li>
</ul>


    



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit ID3.</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Entscheidungsbäume: Abschnitt 8.4</em></li> <li id='id_Mitchell2010'>[Mitchell2010] <strong>Machine Learning</strong><br>Mitchell, T., McGraw-Hill, 2010. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-0711-5467-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-0711-5467-3</a>.<br><em>ID3: Kapitel 3</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Entscheidungsbäume: Abschnitt 19.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="nn-einführung-in-neuronale-netze">NN: Einführung in Neuronale Netze</h1>

<p>Das Perzeptron kann als die Nachahmung einer biologischen Nervenzelle betrachtet werden.
Durch das Zusammenschließen dieser &quot;künstlichen &quot;Nervenzellen&quot; entstehen künstliche
<strong>Neuronale Netze</strong> (NN), die ähnlich wie das Gehirn <strong>lernen</strong> sollen, komplexe Aufgaben
zu bewerkstelligen.</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn01-perceptron.html">NN01 - Das Perzeptron</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression.html">NN02 - Lineare Regression und Gradientenabstieg</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression.html">NN03 - Logistische Regression</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn04-overfitting.html">NN04 - Overfitting und Regularisierung</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp.html">NN05 - Multilayer Perzeptron</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn06-backprop.html">NN06 - Backpropagation</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing.html">NN07 - Training &amp; Testing</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn08-testing.html">NN08 - Performanzanalyse</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn10-cnn.html">NN10 - Vorschau Deep Learning (CNN, RNN)</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of NN: Einführung in Neuronale Netze</h1>
<article class="default">
<h1>NN01 - Das Perzeptron</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/IJdiwITTC9Y' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN1.1 - Einführung</a></li> <li><a href='https://youtu.be/oWcvFyLgqYc' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN1.2 - Fallstudie und Formalisierung</a></li> <li><a href='https://youtu.be/ZvWpI0Doocc' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN1.3 - Das Perzeptron Modell</a></li> <li><a href='https://youtu.be/8Rdw2NBCCJk' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN1.4 - Perzeptron Beispiel</a></li> <li><a href='https://youtu.be/JD8Qsg8_kQI' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN1.5 - Der Perzeptron Lernalgorithmus</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN01-Das_Perzeptron.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN01-Das_Perzeptron.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="definition-maschinelles-lernen">Definition &quot;Maschinelles Lernen&quot;</h3>
<p>Fähigkeit zu lernen, ohne explizit programmiert zu werden. (Arthur Samuel, 1959)</p>
<h3 id="arten-des-lernens">Arten des Lernens</h3>
<ul>
<li>Überwachtes Lernen (e.g. Klassifizierung, Regression)</li>
<li>Unüberwachtes Lernen (e.g. Clustering, Dimensionsreduktion)</li>
<li>Bestärkendes Lernen (e.g. Schach spielen)</li>
</ul>
<h3 id="formalisierung">Formalisierung</h3>
<ul>
<li>Zielfunktion <span class="math align-center">$f$</span></li>
<li>Merkmalraum (input space)</li>
<li>Ausgaberaum (output space)</li>
<li>Datensatz <span class="math align-center">$\mathcal{D}$</span></li>
<li>Hypothesenmenge <span class="math align-center">$\mathcal{H}$</span></li>
<li>Lernalgorithmus <span class="math align-center">$\mathcal{A}$</span></li>
</ul>
<h3 id="das-perzeptron">Das Perzeptron</h3>
<p>Ein einfaches Modell für die <strong>binäre Klassifizierung</strong></p>
<ul>
<li>Bilde gewichtete Summe (Linearkombination) der Merkmale</li>
<li>Vergleiche das Ergebnis mit einem Schwellenwert
<ul>
<li>Positiv, falls über dem Schwellenwert</li>
<li>Negativ, falls unter dem Schwellenwert</li>
</ul>
</li>
<li>Gewichte und Schwellenwert sind unbekannte Parameter des Modells, die es zu lernen gilt &gt; siehe <strong>Perzeptron Lernalgorithmus</strong></li>
</ul>

    </div>

    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-perceptron.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Perzeptron</a></li></ul>
  </div>
</div>



    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Arten des maschinellen Lernens</li> <li>(K2) Formalisierung eines ML-Problems, insbesondere Klassifizierung: Datensatz, Merkmalraum, Hyphotesenfunktion, Zielfunktion</li> <li>(K2) Perzeptron als linearer Klassifizierer</li> <li>(K2) Entscheidungsgrenze</li> <li>(K3) Berechnung der Entscheidungsgrenze</li> <li>(K3) Perzeptron Lernalgorithmus</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106589&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Intro ML (ILIAS)</a></li></ul>
  </div>
</div>



    



    







<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN02 - Lineare Regression und Gradientenabstieg</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/f-DTaKMnkj4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN2.1 - Lineare Regression - Intro</a></li> <li><a href='https://youtu.be/UnLjjMswNRo' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN2.2 - Vergleich Perzeptron und Bsp</a></li> <li><a href='https://youtu.be/H2YvYIaUW1Q' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN2.3 - Kostenfunktiıon und Gradientenvektor</a></li> <li><a href='https://youtu.be/URaVsZnfppQ' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN2.4 - Berechnung Gradientenvektor - Beispiel</a></li> <li><a href='https://youtu.be/5OZF3Qopous' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN2.5 - Berechnung Gradientenvektor - Allgemein</a></li> <li><a href='https://youtu.be/m-TnM13I-no' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN2.6 - Skalierung der Merkmale</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN02-Lineare_Regression.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN02-Lineare_Regression.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="formalisierung">Formalisierung</h3>
<ul>
<li>Ausgabe <span class="math align-center">$y$</span> ist reelle Zahl aus einem stetigen Bereich (zum Beispiel Hauspreis)</li>
<li>Die <strong>Hypothesenfunktion</strong> ist eine gewichtete Summe der Merkmale <span class="math align-center">$x_i$</span> plus eine Konstante <span class="math align-center">$w_0$</span>:
<span class="math align-center">$$h(\mathbf{x}) = \mathbf{w}^T\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \ldots + w_nx_n$$</span></li>
<li>Der <strong>Verlust</strong> (engl. loss) für einen Datenpunkt <span class="math align-center">$\mathbf{x}$</span> ist das <strong>Fehlerquadrat</strong>:
<span class="math align-center">$$\mathcal{L} = (\hat{y} - y)^2 = (h(\mathbf{x}) - y)^2$$</span></li>
<li>Die Kosten (engl. cost) sind der durchschnittliche Verlust über alle Datenpunkte:
<span class="math align-center">$$J = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y} - y)^2 = \frac{1}{2m} \sum_{i=1}^{m} (h(\mathbf{x}) - y)^2$$</span></li>
</ul>
<h3 id="der-gradient">Der Gradient</h3>
<ul>
<li>Der <strong>Gradientenvektor</strong> <span class="math align-center">$\nabla J(\mathbf{w})$</span> setzt sich zusammen aus den partiellen Ableitungen der Kostenfunktion <span class="math align-center">$J$</span> nach den Gewichten <span class="math align-center">$w_i$</span> und zeigt in jedem Punkt <span class="math align-center">$\mathbf{w}$</span> in die <strong>Richtung des steilsten Aufstiegs</strong>:
<span class="math align-center">$$\nabla J = [ \partial J / \partial w_0
    \quad \partial J / \partial w_1 \quad \ldots
    \quad \partial J / \partial w_n]^T$$</span></li>
<li><strong>Schlussfolgerung</strong>: In die entgegengesetzte Richtung, i.e. in Richtung <span class="math align-center">$-\nabla J(\mathbf{w})$</span> geht es am <em>steilsten bergab!</em></li>
<li><strong>IDEE</strong>: Bewege <span class="math align-center">$\mathbf{w}$</span> in Richtung <span class="math align-center">$-\nabla J(\mathbf{w})$</span>, um die Kosten <span class="math align-center">$J$</span> möglichst schnell zu senken.</li>
</ul>
<h3 id="der-gradientenabstieg-engl-gradient-descent">Der Gradientenabstieg (engl. Gradient Descent)</h3>
<ol>
<li>Starte mit zufälligen Gewichten <span class="math align-center">$\mathbf{w}$</span></li>
<li>Berechne den Gradientenvektor im aktuellen Punkt <span class="math align-center">$\mathbf{w}$</span></li>
<li><strong>Gewichtsaktualisierung</strong>: Gehe einen <em>kleinen</em> Schritt in Richtung <span class="math align-center">$-\nabla J(\mathbf{w})$</span>
<span class="math align-center">$$\mathbf{w} _{neu} := \mathbf{w} _{alt} - \alpha \cdot \nabla J(\mathbf{w} _{alt})$$</span>
(<span class="math align-center">$\alpha$</span>: Lernrate/Schrittweite).</li>
<li>Wiederhole Schritte 2-3, bis das globale Minimum von <span class="math align-center">$J$</span> erreicht ist.</li>
</ol>
<h3 id="graphische-übersicht">Graphische Übersicht</h3>
<ul>
<li>Lineare Regression
<a href="#R-image-8e16ef6d74b8a0ca5464059b1998d44a" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression/lin_reg_nn.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8e16ef6d74b8a0ca5464059b1998d44a"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression/lin_reg_nn.png?width=auto&height=auto"></a></li>
<li>Perzeptron
<a href="#R-image-252e5f57faa0485863482f4871c271eb" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression/perzeptron_nn.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-252e5f57faa0485863482f4871c271eb"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression/perzeptron_nn.png?width=auto&height=auto"></a></li>
</ul>

    </div>

    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-regression.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Lineare / Logistische Regression & Gradientenabstieg</a></li></ul>
  </div>
</div>



    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Lineare Regression aus Sicht neuronaler Netze: Graphische Darstellung, Vergleich mit Perzeptron</li> <li>(K2) Formalisierung</li> <li>(K2) Verlust- und Kostenfunktion</li> <li>(K2) Gradientenvektor</li> <li>(K2) Lernrate</li> <li>(K3) Gradientenabstieg</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106590&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Lineare Regression (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Skalierung der Merkmale</strong></p>
<p>Abbildung 1 und Abbildung 2 zeigen die <a href="https://de.wikipedia.org/wiki/H%C3%B6henlinie" rel="external" target="_blank">Höhenlinien</a> (<a href="https://en.wikipedia.org/wiki/Contour_line" rel="external" target="_blank">Contour Lines</a>) von zwei Kostenfunktionen.</p>
<figure class="center">
    <img src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/images/contour_plot_a.png" alt="Abbildung 1" width="40%" height="auto">
    <figcaption><p>Abbildung 1</p></figcaption>
</figure>
<figure class="center">
    <img src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/images/contour_plot_b.png" alt="Abbildung 2" width="40%" height="auto">
    <figcaption><p>Abbildung 2</p></figcaption>
</figure>
<ul>
<li>Erklären Sie, welcher der beiden Fälle nachteilhaft für den Gradientenabstieg Algorithmus ist. Wo liegt der Nachteil?
Wie kann die Merkmalskalierung dem genannten Nachteil entgegenwirken?</li>
<li>Zeigen Sie unter Verwendung Ihrer eigenen, zufällig generierten Datenpunkte aus dem Bereich
<span class="math align-center">$[100, 300] \times [0, 2]$</span>, wie sich Standardisierung, Min-Max Skalierung und Normalisierung auf die Daten auswirken.
Vergleichen Sie dazu die jeweiligen Streudiagramme (scatterplots). Sie können hierzu das folgende <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/lecture/nn/files/Feature_Scaling_Starter.ipynb" rel="external" target="_blank"><strong>Jupyter Notebook</strong></a> als Startpunkt benutzen.</li>
</ul>
  </div>
</div>



    







<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN03 - Logistische Regression</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/GpJmjrqA5RY' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN3.1 - Logistische Regression - Intro</a></li> <li><a href='https://youtu.be/z-jFZeNWMRc' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN3.2 - Logistische Regression - Hypothesenfunktion und Bsp</a></li> <li><a href='https://youtu.be/ruuCKupOhCE' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN3.3 - Logistische Regression - Verlust und Kosten</a></li> <li><a href='https://youtu.be/kPAZsr-r1LA' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN3.4 - Logistische Regression - Gradientenabstieg</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN03-Logistische_Regression.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN03-Logistische_Regression.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="formalisierung">Formalisierung</h3>
<ul>
<li>
<p>Ausgabe <span class="math align-center">$y$</span> ist reelle Zahl aus dem stetigen Bereich <span class="math align-center">$(0,1)$</span></p>
</li>
<li>
<p>Die <strong>Hypothesenfunktion</strong> ist:
<span class="math align-center">$$h(\mathbf{x}) = \sigma (\mathbf{w}^T\mathbf{x}) = \sigma (w_0 + w_1x_1 + w_2x_2 + \ldots + w_nx_n) \tag{1}$$</span></p>
</li>
<li>
<p>Der <strong>Kreuzentropie Verlust</strong> (engl. Cross-Entropy) für einen Datenpunkt <span class="math align-center">$\mathbf{x}$</span>:
<span class="math align-center">$$\mathcal{L}(a, y) =  - y  \log(a) - (1-y)  \log(1-a)\tag{2}$$</span>
wobei hier <span class="math align-center">$a := \hat{y}$</span> die Vorhersage ist.</p>
</li>
<li>
<p>Die Kosten als durchschnittlicher Verlust über alle Datenpunkte <span class="math align-center">$x^{(1)}, \ldots, x^{(m)}$</span>:
<span class="math align-center">$$J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{3}$$</span></p>
</li>
</ul>
<h3 id="gradientenabstieg">Gradientenabstieg</h3>
<ul>
<li>Der Gradient für einen Datenpunkt <span class="math align-center">$\mathbf{x}$</span>:
<span class="math align-center">$$\frac{\partial \mathcal{L}}{\partial w} = (a-y)x \tag{4}$$</span></li>
<li>Der Gradient für alle Datenpunkte <span class="math align-center">$X$</span> in Matrix-Notation:
<span class="math align-center">$$\nabla J = \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{5}$$</span></li>
</ul>
<h3 id="graphische-übersicht">Graphische Übersicht</h3>
<ul>
<li>Logistische Regression
<a href="#R-image-f73c03f47130a0951134da48531c4bf1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression/log_reg_nn.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f73c03f47130a0951134da48531c4bf1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression/log_reg_nn.png?width=auto&height=auto"></a></li>
<li>Lineare Regression
<a href="#R-image-a4bf47a09aa996c559b1d894e9df62f3" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression/lin_reg_nn.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a4bf47a09aa996c559b1d894e9df62f3"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression/lin_reg_nn.png?width=auto&height=auto"></a></li>
<li>Perzeptron
<a href="#R-image-5843c1cce2a388efe3ab12a435a8eff1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression/perzeptron_nn.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-5843c1cce2a388efe3ab12a435a8eff1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression/perzeptron_nn.png?width=auto&height=auto"></a></li>
</ul>

    </div>

    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-regression.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Lineare / Logistische Regression & Gradientenabstieg</a></li></ul>
  </div>
</div>



    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Logistische Regression aus Sicht neuronaler Netze: Graphische Darstellung, Vergleich mit Perzeptron und linearer Regression</li> <li>(K2) Formalisierung</li> <li>(K2) Sigmoid-Aktivierungsfunktion</li> <li>(K2) Verlust- und Kosten (Cross-Entropy Loss)</li> <li>(K3) Gradientenabstieg für logistische Regression</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106591&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Logistische Regression (ILIAS)</a></li></ul>
  </div>
</div>



    



    







<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN04 - Overfitting und Regularisierung</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/KJLT-h_ChRo' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN4.1 - Nichtlineare Modelle</a></li> <li><a href='https://youtu.be/BW91MYPUH_k' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN4.2 - Overfitting und Regularisierung</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN04-Nichtlineare_Modelle_und_Overfitting.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN04-Nichtlineare_Modelle_und_Overfitting.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="nichtlineare-modelle">Nichtlineare Modelle</h3>
<ul>
<li>Einführung von neuen Merkmalen in Form von nichtlienaren Kombinationen der ursprünglichen Merkmale</li>
<li>Erhöhung der Komplexität des Modells ermöglicht das Erfassen von nichtlinearen Beziehungen</li>
<li><strong>Bemerkung</strong>: Die Hypothesenfunktion bleibt linear in den Gewichten, es wird weiterhin logistische Regression in einem <strong>erweiterten</strong> Merkmalraum durchgeführt.</li>
</ul>
<h3 id="überanpassung-und-regularisierung">Überanpassung und Regularisierung</h3>
<ul>
<li>Die <strong>Überanpassung</strong> (engl. Overfitting) ist eines der häufigsten und wichtigsten Probleme in ML und DL</li>
<li>&quot;Was im Bereich des maschinellen Lernens Professionelle von Amateuren unterscheidet, ist ihre Fähigkeit mit Überanpassung umzugehen.&quot; <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn04-overfitting.html#id_AbuMostafa2012">[AbuMostafa2012, S. 119]</a></li>
<li>Anzeichen von Überanpassung sind geringe Trainingskosten und hohe <strong>Testkosten</strong> (Kosten auf nicht-gesehenen Daten).</li>
<li>Regularisierung ist eine Maßnahme gegen Überanpassung. Man kann es sich als eine Reduktion in der Komplexität des Modells vorstellen.</li>
<li>Der Regularisierungsparameter <span class="math align-center">$\lambda$</span> ist ein Hyperparameter. Je größer der <span class="math align-center">$\lambda$</span>-Wert, desto größer der Regularisierungseffekt.</li>
<li>Die <strong>Kostentenfunktion</strong> bei regulariserter logistischer Regression:
<span class="math align-center">$$J = \frac{1}{m} \left\lbrack \sum_{i=1}^m \left( -y^{[i]}log(a^{[i]})-(1-y^{[i]})log(1-a^{[i]}) \right) + \frac{\lambda}{2} \sum_{j=1}^n (w^2_j)  \right\rbrack \tag{1}$$</span></li>
<li>Die <strong>Gewichtsaktualisierung</strong> mit Regularisierungsterm:
<span class="math align-center">$$w_j := w_j - \frac{\alpha}{m} \left\lbrack \sum_{i=1}^m \left( ( a^{[i]} - y^{[i]} )x_j^{[i]} \right) + \lambda w_j  \right\rbrack \tag{2}$$</span></li>
</ul>

    </div>

    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-mlp.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Overfitting & MLP</a></li></ul>
  </div>
</div>



    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Erhöhung der Modell-Komplexität durch Einführung von Merkmalen höherer Ordnung</li> <li>(K2) Unter- und Überanpassung</li> <li>(K2) Regularisierung (Auswirkung auf Gewichte und Modell)</li> <li>(K3) Gradientenabstieg für regularisierte logistische Regression</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106595&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Overfitting (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_AbuMostafa2012'>[AbuMostafa2012] <a href='https://work.caltech.edu/telecourse' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Learning From Data</strong></a><br>Abu-Mostafa, Y. S. und Magdon-Ismail, M. und Lin, H., AMLBook, 2012. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-1-6004-9006-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-1-6004-9006-4</a>.<br><em>Kapitel 4</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN05 - Multilayer Perzeptron</h1>



    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/7ltwa5WWuKI' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN5.1 - MLP Forward Propagation</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN05-MLP.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN05-MLP.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="multilayer-perzeptron-mlp">Multilayer Perzeptron (MLP)</h3>
<ul>
<li>Das Perzeptron kann nur linear separable Daten korrekt klassifizieren.</li>
<li>Durch das Zusammenschließen von mehreren Perzeptronen kann man ein mehrschichtiges Perzeptron (engl. Multilayer Perceptron) aufstellen, das komplexere Funktionen modellieren kann.</li>
<li>Ein MLP wird oft auch als <strong>Feed Forward Neural Network</strong> oder als <strong>Fully Connected Neural Network</strong> bezeichnet.</li>
<li>Die &quot;inneren&quot; Schichten eines solchen Netzwerkes sind sogenannte <strong>versteckte Schichten</strong> (engl. hidden layer). Das sind alle Schichten ausgenommen die Eingangs- und Ausgangsschicht.</li>
</ul>
<h3 id="graphische-übersicht-und-vorwärtslauf">Graphische Übersicht und Vorwärtslauf</h3>
<ul>
<li>Ein Multi-Layer Perzeptron
<a href="#R-image-a8c815d9a0bbf570186fb8fae000069d" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp/mlp.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a8c815d9a0bbf570186fb8fae000069d"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp/mlp.png?width=auto&height=auto"></a>
Ein Vorwärtslauf (forward pass):
<span class="math align-center">$$a^{[1]} = ReLU \left( W^{[1]} \cdot \mathbb{x} + b^{[1]} \right) \tag{1}$$</span>
<span class="math align-center">$$\hat{y} := a^{[2]} = \sigma \left( W^{[2]} \cdot a^{[1]} + b^{[2]} \right) \tag{2}$$</span></li>
</ul>

    </div>

    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-mlp.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Overfitting & MLP</a></li></ul>
  </div>
</div>



    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Multi-Layer Perzeptron (MLP): Graphische Darstellung, Vorwärtslauf</li> <li>(K2) Aktivierungsfunktionen (insbesondere ReLU)</li> <li>(K3) Vorwärtslauf (forward pass) für ein gegebenes MLP</li> <li>(K3) Berechnung der einzelnen Aktivierungen</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106592&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Multilayer Perzeptron (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Lineares MLP</strong></p>
<p>Gegeben sei ein MLP mit linearen Aktivierungsfunktionen, d.h. für jedes Neuron berechnet sich der
Output durch die gewichtete Summe der Inputs: <span class="math align-center">$y = g(w^T x)$</span>, wobei <span class="math align-center">$g(z) = z$</span> gilt, also <span class="math align-center">$y = w^T x$</span>.
Zeigen Sie, dass dieses Netz durch eine einzige Schicht mit linearen Neuronen ersetzt werden kann.</p>
<p>Betrachten Sie dazu ein zwei-schichtiges Netz (i.e. bestehend aus Eingabe-Schicht, Ausgabe-Schicht und einer versteckten Schicht)
und schreiben Sie die Gleichung auf, die die Ausgabe als Funktion der Eingabe darstellt.</p>
<p>Als Beispiel sei das zwei-schichtige MLP mit den folgenden Gewichten und Bias-Werten gegeben:</p>
<p>Schicht 1: <span class="math align-center">$W_1 = [[2, 2],[3, -2]]$</span>, <span class="math align-center">$b_1 = [[1],[-1]]$</span>
Schicht 2: <span class="math align-center">$W_2 = [[-2, 2]]$</span>, <span class="math align-center">$b_2 = [[-1]]$</span></p>
<ul>
<li>Stellen Sie dieses Netzwerk graphisch dar. Was ist die Anzahl der Zellen in den einzelnen Schichten?</li>
<li>Berechnen Sie die Ausgabe für eine Beispiel-Eingabe Ihrer Wahl.</li>
<li>Stellen Sie ein ein-schichtiges Netz auf, das für jede Eingabe die gleiche Ausgabe wie das obige Netzwerk berechnet und es somit ersetzen könnte.</li>
</ul>
  </div>
</div>



    







<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN06 - Backpropagation</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/G9x75THjueQ' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN6.1 - MLP Backpropagation 1</a></li> <li><a href='https://youtu.be/9Ku0dJ8pGrU' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN6.2 - MLP Backpropagation 2</a></li> <li><a href='https://youtu.be/uvT4WPIIkwQ' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN6.3 - MLP Zusammenfassung</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN06-MLP_Backpropagation.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN06-MLP_Backpropagation.pdf</a></li> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN06.2-MLP_Backpropagation_Beispiel.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN06.2-MLP_Backpropagation_Beispiel.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="forwärts--und-rückwärtslauf">Forwärts- und Rückwärtslauf</h3>
<ul>
<li>
<p>Im Forwärtslauf (engl. forward pass oder forward propagation) wird ein einzelner <strong>Forwärtsschritt</strong> von Schicht <span class="math align-center">$[l-1]$</span> auf Schicht <span class="math align-center">$[l]$</span> wie folgt berechnet:
<span class="math align-center">$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \tag{1}$$</span>
<span class="math align-center">$$A^{[l]} = g(Z^{[l]}) \tag{2}$$</span>
Dabei bezeichnet <span class="math align-center">$g$</span> die Aktivierungsfunktion (z.B. Sigmoid oder ReLU).</p>
</li>
<li>
<p>Im Rückwärtslauf (engl. <em>backpropagation</em>) werden in einem einzelnen <strong>Rückwärtsschritt</strong> von Schicht <span class="math align-center">$[l]$</span> auf Schicht <span class="math align-center">$[l-1]$</span> die folgenden Gradienten berechnet:</p>
<span class="math align-center">$$dZ^{[l]} := \frac{\partial J }{\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) \tag{3}$$</span>
<span class="math align-center">$$dW^{[l]} := \frac{\partial J }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{4}$$</span>
<span class="math align-center">$$db^{[l]} := \frac{\partial J }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\tag{5}$$</span>
<span class="math align-center">$$dA^{[l-1]} := \frac{\partial J }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{6}$$</span>
<p>Dabei steht &quot;<span class="math align-center">$*$</span>&quot; für die elementweise Multiplikation.</p>
</li>
<li>
<p>Beachten Sie:</p>
<ul>
<li>Der Forwärtsschirtt übernimmt <span class="math align-center">$A^{[l-1]}$</span> von dem vorherigen Schritt und gibt <span class="math align-center">$A^{[l]}$</span> an den nächsten Schritt weiter.</li>
<li>Der Rückwärtschritt übernimmt <span class="math align-center">$dA^{[l]}$</span> von dem vorherigen Schritt und gibt <span class="math align-center">$dA^{[l-1]}$</span> an den nächsten Rückwärtsschritt weiter.</li>
</ul>
</li>
</ul>
<h3 id="parameteraktualisierung">Parameteraktualisierung</h3>
<ul>
<li>Die Aktualisierung der Parameter in Schicht <span class="math align-center">$l$</span> erfolgt wie gewohnt durch:
<span class="math align-center">$$W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{7}$$</span>
<span class="math align-center">$$b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{8}$$</span>
Dabei bezeichnet <span class="math align-center">$\alpha$</span> die Lernrate.</li>
</ul>

    </div>

    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-backprop.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Backpropagation</a></li></ul>
  </div>
</div>



    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Forwärts- und Rückwärtslauf in Matrix Notation mit mehreren Datenpunkten als Eingabe</li> <li>(K2) Ableitung der Aktivierungsfunktionen</li> <li>(K3) Berechnung der partiellen Ableitungen</li> <li>(K3) Rückwärtslauf (backpropagation) für ein gegebenes MLP</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106593&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Backpropagation (ILIAS)</a></li></ul>
  </div>
</div>



    



    







<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN07 - Training &amp; Testing</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/PUw-TvLJULI' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN7.1 - Training, Testing, Validierung</a></li> <li><a href='https://youtu.be/DqjdZ8HaDSo' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN7.2 - Kreuzvalidierung</a></li> <li><a href='https://youtu.be/7XATTMNI-gI' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN7.3 - Beispiel</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN07-Testing-Validierung.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN07-Testing-Validierung.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="training-und-testing">Training und Testing</h3>
<ul>
<li>
<p>Der tatsächliche <strong>Erfolg</strong> eines Modells wird nicht durch niedrige Trainingskosten gemessen, sondern durch geringe Kosten auf ungesehenen Daten, d.h. <strong>hohe Vorhersagekraft, gute Generalisierung</strong>!</p>
</li>
<li>
<p>Die Menge aller gelabelten Daten in <strong>Trainingsset und Testset</strong> aufteilen, Testset nicht während des Trainings einsetzen!.</p>
<ul>
<li><span class="math align-center">$E_{in}$</span> bezeichnet den Fehler auf dem Trainingsset, auch <strong>in-sample error</strong>.</li>
<li><span class="math align-center">$E_{out}$</span> bezeichnet den Fehler auf dem gesamten Eingaberaum <span class="math align-center">$X$</span>, auch <strong>out-of-sample error</strong>. <span class="math align-center">$E_{out}$</span> ist der eigentliche Indikator für den zukünftigen Erfolg des Modells, ist uns aber nicht zugänglich.</li>
<li><span class="math align-center">$E_{test}$</span> bezeichnet den Fehler auf dem Testset und ist eine <strong>Näherung</strong> für <span class="math align-center">$E_{out}$</span>.</li>
</ul>
<blockquote>
<p>Analogie:<br>
<span class="math align-center">$E_{in}$</span> : Erfolg in Übungsaufgaben und Probeprüfungen.<br>
<span class="math align-center">$E_{test}$</span> : Erfolg in Endprüfung.</p>
</blockquote>
</li>
<li>
<p>Die Näherung <span class="math align-center">$E_{test}$</span> sollte möglichst genau sein, damit es als ein verlässliches <strong>Gütesiegel</strong> dienen kann.</p>
<ul>
<li>Das Testset sollte genug Daten enthalten. Üblicher Anteil an Testdaten:
<ul>
<li>bei <span class="math align-center">$|D| \approx 100.000 \rightarrow$</span> ca. 20%</li>
<li>bei <span class="math align-center">$|D| \approx 10.000.000 \rightarrow$</span> ca. 1%</li>
<li>Beispiel: Hat man 1000 Beispiele im Testset, wird <span class="math align-center">$E_{test}$</span> mit <span class="math align-center">$\ge 98\%$</span> Wahrscheinlichkeit in der <span class="math align-center">$\pm 5\%$</span> Umgebung von <span class="math align-center">$E_{out}$</span> liegen (für theoretische Grundlagen und Herleitung siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing.html#id_AbuMostafa2012">[AbuMostafa2012, S. 39-69]</a>).</li>
</ul>
</li>
<li>Trainingsdaten und Testdaten sollten möglichst aus derselben Verteilung kommen, wie die zukünftigen <strong>Real-World-Daten</strong>.</li>
</ul>
</li>
<li>
<p><strong>Wichtige Bemerkung</strong>:</p>
<ul>
<li>Testdaten nicht anfassen, bis das Modell Einsatzbereit ist!</li>
<li>Die Testdaten dürfen in <strong>keinster Weise</strong> bei der Auswahl der endgültigen Hypothese eingesetzt werden, weder bei der Berechnung der Parameter (Training), noch bei der Bestimmung der Hyperparameter (Hyperparameter-Tuning).</li>
<li>Sobald der Testfehler die Auswahl der endgültigen Hypothese beeinflusst, kann sie nicht mehr als &quot;Gütesiegel&quot; eingesetzt werden.<br>
<strong>CHECK</strong>: Hätte man zufällig andere Testdaten gewählt, könnte sich dadurch die endgültige Hypothese ändern?</li>
</ul>
</li>
</ul>
<h3 id="validierung-und-modellauswahl">Validierung und Modellauswahl</h3>
<ul>
<li>
<p>Das Ziel ist es, das Modell mit bester Generalisierung, also kleinstem <span class="math align-center">$E_{out}$</span> zu bestimmen. <span class="math align-center">$E_{out}$</span> ist jedoch unbekannt und die Näherung <span class="math align-center">$E_{test}$</span> <em>darf nicht</em> bei der Modellauswahl eingesetzt werden.</p>
</li>
<li>
<p>LÖSUNG: Einen weiteren Teil der Daten als <strong>Validierungsset</strong> (auch <em>development set</em>) beiseitelegen und nicht für das Training (i.e. Minimierung des Trainingsfehlers <span class="math align-center">$E_{in}$</span>) verwenden!</p>
</li>
<li>
<p><strong>Bemerkung</strong>:<br>
Das Wort <strong>Modell</strong> kann je nach Kontext unterschiedliche Bedeutungen annehmen.<br>
Ein Modell im aktuellen Kontext ist als ein Paar <span class="math align-center">$(\mathcal{H},\mathcal{A})$</span> von Hypothesenraum (bzw. <strong>Modellarchitektur</strong>) und <strong>Lernalgorithmus</strong> definiert.</p>
<ul>
<li>Die Auswahl eines Modells kann aus einer Menge von Modellen unterschiedlicher Art erfolgen (z.B. lineare Modelle, polynomiale Modelle, neuronale Netze), oder von Modellen derselben Art aber mit unterschiedlichen Hyperparametern (z.B. Neuronale Netze mit unterschiedlicher Anzahl von versteckten Schichten).</li>
<li>Außerdem kann dieselbe Modellarchitektur <span class="math align-center">$\mathcal{H}$</span> mit unterschiedlichen Lernalgorithmen trainiert werden, was wiederum die endgültige Hypothese beeinflussen kann. Die Bestimmung der Hyperparameter von <span class="math align-center">${\mathcal{A}}$</span> (wie z.B. Optimierungsfunktion, Lernrate, Kostenfunktion, Regularisierungsparameter usw.) sind daher auch Teil der Modellauswahl.</li>
</ul>
</li>
<li>
<p>Der <strong>Validierungsfehler <span class="math align-center">$E_{val}$</span></strong> kann nun als Entscheidungsgrundlage an verschiedenen Stellen des Lernrpozesses eingesetzt werden, wie zum Beispiel:</p>
<ul>
<li>Bei der <strong>Auswahl geeigneter Hyperparameter</strong> wie z.B. Anzahl Schichten, Anzahl Zellen/Schicht, Aktivierungsfunktion, Regularisierungsparameter (siehe Abbildung 1).</li>
</ul>
<figure class="center">
    <img src="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing/val1.png" alt="Abbildung 1 - Einsatz der Validierung für das Hyperparameter-Tuning" width="auto" height="auto">
    <figcaption><p>Abbildung 1 - Einsatz der Validierung für das Hyperparameter-Tuning</p></figcaption>
</figure>
<ul>
<li>Bei der <strong>Auswahl der endgültigen Hypothese</strong> (<span class="math align-center">$\rightarrow$</span> Parameterauswahl!): unter allen Hypothesen, die während des Trainings durchlafen werden, wähle jene mit kleinstem <span class="math align-center">$E_{val}$</span> (siehe Abbildung 2).</li>
</ul>
<figure class="center">
    <img src="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing/val2.png" alt="Abbildung 2 - Einsatz der Validierung bei der Auswahl der entgültigen Hypothese" width="auto" height="auto">
    <figcaption><p>Abbildung 2 - Einsatz der Validierung bei der Auswahl der entgültigen Hypothese</p></figcaption>
</figure>
<ul>
<li>Bei der graphischen <strong>Darstellung von Lernkurven</strong> für die Diagnose von Über- und Unteranpassung (siehe Abbildung 3).</li>
</ul>
<figure class="center">
    <img src="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing/val3.png" alt="Abbildung 3 - Lernkurven" width="auto" height="auto">
    <figcaption><p>Abbildung 3 - Lernkurven</p></figcaption>
</figure>
</li>
<li>
<p>Übliche train/val/test Aufteilung der Daten (in Prozent):</p>
<ul>
<li>bei <span class="math align-center">$|D| \approx 100.000 \rightarrow$</span> ca. 60/20/20</li>
<li>bei <span class="math align-center">$|D| \approx 10.000.000 \rightarrow$</span> ca. 98/1/1</li>
</ul>
</li>
<li>
<p><strong>Bemerkung</strong>:<br>
Das Modell ist trainiert für gute Ergebnisse auf Trainingsdaten und &quot;fine-tuned&quot; für gute Ergebnisse auf den Validierungsdaten. Ergebnisse auf Testdaten werden mit hoher wahrscheinlichkeit schlechter ausfallen, als auf Validierungsdaten (<span class="math align-center">$E_{val}$</span> ist eine zu optimistische Näherung).</p>
</li>
<li>
<p>Sind Validierungs- und/oder Trainingsset zu klein, führt das zu schlechten Näherungen <span class="math align-center">$E_{val}$</span> und folglich zu schlechten Entscheidungen.</p>
<ul>
<li>Bei der Aufteilung muss ein gutes Trade-off gefunden werden.</li>
<li>Wenn kein Gütesiegel notwendig ist, kann man auf das Testset verzichten und die Daten in Trainings- und Validierungsset aufteilen.</li>
<li>Für eine bessere Näherung mit weniger Validierungsdaten kann k-fache Kreuzvalidierung eingesetzt werden (wenn genug Rechenkapazität vorhanden ist).</li>
</ul>
</li>
</ul>
<h3 id="k-fache-kreuzvalidierung-engl-k-fold-cross-validation">K-fache Kreuzvalidierung (engl. k-fold cross-validation):</h3>
<ul>
<li>
<p>Das Modell <span class="math align-center">$(\mathcal{H_m},\mathcal{A_m})$</span> wird <span class="math align-center">$k$</span> mal trainiert und validiert, jedes mal mit unterschiedlichen Trainings- und Validierungsmengen:</p>
<ul>
<li>
<p>Die Trainingsdaten werden in <span class="math align-center">$k$</span> disjunkte Teilmengen <span class="math align-center">$D_1, D_2, ..., D_k$</span> aufgeteilt.</p>
</li>
<li>
<p>Bei dem <span class="math align-center">$i$</span>-ten Training werden die Teilmenge <span class="math align-center">$D_i$</span> für die Berechnung des Validierungsfehlers <span class="math align-center">$e_i := E_{val}(h_m^{*(i)})$</span> und die restlichen <span class="math align-center">$k-1$</span> Teilmengen für das Training verwendet.</p>
</li>
<li>
<p>Der <strong>Kreuzvalidierungsfehler</strong> des Modells <span class="math align-center">$(\mathcal{H_m},\mathcal{A_m})$</span> ist der Durchschnitt der <span class="math align-center">$k$</span> Validierungsfehler <span class="math align-center">$e_1, e_2, ..., e_k$</span> (siehe Abbildung 4).
<span class="math align-center">$$E_{CV}(m) := \frac{1}{k} \sum_{i=1}^{k} e_i = \frac{1}{k} \sum_{i=1}^{k} E_{val}(h_m^{*(i)})$$</span></p>
</li>
</ul>
<figure class="center">
    <img src="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing/val4.png" alt="Abbildung 4 - Kreuzvalidierung" width="auto" height="auto">
    <figcaption><p>Abbildung 4 - Kreuzvalidierung</p></figcaption>
</figure>
</li>
<li>
<p>Bemerkung: Die Kreuzvalidierung wird nur bei der Modellauswahl eingesetzt: es liefert verlässlichere Näherungen für <span class="math align-center">$E_{out}$</span> und führt daher zu besseren Entscheidungen. Das zuletzt ausgewählte Modell wird danach wie gewohnt auf den gesamten Trainigsdaten (ausgenommen Testdaten) trainiert und zum Schluss mit den Testdaten evaluiert.</p>
</li>
</ul>

    </div>

    




    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Trainings-, Validierungs- und Testfehler</li> <li>(K2) Zweck einer Testmenge</li> <li>(K2) Kreuzvalidierung</li> <li>(K2) Hyperparameter-Tuning</li> <li>(K2) Lernkurven</li></ul>
  </div>
</div>



    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106594&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Training & Testing (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_AbuMostafa2012'>[AbuMostafa2012] <a href='https://work.caltech.edu/telecourse' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Learning From Data</strong></a><br>Abu-Mostafa, Y. S. und Magdon-Ismail, M. und Lin, H., AMLBook, 2012. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-1-6004-9006-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-1-6004-9006-4</a>.</li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN08 - Performanzanalyse</h1>



    
    
    
    





    
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/T-WYL28iwdU' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN8.1 - Confusion Matrix</a></li> <li><a href='https://youtu.be/fpsNzn4Moow' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN8.2 - Precision und Recall</a></li> <li><a href='https://youtu.be/Wx_HAuIXTAQ' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN8.3 - Precision Recall Trade-off</a></li> <li><a href='https://youtu.be/UAV7EpdIe6Q' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN8.4 - F1-Score</a></li> <li><a href='https://youtu.be/vsmoYiArtrA' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN8.5 - Harmonisches Mittel- Intuition</a></li></ul>
  </div>
</div>




    
    




    
    
        
        
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="far fa-file-powerpoint"></i> Folien
  </div>
  <div class="box-content">
<ul> <li><a href='https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/nn/files/NN08-Performanzanalyse.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>NN08-Performanzanalyse.pdf</a></li></ul>
  </div>
</div>




    <div class="recap">
        <h2 id="kurze-übersicht">Kurze Übersicht</h2>
<h3 id="performanzmetriken-für-klassifizierungsprobleme">Performanzmetriken für Klassifizierungsprobleme</h3>
<h4 id="wahrheitsmatrix-engl-confusion-matrix">Wahrheitsmatrix (engl. Confusion Matrix)</h4>
<ul>
<li>Gibt eine Übersicht über die Anzahl von richtig und falsch klassifizierten Datenpunkten (bei binärer Klassifizierung)
<ul>
<li><span class="math align-center">$TP =$</span> # True Positives <span class="math align-center">$=$</span> Anzahl richtiger 1-Vorhersagen</li>
<li><span class="math align-center">$FP =$</span> # False Positives <span class="math align-center">$=$</span> Anzahl falscher 1-Vorhersagen</li>
<li><span class="math align-center">$FN =$</span> # False Negatives <span class="math align-center">$=$</span> Anzahl falscher 0-Vorhersagen</li>
<li><span class="math align-center">$TN =$</span> # True Negatives <span class="math align-center">$=$</span> Anzahl richtiger 0-Vorhersagen</li>
</ul>
</li>
<li>Bei Klassifizierungsproblemen mit <span class="math align-center">$N$</span> Klassen hat man eine <span class="math align-center">$N \times N$</span> Matrix, die in Position <span class="math align-center">$(i,j)$</span> die Anzahl der Klasse-<span class="math align-center">$j$</span>-Beispiele enthält, die als Klasse-<span class="math align-center">$i$</span> vorhergesagt wurden.</li>
</ul>
<figure class="center">
    <img src="https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn08-testing/nn8-1.png" alt="Abbildung 1 - Wahrheitsmatrix bei binärer Klassifizierung" width="auto" height="auto">
    <figcaption><p>Abbildung 1 - Wahrheitsmatrix bei binärer Klassifizierung</p></figcaption>
</figure>
<h4 id="treffergenauigkeit-engl-accuracy">Treffergenauigkeit (engl. Accuracy)</h4>
<ul>
<li>
<p>Anzahl richtig klassifizierter Datenpunkte, Erfolgsrate (engl. correct rate)
<span class="math align-center">$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$</span></p>
</li>
<li>
<p>Accuracy vermittelt ein falsches Bild des Erfolges bei unausgewogenen Datensätzen<br>
Beispiel:</p>
<ul>
<li>Klasse 1 hat 10, Klasse 0 hat 990 Beispiele.</li>
<li>Ein Modell, das immer 0 ausgibt, hat <span class="math align-center">$990/1000 = 0.99$</span> Treffergenauigkeit, ist aber offensichtlich kein gutes Modell!</li>
</ul>
</li>
</ul>
<h4 id="precision">Precision</h4>
<ul>
<li>Positive Predictive Value (PPV)</li>
<li>Antwort auf: Von allen <strong>positiven Vorhersagen</strong>, wie viele sind richtig?
<span class="math align-center">$$Precision = \frac{TP}{TP + FP}$$</span></li>
<li>Wahrscheinlichkeit, dass ein positiv klassifiziertes Beispiel auch tatsächlich positiv ist.</li>
<li>Je näher an 1, desto besser.</li>
<li>Accuracy of <strong>positive predictions</strong>.</li>
</ul>
<h4 id="recall">Recall</h4>
<ul>
<li>True Positive Rate, auch Sensitivität (engl. Sensitivity)</li>
<li>Antwort auf: Von allen <strong>positiven Beispielen</strong>, wie viele wurden richtig klassifiziert?
<span class="math align-center">$$Recall = \frac{TP}{TP + FN}$$</span></li>
<li>Wahrscheinlichkeit, dass ein positives Beispiel tatsächlich als solches erkannt wird.</li>
<li>Je näher an 1, desto besser.</li>
<li>Accuracy of <strong>positive examples</strong>.</li>
</ul>
<h4 id="precision-recall-trade-off">Precision-Recall Trade-off</h4>
<ul>
<li>Ein gutes Modell sollte hohe Precision und zugleich hohes Recall haben.</li>
<li>Man kann die Precision eines Modells beliebig erhöhen (durch das Vergrößern des Schwellenwertes bei der Klassifizierung), jedoch wird dabei der Recall abnehmen.</li>
<li>Genau so kann man den Recall eines Modells beliebig erhöhen (durch das Verkleinern des Schwellenwertes bei der Klassifizierung), jedoch wird dabei die Precision abnehmen.</li>
<li>Es gilt ein gutes Trade-off zu finden.</li>
<li>Eine Zwei-Zahlen-Metrik erschwert den Entscheidungsprozess bei Evaluierung und Modellauswahl.</li>
</ul>
<h4 id="hahahugoshortcode49s18hbhb-score-harmonisches-mittel"><span class="math align-center">$F_1$</span>-Score (Harmonisches Mittel)</h4>
<ul>
<li>Fasst Precision (P) und Recall (R) in einer Metrik zusammen (Harmonisches Mittel von P und R):
<span class="math align-center">$$F_1-Score = \frac{2}{\frac{1}{P} + \frac{1}{R}} = 2 \cdot \frac{PR}{P + R}$$</span></li>
<li>Der <span class="math align-center">$F_1$</span>-Score wird nur dann hoch sein, wenn P und R beide hoch sind.</li>
<li>Je näher an 1, desto besser.</li>
<li>Sehr kleine P und R Werte ziehen den <span class="math align-center">$F_1$</span>-Score sehr stark herunter. In dieser Hinsicht gibt diese Metrik ein akkurates Bild über den Erfolg eines Modells.</li>
</ul>

    </div>

    




    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Performanzmetriken für die Evaluierung von Klassifizierungsmodellen</li> <li>(K2) Wahrheitsmatrix (engl. Confusion Matrix)</li> <li>(K2) Treffergenauigkeit (engl. Accuracy)</li> <li>(K2) Precision (engl. Precision)</li> <li>(K2) Recall</li> <li>(K2) <span class="math align-center">$F_1$</span>-Score (Harmonisches Mittel)</li> <li>(K3) Berechnung und Deutung von Precision und Recall</li> <li>(K3) Berechnung und Deutung des <span class="math align-center">$F_1$</span>-Scores</li> <li>(K3) Einsatz bei Evaluierung und Auswahl von Modellen</li></ul>
  </div>
</div>



    



    



    







<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>NN10 - Vorschau Deep Learning (CNN, RNN)</h1>



    <div style="text-align:center;">
    Inhalt befindet sich im Aufbau<br>
    und wird rechtzeitig bereitgestellt.
</div>




<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="suche">Suche</h1>

<p>Problemlösen durch Suche im Problemgraphen. Aus den Basisalgorithmen Tree-Search und Graph-Search
entstehen je nach verwendeter Datenstruktur und nach betrachteten Kosten unterschiedliche Suchalgorithmen.</p>
<ul>
<li>Uninformierte Suche: ... jeder Schritt &quot;kostet&quot; gleich viel: nur die Anzahl der Schritte zählt ...</li>
<li>Informierte Suche: ... Einsatz einer Kostenfunktion ...</li>
<li>Lokale Suche: ... das Ziel ist im Weg ...</li>
</ul>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html">Suche mit Tiefensuche</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Suche mit Breitensuche</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html">Suche mit Branch-and-Bound</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html">Suche mit Best First</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">Suche mit A*</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html">Lokale Suche: Gradientensuche</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing.html">Lokale Suche: Simulated Annealing</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Suche</h1>
<article class="default">
<h1>Suche mit Tiefensuche</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Die Tiefensuche gehört zu den &quot;Uninformierten Suchverfahren&quot;: Es werden keine weiteren
Pfadkosten, sondern nur die Anzahl der Schritte berücksichtigt.</p>
<p>Die Tiefensuche entsteht, wenn man bei der Tree-Search oder der Graph-Search für die
Datenstruktur einen <strong>Stack</strong> benutzt: Expandierte Nachfolger werden immer <strong>oben</strong>
auf den Stack gelegt, und der nächste zu expandierende Knoten wird <strong>oben</strong> vom Stack
genommen. Dadurch verfolgt die Tiefensuche einen Pfad immer erst in die Tiefe.</p>
<p>Bei Sackgassen erfolgt automatisch ein Backtracking, d.h. es wird zum letzten Knoten mit
einer Alternative zurückgegangen. Dies liegt daran, dass bei einer Sackgasse keine
Nachfolger expandiert und oben auf den Stack gelegt werden.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/NzTugnuHSZ8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Tiefensuche</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/01a264be691d64ecb2ca389006177bec25cc3d31a750c648e0d7450d07cbf907dd98e0248ef97d40999f7fb06355f5235827bf05f3a087b8e2511bbd68d2d5da' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Tiefensuche</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Verwendete Datenstrukturen</li> <li>(K2) Algorithmische Abläufe, Terminierung</li> <li>(K2) Optimalität, Vollständigkeit und Komplexität</li> <li>(K3) Uninformierte Suchverfahren: Tiefensuche</li></ul>
  </div>
</div>




    <h2 id="hole-das-buch">Hole das Buch</h2>
<div class='center'>
<p><a href="#R-image-348d0142d2e1f5f4f11fa3620caf09be" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs/scene.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-348d0142d2e1f5f4f11fa3620caf09be"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs/scene.png?width=60%25&height=auto"></a></p>
</div>
<p>Das Beispiel ist ein Büroflur in der Uni. Neben den Büros gibt es eine Bibliothek
und einen Kopiererraum, wo auch der Roboter sich gerade aufhält. Die Aufgabe für
den Roboter lautet: Hole das Buch aus der Bibliothek (und bringe es zum Kopier).
(Damit das Beispiel und der sich daraus ergebende Problemgraph nicht zu groß und zu
unübersichtlich werden, soll das Ziel hier darin liegen, dass der Roboter das Buch
in der Bibliothek aufnimmt.)</p>
<p>Es stehen zwei Aktionen zur Verfügung:</p>
<ol>
<li>Der Roboter kann von einem in den nächsten Raum wechseln (Kosten siehe Pfeile)</li>
<li>Der Roboter kann das Buch aufnehmen (Kosten: 3)</li>
</ol>
<p>Dabei sind die Durchgänge teilweise nur in einer Richtung zu benutzen (Pfeilrichtung).</p>
<h2 id="problemgraph-zum-kopiererbeispiel">Problemgraph zum Kopiererbeispiel</h2>
<div class='center'>
<p><a href="#R-image-591960b0940a6adacfe3c79607bb7039" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs/graph.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-591960b0940a6adacfe3c79607bb7039"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs/graph.png?width=60%25&height=auto"></a></p>
</div>
<p>=&gt; <span class='alert'><strong>Problemlösen == Suche im Graphen</strong></span></p>
<p><strong>Uninformierte (&quot;blinde&quot;) Suche</strong>:</p>
<p>Keine Informationen über die Kosten eines Pfades: Nur die <span class='alert'>Pfadlänge</span> (Anzahl der Schritte) zählt.</p>
<p>Varianten:</p>
<ul>
<li><strong>Tiefensuche</strong></li>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Breitensuche</a></li>
</ul>
<h2 id="anmerkungen-wegesuche-landkarte">Anmerkungen Wegesuche (Landkarte)</h2>
<p>Bei der Wegesuche hat man den Problemgraphen bereits durch die Orte und die Verbindungen (Straßen)
zwischen ihnen gegeben. Es gibt nur eine ausführbare Aktion: &quot;<em>fahre nach</em>&quot;.</p>
<p>Dabei können nur die Anzahl der Zwischenstationen auf dem Weg gezählt werden (&quot;uninformierte
Suche&quot;), oder man ordnet den Kanten Kosten zu (bei der Wegesuche wären dies die Entfernungen
zwischen den Orten oder die Zeit, die man von A nach B braucht) und landet damit bei der
&quot;informierten Suche&quot;.</p>
<p>Normalerweise hat man eine Ordnung auf den Aktionen, d.h. für einen Knoten ergibt sich daraus
eine Reihenfolge, in der die Aktionen angewendet werden und die Nachfolger expandiert werden.
Bei der Wegesuche hat man dies nicht, insofern muss man willkürlich eine Ordnung festlegen.
In dieser Veranstaltung ist dies die alphabetische Reihenfolge der Knoten (Orte).</p>
<h2 id="tiefensuche-ts-dfs">Tiefensuche (<em>TS</em>, <em>DFS</em>)</h2>
<p><strong>Erinnerung Tree-Search</strong></p>
<ol>
<li>Füge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein</li>
<li>Entnehme Knoten aus der Datenstruktur:
<ul>
<li>Knoten ist gesuchtes Element: Abbruch, melde &quot;<em>gefunden</em>&quot;</li>
<li>Expandiere alle Nachfolger des Knotens und füge diese in die
Datenstruktur ein</li>
</ul>
</li>
<li>Falls die Datenstruktur leer ist: Abbruch, melde &quot;<em>nicht gefunden</em>&quot;</li>
<li>Gehe zu Schritt 2</li>
</ol>
<p>=&gt; Was passiert, wenn wir einen <span class='alert'><strong>Stack</strong></span> einsetzen?</p>
<div class='center'>
<p><a href="#R-image-4a908f07e4f27b2d866c28abc57b2df9" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs/tafelbeispiel.png?width=90%25&height=auto" style=" height: auto; width: 90%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-4a908f07e4f27b2d866c28abc57b2df9"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs/tafelbeispiel.png?width=90%25&height=auto"></a></p>
</div>
<h2 id="bemerkungen">Bemerkungen</h2>
<ul>
<li>
<p>Nachfolger eines Knotens: Alle von diesem Zustand durch Aktionen erreichbare Zustände</p>
</li>
<li>
<p>Suchalgorithmus mit <span class='alert'><strong>Stack</strong></span> als Datenstruktur =&gt; <strong>Tiefensuche</strong></p>
<ul>
<li>Zu betrachtender Knoten in Schritt 2 wird <em>oben</em> vom Stack genommen</li>
<li>Expandierte Knoten werden in Schritt 2.a <em>oben</em> auf den Stack gelegt
Dabei i.A. die vorgegebene Reihenfolge der Nachfolgeknoten beachten!</li>
</ul>
<p>Auswirkung: Weg wird in die <strong>Tiefe</strong> verfolgt (deshalb &quot;Tiefensuche&quot;)</p>
</li>
<li>
<p>Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html#id_Russell2020">[Russell2020]</a> wird die Datenstruktur zum Halten der zu expandierenden Knoten (also
hier im Fall der Tiefensuche der Stack) auch &quot;<strong>Frontier</strong>&quot; genannt.</p>
</li>
<li>
<p><strong>Backtracking</strong>: Wenn der Weg in eine Sackgasse führt, d.h. ein Knoten
keine Nachfolger hat, werden bei der Expansion des Knotens keine Nachfolger
auf den Stack gelegt. Die Evaluation des nächsten Knotens auf dem Stack
bewirkt deshalb ein <em>Zurückspringen</em> im Suchbaum zum letzten Knoten auf dem
aktuellen Weg mit noch offenen Alternativen ...</p>
</li>
</ul>
<h2 id="konventionen-für-diese-lehrveranstaltung">Konventionen für diese Lehrveranstaltung</h2>
<p>In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege
in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html#id_Russell2020">[Russell2020]</a>). Dies
erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt.
Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die <strong>partiellen Wege</strong>
aufzuschreiben.</p>
<p>Nicht Bestandteil der Algorithmen, dient aber der Nachvollziehbarkeit: Expandierte Knoten
sollen alphabetisch sortiert an der korrekten Stelle in der Datenstruktur auftauchen, dabei
soll aber natürlich die Reihenfolge der ursprünglich in der Datenstruktur enthaltenen Knoten
nicht modifiziert werden. (Bei &quot;echten&quot; Problemen wird die Reihenfolge der expandierten
Nachfolger in der Regel durch eine Reihenfolge der anwendbaren Operationen bestimmt.)</p>
<h2 id="weitere-hinweise">Weitere Hinweise</h2>
<ul>
<li>
<p>Die Tiefensuche wurde zufällig am Beispiel Tree-Search eingeführt. Man kann auch Graph-Search
einsetzen. Wichtig ist nur, dass als Datenstruktur ein <strong>Stack</strong> genutzt wird.</p>
</li>
<li>
<p>Bei Tree-Search werden bereits besuchte Knoten u.U. immer wieder besucht. Zyklen im aktuell
entwickelten Pfad sind also möglich! Außerdem sind mehrere Wege zum selben (Zwischen-/End-)
Knoten in der Datenstruktur möglich!</p>
</li>
<li>
<p>Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html#id_Russell2020">[Russell2020]</a> wird der Begriff &quot;Backtracking&quot; für den rekursiven Tiefensuche-Algorithmus
verwendet. Dies steht im Gegensatz zum üblichen Sprachgebrauch in der KI!</p>
</li>
</ul>
<h2 id="tiefensuche-rekursive-variante">Tiefensuche (rekursive Variante)</h2>
<ol>
<li>Startknoten ist gesuchtes Element: Abbruch, melde &quot;<em>gefunden</em>&quot;</li>
<li>Für jeden Nachfolger des Startknotens:
<ul>
<li>Rufe Tiefensuche für aktuellen (Nachfolger-) Knoten auf</li>
<li>Ergebnis &quot;<em>gefunden</em>&quot;: Abbruch, melde &quot;<em>gefunden</em>&quot;</li>
</ul>
</li>
<li>Abbruch, melde &quot;<em>nicht gefunden</em>&quot;</li>
</ol>
<h3 id="bemerkungen-1">Bemerkungen</h3>
<ul>
<li>Eigenschaften wie &quot;normale&quot; Tiefensuche</li>
<li>Einfacher zu implementieren: Nutzung des Stacks wird auf den Compiler
verlagert (Funktionsaufruf, Stack des Prozesses ...)</li>
<li>Speicherbedarf: Für jeden Knoten wird nur der nächste Knoten expandiert,
plus Speicher für die Funktion</li>
</ul>
<h2 id="eigenschaften-der-tiefensuche">Eigenschaften der Tiefensuche</h2>
<p>Siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html">Breitensuche</a></p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Uninformierte Suchverfahren
<ul>
<li>Keine weiteren Pfadkosten (nur Anzahl der Schritte)</li>
<li>Tiefensuche: Verfolge einen Pfad zuerst in die Tiefe</li>
<li>Backtracking bei Sackgassen (automatisch durch den Stack)</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106596&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Tiefensuche (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Tiefensuche: Abschnitt 3.4.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Suche mit Breitensuche</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Die Breitensuche gehört zu den &quot;Uninformierten Suchverfahren&quot;: Es werden keine weiteren
Pfadkosten, sondern nur die Anzahl der Schritte berücksichtigt.</p>
<p>Die Breitensuche entsteht, wenn man bei der Tree-Search oder der Graph-Search für die
Datenstruktur eine <strong>Queue</strong> benutzt: Expandierte Nachfolger werden immer <strong>hinten</strong>
in die Queue eingefügt, und der nächste zu expandierende Knoten wird <strong>vorn</strong> aus der
Queue genommen. Dadurch wird bei der Breitensuche der Suchbaum ebenenweise entwickelt.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/WLhXt6ZpyD8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Breitensuche</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/2b90123c48bc56189a083a0c9131281307f860481a17fe3dadd3bd4891d01ef157d6c12a39b5255dd39ab36fe8fd36c714ca6873277ce98667094a3f91359a2e' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Breitensuche</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Verwendete Datenstrukturen</li> <li>(K2) Algorithmische Abläufe, Terminierung</li> <li>(K2) Optimalität, Vollständigkeit und Komplexität</li> <li>(K3) Uninformierte Suchverfahren: Breitensuche</li></ul>
  </div>
</div>




    <h2 id="hole-das-buch">Hole das Buch</h2>
<div class='center'>
<p><a href="#R-image-3b48ddbefb8b6ef559e5ecc954b839ea" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs/graph.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-3b48ddbefb8b6ef559e5ecc954b839ea"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs/graph.png?width=60%25&height=auto"></a></p>
</div>
<p>=&gt; <span class='alert'><strong>Problemlösen == Suche im Graphen</strong></span></p>
<p><strong>Uninformierte (&quot;blinde&quot;) Suche</strong>:</p>
<p>Keine Informationen über die Kosten eines Pfades: Nur die <span class='alert'>Pfadlänge</span> (Anzahl der Schritte) zählt.</p>
<p>Varianten:</p>
<ul>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html">Tiefensuche</a></li>
<li><strong>Breitensuche</strong></li>
</ul>
<h2 id="breitensuche-bs-bfs">Breitensuche (<em>BS</em>, <em>BFS</em>)</h2>
<p><strong>Erinnerung Graph-Search</strong></p>
<ol>
<li>Füge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein</li>
<li>Entnehme Knoten aus der Datenstruktur:
<ul>
<li>Knoten ist gesuchtes Element: Abbruch, melde &quot;<em>gefunden</em>&quot;</li>
<li>Markiere aktuellen Knoten, und</li>
<li>Expandiere alle Nachfolger des Knotens und füge alle unmarkierten
Nachfolger, die noch nicht in der Datenstruktur sind, in die
Datenstruktur ein</li>
</ul>
</li>
<li>Falls die Datenstruktur leer ist: Abbruch, melde &quot;<em>nicht gefunden</em>&quot;</li>
<li>Gehe zu Schritt 2</li>
</ol>
<p>=&gt; Was passiert, wenn wir eine <span class='alert'><strong>Queue</strong></span> einsetzen?</p>
<div class='center'>
<p><a href="#R-image-92156206552a3df2058b4baf18bb8ab0" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs/tafelbeispiel.png?width=90%25&height=auto" style=" height: auto; width: 90%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-92156206552a3df2058b4baf18bb8ab0"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs/tafelbeispiel.png?width=90%25&height=auto"></a></p>
</div>
<h2 id="bemerkungen">Bemerkungen</h2>
<ul>
<li>
<p>Nachfolger eines Knotens: Alle von diesem Zustand durch Aktionen erreichbare Zustände</p>
</li>
<li>
<p>Suchalgorithmus mit <span class='alert'><strong>Queue</strong></span> als Datenstruktur =&gt; <strong>Breitensuche</strong></p>
<ul>
<li>Zu betrachtender Knoten in Schritt 2 wird <em>vorn</em> aus der Queue genommen</li>
<li>Expandierte Knoten werden in Schritt 2.a <em>hinten</em> in die Queue eingefügt
Dabei i.A. die vorgegebene Reihenfolge der Nachfolgeknoten beachten!</li>
</ul>
<p>Auswirkung: Suchbaum wird <strong>ebenenweise</strong> aufgebaut (deshalb &quot;Breitensuche&quot;)</p>
</li>
<li>
<p>Graph-Search: Markierte Knoten müssen geeignet gespeichert werden: separate Datenstruktur
=&gt; Aufwand!</p>
</li>
</ul>
<h2 id="konventionen-für-diese-lehrveranstaltung">Konventionen für diese Lehrveranstaltung</h2>
<p>In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege
in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html#id_Russell2020">[Russell2020]</a>). Dies
erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt.
Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die <strong>partiellen Wege</strong>
aufzuschreiben.</p>
<p>Nicht Bestandteil der Algorithmen, dient aber der Nachvollziehbarkeit: Expandierte Knoten
sollen alphabetisch sortiert an der korrekten Stelle in der Datenstruktur auftauchen, dabei
soll aber natürlich die Reihenfolge der ursprünglich in der Datenstruktur enthaltenen Knoten
nicht modifiziert werden. (Bei &quot;echten&quot; Problemen wird die Reihenfolge der expandierten
Nachfolger in der Regel durch eine Reihenfolge der anwendbaren Operationen bestimmt.)</p>
<h2 id="weitere-hinweise">Weitere Hinweise</h2>
<ul>
<li>
<p>Die Breitensuche wurde zufällig am Beispiel Graph-Search eingeführt.
Man kann auch die Tree-Search-Variante einsetzen. Wichtig ist nur, dass als
Datenstruktur eine Queue genutzt wird.</p>
</li>
<li>
<p>Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html#id_Russell2020">[Russell2020]</a> wird die Breitensuche ebenfalls auf der Basis des
Graph-Search-Algorithmus eingeführt. Allerdings wird die Abbruchbedingung
modifiziert: Die Zielbedingung wird nicht erst (wie bei Graph-Search
eigentlich definiert) geprüft, wenn ein Knoten aus der Queue entnommen wird,
sondern bereits bei der Erzeugung der Nachfolgerknoten (vor dem Einfügen in
die Queue). Dadurch spart man sich die Expansion einer zusätzlichen Ebene:
Die Komplexität wäre in diesem Fall &quot;nur&quot; <span class="math align-center">$O(b^{d})$</span>.</p>
</li>
</ul>
<h2 id="vergleich-ts-bs">Eigenschaften Breitensuche vs. Tiefensuche</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left"><strong>Tiefensuche</strong></th>
          <th style="text-align: left"><strong>Breitensuche</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Vollständigkeit</td>
          <td style="text-align: left">nein<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></td>
          <td style="text-align: left">ja<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></td>
      </tr>
      <tr>
          <td style="text-align: left">Optimalität</td>
          <td style="text-align: left">nein</td>
          <td style="text-align: left">ja</td>
      </tr>
      <tr>
          <td style="text-align: left">Zeitkomplexität</td>
          <td style="text-align: left"><span class="math align-center">$O(b^m)$</span></td>
          <td style="text-align: left"><span class="math align-center">$O(b^{d+1})$</span></td>
      </tr>
      <tr>
          <td style="text-align: left">Speicherkomplexität</td>
          <td style="text-align: left"><span class="math align-center">$O(bm)$</span></td>
          <td style="text-align: left"><span class="math align-center">$O(b^{d+1})$</span></td>
      </tr>
  </tbody>
</table>
<ul>
<li>Zeitkomplexität: maximal zu expandierende Knotenzahl</li>
<li>Speicher:
<ul>
<li>TS: in jeder Tiefe weitere <span class="math align-center">$b$</span> Knoten speichern</li>
<li>BS: alle Knoten einer Ebene im Speicher halten<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></li>
</ul>
</li>
</ul>
<p><strong>b</strong>: Verzweigungsfaktor, <strong>d</strong>: Ebene d. höchsten Lösungsknotens, <strong>m</strong>:
Länge d. längsten Pfades</p>
<h2 id="praxisvergleich-breitensuche-vs-tiefensuche">Praxisvergleich Breitensuche vs. Tiefensuche</h2>
<p><strong>Breitensuche</strong>:
Annahme: <span class="math align-center">$b=10$</span>, 10.000 Knoten/s, 1.000 Byte/Knoten</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>Tiefe</strong></th>
          <th style="text-align: left"><strong>exp. Knoten</strong></th>
          <th style="text-align: left"><strong>Zeit</strong></th>
          <th style="text-align: left"><strong>Speicher</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">2</td>
          <td style="text-align: left"><span class="math align-center">$10^3$</span></td>
          <td style="text-align: left">0.1 s</td>
          <td style="text-align: left">1 MB</td>
      </tr>
      <tr>
          <td style="text-align: left">4</td>
          <td style="text-align: left"><span class="math align-center">$10^5$</span></td>
          <td style="text-align: left">10 s</td>
          <td style="text-align: left">100 MB</td>
      </tr>
      <tr>
          <td style="text-align: left">6</td>
          <td style="text-align: left"><span class="math align-center">$10^7$</span></td>
          <td style="text-align: left">20 min</td>
          <td style="text-align: left">10 GB</td>
      </tr>
      <tr>
          <td style="text-align: left">8</td>
          <td style="text-align: left"><span class="math align-center">$10^9$</span></td>
          <td style="text-align: left">30 h</td>
          <td style="text-align: left">1 TB</td>
      </tr>
      <tr>
          <td style="text-align: left">10</td>
          <td style="text-align: left"><span class="math align-center">$10^{11}$</span></td>
          <td style="text-align: left">130 d</td>
          <td style="text-align: left">100 TB</td>
      </tr>
  </tbody>
</table>
<p><strong>Tiefensuche</strong>:
Annahme: längster Pfad (Tiefe) <span class="math align-center">$m=1000$</span></p>
<p>=&gt; Speicherbedarf <span class='alert'><strong>ca. 10 MB</strong></span></p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Uninformierte Suchverfahren
<ul>
<li>Keine weiteren Pfadkosten (nur Anzahl der Schritte)</li>
<li>Breitensuche: Verfolge alle Pfade (baue den Suchbaum ebenenweise auf)</li>
</ul>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>gilt für Tree-Search-Variante; vollständig in Graph-Search-Variante bei endlichem Suchraum&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>falls <em>b</em> endlich&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><span class="math align-center">$O(b^{d})$</span> mit vorgezogener Zielprüfung (vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html#id_Russell2020">[Russell2020]</a>)&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106597&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Breitensuche (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Breitensuche: Abschnitt 3.4.1</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Suche mit Branch-and-Bound</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Branch-and-Bound gehört zu den &quot;Informierten Suchverfahren&quot;: Es werden
(reale) Pfadkosten statt der Anzahl der Schritte berücksichtigt.</p>
<p>Branch-and-Bound entsteht, wenn man bei der Tree-Search oder der Graph-Search für die
Datenstruktur eine sortierte <strong>Queue</strong> (Prioritätsqueue) benutzt: Expandierte Nachfolger
werden immer <strong>hinten</strong> in die Queue eingefügt, diese wird nach den <strong>Kosten der partiellen
Pfade sortiert</strong> und der nächste zu expandierende Knoten (d.h. der bisher günstigste partielle
Weg) wird <strong>vorn</strong> aus der Queue genommen. Branch-and-Bound arbeitet mit den bisher
entstandenen (realen) Kosten der partiellen Wege.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/WBL-sihpbaM' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Branch-and-Bound</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/a9df47470cf25d0f57646f04280a04e6ff323f142151641b95c76b47ba0bcd10cca5d53d287842e551e119a6b9ba21609f7ddc05c38aca416115e14f9104810a' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Branch-and-Bound</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Verwendete Datenstrukturen</li> <li>(K2) Algorithmische Abläufe, Terminierung</li> <li>(K2) Optimalität, Vollständigkeit und Komplexität</li> <li>(K2) Bedeutung nicht-negativer Kosten für Branch-and-Bound</li> <li>(K3) Informierte Suchverfahren: Branch-and-Bound</li></ul>
  </div>
</div>




    <h2 id="hole-das-buch">Hole das Buch</h2>
<div class='center'>
<p><a href="#R-image-84ffcee82b3a4ee6cb2941edd7ffff00" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound/graph.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-84ffcee82b3a4ee6cb2941edd7ffff00"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound/graph.png?width=60%25&height=auto"></a></p>
</div>
<p>=&gt; <span class='alert'><strong>Problemlösen == Suche im Graphen</strong></span></p>
<p><strong>Informierte Suche: Nutzung der Kostenfunktion</strong>:</p>
<p><strong>Gesamtkosten</strong>: <span class="math align-center">$f(n) = g(n) + h(n)$</span></p>
<ul>
<li><span class="math align-center">$n \in S$</span> auf aktuellem Weg erreichter Knoten</li>
<li><span class="math align-center">$g(n)$</span> tatsächliche Kosten für Weg vom Start bis Knoten <span class="math align-center">$n$</span></li>
<li><span class="math align-center">$h(n)$</span> geschätzte Restkosten für Weg von Knoten <span class="math align-center">$n$</span> zum Ziel
=&gt; <span class="math align-center">$h(n)$</span> wird auch &quot;heuristische Funktion&quot; oder &quot;Heuristik&quot; genannt</li>
</ul>
<p>Varianten:</p>
<ul>
<li><strong>Branch-and-Bound</strong></li>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html">Best First</a></li>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A*</a></li>
</ul>
<h2 id="branch-and-bound-bnb">Branch-and-Bound (<em>BnB</em>)</h2>
<p>Variante der Breitensuche mit Kosten</p>
<ul>
<li>
<p>Idee: Expandiere den <em>bisher</em> <span class='alert'>günstigsten</span> partiellen Weg</p>
</li>
<li>
<p>Kostenfunktion: <span class="math align-center">$f(n) = g(n)$</span></p>
</li>
<li>
<p>Datenstruktur: <strong>sortierte Queue</strong> (Prioritätsqueue)</p>
</li>
<li>
<p>Voraussetzung: <span class='alert'>alle Aktionen</span> haben <span class='alert'>positive Kosten</span>
(jeden Knoten <span class="math align-center">$n$</span> gilt: <span class="math align-center">$g(n) > 0$</span>)</p>
</li>
</ul>
<p><em>Hinweis</em>: Die Branch-and-Bound-Suche taucht im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html#id_Russell2020">[Russell2020]</a> als Erweiterung
der &quot;Uniformen Suche&quot; auf ...</p>
<h2 id="bnb-finde-einen-weg-von-a-nach-h">BnB: Finde einen Weg von A nach H</h2>
<p><a href="#R-image-ef49f8a64a3a5cd814f4130b037a0898" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound/tafelbeispiel.png?width=90%25&height=auto" style=" height: auto; width: 90%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-ef49f8a64a3a5cd814f4130b037a0898"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound/tafelbeispiel.png?width=90%25&height=auto"></a></p>
<h2 id="bemerkungen-zu-bnb-mit-graph-search">Bemerkungen zu BnB mit Graph-Search</h2>
<p>Graph-Search fordert: Expandierte Nachfolgerknoten, die schon in der Queue
sind, sollen nicht (erneut) in die Queue aufgenommen werden.</p>
<ul>
<li>
<p>Problem dabei: Was ist mit den Kosten?! Der neu expandierte Weg könnte
günstiger sein als der schon in der Queue enthaltene.</p>
</li>
<li>
<p>Lösung (vgl. Optimierungsmöglichkeiten für A*):</p>
<p>Füge zunächst alle neu expandierten partiellen Pfade (mit unmarkierten
Endknoten) in die Queue ein, sortiere diese und behalte von mehreren
Pfaden zum gleichen Knoten nur den jeweils günstigsten in der Queue</p>
</li>
</ul>
<p>Pfade, deren Endknoten bereits früher im Pfad vorkommt (Schleifen), werden
bei Graph-Search in Schritt 2 nicht in die Queue aufgenommen (der Endknoten
wäre bei einer Schleife ja bereits markiert und der neue Pfad würde bei
Graph-Search nicht weiter beachtet).</p>
<p>Das Einfärben ist kein Problem, da nur der jeweils günstigste Knoten aus der
Queue genommen, gefärbt und expandiert wird. D.h. alle anderen Wege sind zu
diesem Zeitpunkt bereits teurer. Wenn man nun (später) über einen anderen Weg
zu einem bereits gefärbten Knoten kommt, kann der neue Weg nicht günstiger sein
(positive Kosten vorausgesetzt).</p>
<h2 id="konventionen-für-diese-lehrveranstaltung">Konventionen für diese Lehrveranstaltung</h2>
<p>In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege
in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html#id_Russell2020">[Russell2020]</a>). Dies
erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt.
Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die <strong>partiellen Wege</strong>
aufzuschreiben.</p>
<p>Auf dem Papier sortiert sich die Queue schlecht, deshalb können Sie darauf verzichten,
wenn Sie den im nächsten Schritt zu expandierenden Weg unterstreichen. Wer nicht mit
Unterstreichen arbeiten will, muss eben dann manuell sortieren ...</p>
<p>Wenn bei der Graph-Search-Variante ein Weg nicht in die Queue aufgenommen wird, weil
bereits ein anderer (günstigerer) Weg zum selben (Zwischen-/End-) Knoten bereits in der
Queue enthalten ist, schreiben Sie dies geeignet auf. Dies gilt auch für den analogen
Fall, wenn ein Weg aus der Queue entfernt wird, weil ein günstigerer Weg zum selben
(Zwischen-/End-) Knoten eingefügt werden soll.</p>
<h2 id="eigenschaften-von-bnb">Eigenschaften von BnB</h2>
<p>Siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A*</a></p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Informierte Suchverfahren
<ul>
<li>Nutzen reale Pfadkosten und/oder Schätzungen der Restkosten</li>
<li>Branch-and-Bound: nur reale Pfadkosten <span class="math align-center">$g(n)$</span></li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106598&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Branch-and-Bound (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Branch-and-Bound (Uniforme Suche): Abschnitt 3.4.2</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Suche mit Best First</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Best First gehört wie Branch-and-Bound zu den &quot;Informierten Suchverfahren&quot;: Es werden
Pfadkosten (in diesem Fall Schätzungen) statt der Anzahl der Schritte berücksichtigt.</p>
<p>Best First arbeitet algorithmisch wie Branch-and-Bound, allerdings werden immer nur die
geschätzten Restkosten eines Knotens zum Ziel berücksichtigt.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/dNyLOQuD_aI' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Best First</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/4c065b44bafcd006400d7ae3ccdc25e04577d164e1aa0e118e29fa643b53bfbcaaa14dd12b93fa89105abc6f782947f7c04c849fb673a92c710da5c6b8c99083' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Best First</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K1) Verwendete Datenstrukturen</li> <li>(K2) Algorithmische Abläufe, Terminierung</li> <li>(K2) Optimalität, Vollständigkeit und Komplexität</li> <li>(K3) Informierte Suchverfahren Best-First</li></ul>
  </div>
</div>




    <h2 id="hole-das-buch">Hole das Buch</h2>
<div class='center'>
<p><a href="#R-image-7895c8da865bbf127eeb2a2db26a7852" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst/graph.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7895c8da865bbf127eeb2a2db26a7852"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst/graph.png?width=60%25&height=auto"></a></p>
</div>
<p>=&gt; <span class='alert'><strong>Problemlösen == Suche im Graphen</strong></span></p>
<p><strong>Informierte Suche: Nutzung der Kostenfunktion</strong>:</p>
<p><strong>Gesamtkosten</strong>: <span class="math align-center">$f(n) = g(n) + h(n)$</span></p>
<ul>
<li><span class="math align-center">$n \in S$</span> auf aktuellem Weg erreichter Knoten</li>
<li><span class="math align-center">$g(n)$</span> tatsächliche Kosten für Weg vom Start bis Knoten <span class="math align-center">$n$</span></li>
<li><span class="math align-center">$h(n)$</span> geschätzte Restkosten für Weg von Knoten <span class="math align-center">$n$</span> zum Ziel
=&gt; <span class="math align-center">$h(n)$</span> wird auch &quot;heuristische Funktion&quot; oder &quot;Heuristik&quot; genannt</li>
</ul>
<p>Varianten:</p>
<ul>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html">Branch-and-Bound</a></li>
<li><strong>Best First</strong></li>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A*</a></li>
</ul>
<h2 id="best-first-bf-bfs">Best-First (<em>BF</em>, <em>BFS</em>)</h2>
<ul>
<li>
<p>Idee: Expandiere den partiellen Weg, der <span class='alert'>verspricht</span>, dem Ziel am
nächsten zu sein (<strong>Heuristik</strong>)</p>
</li>
<li>
<p>Kostenfunktion: <span class="math align-center">$f(n) = h(n)$</span></p>
</li>
<li>
<p>Datenstruktur: <strong>sortierte Queue</strong> (Prioritätsqueue)</p>
</li>
<li>
<p>Voraussetzungen: <span class="math align-center">$h(n)$</span> positiv, <span class="math align-center">$h(n) = 0$</span> für den Zielknoten</p>
</li>
</ul>
<h2 id="konventionen-bf">Konventionen BF</h2>
<p>In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege
in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html#id_Russell2020">[Russell2020]</a>). Dies
erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt.
Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die <strong>partiellen Wege</strong>
aufzuschreiben.</p>
<p>Auf dem Papier sortiert sich die Queue schlecht, deshalb können Sie darauf verzichten,
wenn Sie den im nächsten Schritt zu expandierenden Weg unterstreichen. Wer nicht mit
Unterstreichen arbeiten will, muss eben dann manuell sortieren ...</p>
<p>Wenn bei der Graph-Search-Variante ein Weg nicht in die Queue aufgenommen wird, weil
bereits ein anderer (günstigerer) Weg zum selben (Zwischen-/End-) Knoten bereits in der
Queue enthalten ist, schreiben Sie dies geeignet auf. Dies gilt auch für den analogen
Fall, wenn ein Weg aus der Queue entfernt wird, weil ein günstigerer Weg zum selben
(Zwischen-/End-) Knoten eingefügt werden soll.</p>
<h2 id="eigenschaften-von-bf">Eigenschaften von BF</h2>
<p>Siehe <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html">A*</a></p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Informierte Suchverfahren
<ul>
<li>Nutzen reale Pfadkosten und/oder Schätzungen der Restkosten</li>
<li>Best-First: nur Schätzungen <span class="math align-center">$h(n)$</span></li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106599&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Best First (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p>Betrachten Sie folgende Landkarte und Restwegschätzungen:</p>
<p><a href="#R-image-22d15bf8fd7af4f42a34587f2ff0260c" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/MapGermanyGraph.svg/476px-MapGermanyGraph.svg.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-22d15bf8fd7af4f42a34587f2ff0260c"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/MapGermanyGraph.svg/476px-MapGermanyGraph.svg.png?width=auto&height=auto"></a>
Quelle: <a href="https://commons.wikimedia.org/wiki/File:MapGermanyGraph.svg" rel="external" target="_blank">MapGermanyGraph.svg</a> by <a href="https://de.wikipedia.org/wiki/Benutzer:Regnaron" rel="external" target="_blank">Regnaron</a> and <a href="https://commons.wikimedia.org/wiki/User:Jahobr" rel="external" target="_blank">Jahobr</a> on Wikimedia Commons (<a href="https://en.wikipedia.org/wiki/en:public_domain" rel="external" target="_blank">Public Domain</a>)</p>
<p><a href="#R-image-8b9cc26a9eb09d8fbc7dc6cf032d664a" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/searching/images/challenge.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8b9cc26a9eb09d8fbc7dc6cf032d664a"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/searching/images/challenge.png?width=auto&height=auto"></a></p>
<p>Finden Sie mit der <strong>Best-First-Suche</strong> jeweils einen Weg von Würzburg nach München. Vergleichen Sie das Ergebnis mit der Gradienten-Suche.</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Best First: Abschnitt 3.5.1, Heuristiken: Kapitel 3.6</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Suche mit A*</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>A* zählt zu den Verfahren der informierten Suche. Dabei verwendet A* sowohl die
realen Pfadkosten als auch die Schätzungen der Restkosten, d.h. die Kostenfunktion für
A* ist <span class="math align-center">$f(n) = g(n)+h(n)$</span>.</p>
<p>A* ist vollständig und optimal, allerdings muss die Heuristik bei der Tree-Search-Variante
<strong>zulässig</strong> sein (d.h. sie muss <em>unterschätzen</em>, beispielsweise die Luft-Linie) und bei der
Graph-Search-Variante muss sie <strong>konsistent</strong> sein (d.h. für jeden Knoten die
Dreiecks-Ungleichung erfüllen).</p>
<p>A* hat wie BnB exponentiellen Aufwand. Durch die zusätzliche Verwendung der Heuristik
werden die partiellen Pfade in der Queue aber geschickter sortiert, so dass A* in der
Regel mit weniger Suchschritten als BnB auskommt.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/eMkEN-HtEs8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL A*</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/b4967369536da2af1f8b0efc443d7c2275ff292f786144c83524e1eb117dd5fc305993716fe4dc472056114d43360483755c45dd8e5938f635efe4fa0ea02509' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL A*</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Verwendete Datenstrukturen</li> <li>(K2) Algorithmische Abläufe, Terminierung</li> <li>(K2) Optimalität, Vollständigkeit und Komplexität</li> <li>(K2) Bedingung(en) an Restkostenabschätzung bei A*</li> <li>(K3) Informierte Suchverfahren A*</li></ul>
  </div>
</div>




    <h2 id="hole-das-buch">Hole das Buch</h2>
<div class='center'>
<p><a href="#R-image-f6d497593ebd1e8aa0e7d6ce1593b28a" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar/graph.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f6d497593ebd1e8aa0e7d6ce1593b28a"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar/graph.png?width=60%25&height=auto"></a></p>
</div>
<p>=&gt; <span class='alert'><strong>Problemlösen == Suche im Graphen</strong></span></p>
<p><strong>Informierte Suche: Nutzung der Kostenfunktion</strong>:</p>
<p><strong>Gesamtkosten</strong>: <span class="math align-center">$f(n) = g(n) + h(n)$</span></p>
<ul>
<li><span class="math align-center">$n \in S$</span> auf aktuellem Weg erreichter Knoten</li>
<li><span class="math align-center">$g(n)$</span> tatsächliche Kosten für Weg vom Start bis Knoten <span class="math align-center">$n$</span></li>
<li><span class="math align-center">$h(n)$</span> geschätzte Restkosten für Weg von Knoten <span class="math align-center">$n$</span> zum Ziel
=&gt; <span class="math align-center">$h(n)$</span> wird auch &quot;heuristische Funktion&quot; oder &quot;Heuristik&quot; genannt</li>
</ul>
<p>Varianten:</p>
<ul>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html">Branch-and-Bound</a></li>
<li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html">Best First</a></li>
<li><strong>A*</strong></li>
</ul>
<h2 id="a-suche">A*-Suche</h2>
<ul>
<li>
<p>Kombination aus Branch-and-Bound und Best-First-Suche</p>
</li>
<li>
<p>Kostenfunktion: <span class="math align-center">$f(n) = g(n) + h(n)$</span></p>
</li>
<li>
<p>Datenstruktur: <strong>sortierte Queue</strong> (Prioritätsqueue)</p>
</li>
<li>
<p>Voraussetzung:</p>
<ol>
<li>Alle Aktionen haben positive Kosten (<span class="math align-center">$g(n) \ge \epsilon$</span>)</li>
<li>Heuristik <span class="math align-center">$h(n)$</span> muss <em>zulässig/konsistent</em> sein</li>
</ol>
</li>
</ul>
<div class='center'>
<p><a href="#R-image-2527575505ece35fb4688423fdd23bb3" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar/tafelbeispiel.png?width=90%25&height=auto" style=" height: auto; width: 90%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-2527575505ece35fb4688423fdd23bb3"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar/tafelbeispiel.png?width=90%25&height=auto"></a></p>
</div>
<h2 id="konventionen-für-diese-lehrveranstaltung">Konventionen für diese Lehrveranstaltung</h2>
<p>In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege
in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html#id_Russell2020">[Russell2020]</a>). Dies
erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt.
Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die <strong>partiellen Wege</strong>
aufzuschreiben.</p>
<p>Notieren Sie die Bestandteile der Kosten für jeden partiellen Weg in der Queue einzeln:
&quot;<span class="math align-center">$g(n) + h(n) = f(n)$</span>&quot;. Das erleichtert Ihnen die weiteren Schritte, da Sie dort ja nur
mit <span class="math align-center">$g(n)$</span> weiter rechnen dürfen. Gleichzeitig erleichtert es die Nachvollziehbarkeit.</p>
<p>Auf dem Papier sortiert sich die Queue schlecht, deshalb können Sie darauf verzichten,
wenn Sie den im nächsten Schritt zu expandierenden Weg unterstreichen. Wer nicht mit
Unterstreichen arbeiten will, muss eben dann manuell sortieren ...</p>
<p>Wenn bei der Graph-Search-Variante ein Weg nicht in die Queue aufgenommen wird, weil
bereits ein anderer (günstigerer) Weg zum selben (Zwischen-/End-) Knoten bereits in der
Queue enthalten ist, schreiben Sie dies geeignet auf. Dies gilt auch für den analogen
Fall, wenn ein Weg aus der Queue entfernt wird, weil ein günstigerer Weg zum selben
(Zwischen-/End-) Knoten eingefügt werden soll.</p>
<h2 id="a-suche----anforderungen-an-heuristik-tree-search">A*-Suche -- Anforderungen an Heuristik (Tree-Search)</h2>
<p><strong>Tree-Search-Variante</strong>: Die Heuristik muss <span class='alert'><strong>zulässig</strong></span> sein:</p>
<ul>
<li>Seien <span class="math align-center">$h^\star(n)$</span> die tatsächlichen optimalen Restkosten von einem Knoten
<span class="math align-center">$n$</span> zum nächsten Ziel.</li>
<li>Dann muss für jeden beliebigen Knoten <span class="math align-center">$n$</span> gelten: <span class="math align-center">$$h(n) \le h^\star(n)$$</span></li>
<li>Außerdem muss gelten:
<ul>
<li><span class="math align-center">$h(n) \ge 0$</span> für jeden Knoten <span class="math align-center">$n$</span></li>
<li><span class="math align-center">$h(n) = 0$</span> für jeden Zielknoten <span class="math align-center">$n$</span></li>
</ul>
</li>
</ul>
<p>=&gt; Beispiel: Luftlinie als Abschätzung</p>
<p><strong>Hinweis</strong>: Im der englischen Ausgabe des <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html#id_Russell2020">[Russell2020]</a> wird die
<strong>zulässige Heuristik</strong> auch &quot;<strong>admissible heuristic</strong>&quot; genannt.</p>
<h2 id="a-ist-optimal">A* ist optimal</h2>
<p>A* (Tree-Search-Variante) mit zulässiger Heuristik ist optimal.</p>
<p>Beweis siehe Übung :-)</p>
<h2 id="einfache-verbesserungen-a-tree-search">Einfache Verbesserungen A* (Tree-Search)</h2>
<ul>
<li>
<p>Dynamische Programmierung: Behalte von mehreren Pfaden zum gleichen Knoten
nur den günstigsten in der Queue</p>
</li>
<li>
<p>Pfade, deren Endknoten bereits früher im Pfad vorkommt (Schleifen), werden
in Schritt 2 nicht in die Queue aufgenommen</p>
</li>
<li>
<p>Übergang zur Graph-Search-Variante und Markierung von Knoten</p>
<p>=&gt; Achtung: Dann schärfere Anforderungen an Heuristik (Konsistenz)</p>
</li>
</ul>
<h2 id="a-suche----anforderungen-an-heuristik-graph-search">A*-Suche -- Anforderungen an Heuristik (Graph-Search)</h2>
<p><strong>Graph-Search-Variante</strong>: Die Heuristik muss <span class='alert'><strong>konsistent</strong></span> sein:</p>
<p>Für jeden Knoten <span class="math align-center">$n$</span> und jeden durch eine Aktion <span class="math align-center">$a$</span> erreichten Nachfolger <span class="math align-center">$m$</span>
gilt: <span class="math align-center">$$h(n) \le c(n,a,m) + h(m)$$</span> mit <span class="math align-center">$c(n,a,m)$</span> Schrittkosten für den Weg von
<span class="math align-center">$n$</span> nach <span class="math align-center">$m$</span> mit Aktion <span class="math align-center">$a$</span>.</p>
<p>Außerdem muss gelten:</p>
<ul>
<li><span class="math align-center">$h(n) \ge 0$</span> für jeden Knoten <span class="math align-center">$n$</span></li>
<li><span class="math align-center">$h(n) = 0$</span> für jeden Zielknoten <span class="math align-center">$n$</span></li>
</ul>
<p>=&gt; Eine konsistente Heuristik ist gleichzeitig zulässig.</p>
<p><strong>Hinweis</strong>: Im der englischen Ausgabe des <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html#id_Russell2020">[Russell2020]</a> wird die
<strong>konsistente Heuristik</strong> auch &quot;<strong>consistent heuristic</strong>&quot; genannt.</p>
<h2 id="vergleich-bnb-bf-astar">Eigenschaften Branch-and-Bound, Best-First, A*</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left"><strong>Branch-and-Bound</strong></th>
          <th style="text-align: left"><strong>Best-First</strong></th>
          <th style="text-align: left"><strong>A*</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Kosten</td>
          <td style="text-align: left"><span class="math align-center">$f(n) = g(n)$</span></td>
          <td style="text-align: left"><span class="math align-center">$f(n) = h(n)$</span></td>
          <td style="text-align: left"><span class="math align-center">$f(n) = g(n) + h(n)$</span></td>
      </tr>
      <tr>
          <td style="text-align: left">Vollständigkeit</td>
          <td style="text-align: left">ja<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></td>
          <td style="text-align: left">nein<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></td>
          <td style="text-align: left">ja</td>
      </tr>
      <tr>
          <td style="text-align: left">Optimalität</td>
          <td style="text-align: left">ja</td>
          <td style="text-align: left">nein</td>
          <td style="text-align: left">ja</td>
      </tr>
      <tr>
          <td style="text-align: left">Aufwand</td>
          <td style="text-align: left">exponentiell</td>
          <td style="text-align: left">exponentiell</td>
          <td style="text-align: left">exponentiell</td>
      </tr>
      <tr>
          <td style="text-align: left">Bemerkung</td>
          <td style="text-align: left">Probiert erst alle &quot;kleinen&quot; Pfade</td>
          <td style="text-align: left">Suchverlauf stark abh. v. Heuristik</td>
          <td style="text-align: left">Heuristik: zulässig bzw. konsistent</td>
      </tr>
  </tbody>
</table>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>
<p>Informierte Suchverfahren</p>
<ul>
<li>Nutzen reale Pfadkosten und/oder Schätzungen der Restkosten</li>
<li>A*: komplette Kostenfunktion <span class="math align-center">$f(n) = g(n)+h(n)$</span>
=&gt; besondere Anforderungen an die Heuristik!
(Tree-Search: <em>zulässige</em> Heuristik; Graph-Search: <em>konsistente</em> Heuristik)</li>
</ul>
</li>
<li>
<p>Ausblick auf Verbesserungen der vorgestellten Suchverfahren:</p>
<ul>
<li>Beschränkung der Suchtiefe (&quot;Depth-Limited-Search&quot;)</li>
<li>Iterative Vertiefung der Suchtiefe (&quot;Iterative-Deepening-Search&quot;),
beispielsweise IDA* (&quot;Iterative-Deepening A*&quot;)</li>
<li>Beschränkung des verwendeten Speichers, beispielsweise SMA*
(&quot;Simplified Memory-Bounded A*&quot;)</li>
</ul>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>BnB vollständig: Kosten größer Epsilon (positiv)&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>gilt für Tree-Search-Variante; vollständig bei Graph-Search und endlichen Problemräumen&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106600&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest A* (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Informierte und uninformierte Suche</strong></p>
<p>Betrachten Sie folgendes Problem:</p>
<p><a href="#R-image-0b0e88299ad3a289fa1bf1a6a58c5bb7" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/searching/images/challenges_robby.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0b0e88299ad3a289fa1bf1a6a58c5bb7"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/searching/images/challenges_robby.png?width=auto&height=auto"></a></p>
<p>Dargestellt ist eine typische Büroumgebung mit verschiedenen Räumen und einem Flur. Die Pfeile in den Durchgängen geben an, in welche Richtung der jeweilige Durchgang durchschritten werden darf. Die Werte an den Pfeilen geben die Kosten für den Übergang von einem Raum in den anderen an.</p>
<p>Ein Roboter <code>Robbi</code>, der sich zunächst im Kopierer-Raum aufhält, soll den Weg zur Bibliothek finden und dort das Buch aufnehmen. Der Roboter kann sich immer nur entlang den Pfeilen in einen Nachbarraum bewegen (Aktion <code>move</code>). Die Kosten für das Aufnehmen des Buches betragen 3 Einheiten (Aktion <code>take</code>). Weitere Aktionen gibt es nicht.</p>
<ol>
<li>Zeichnen Sie den Problemgraphen. Markieren Sie Start- und Zielknoten.</li>
<li>Finden Sie den Weg mit Tiefensuche und mit Breitensuche (Tree-Search). Welche Unterschiede stellen Sie fest?</li>
<li>Welche Wege würden mit der jeweiligen Graph-Search-Variante nicht weiter untersucht?</li>
<li>Suchen Sie nun einen Weg mit A* (Tree-Search). Definieren Sie zunächst Restkostenschätzungen. Was müssen Sie dabei beachten?</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Problemlösen, Suche</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>A*: Abschnitt 3.5.2, Heuristiken: Kapitel 3.6</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Lokale Suche: Gradientensuche</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Lokale Suchverfahren: Nur das Ergebnis zählt! Nicht der Weg ist das Ziel, sondern nur das
Erreichen des Ziels.</p>
<p>In Analogie zum Bergsteigen: Gehe in Richtung des stärksten Anstiegs kann man die Suche so
formulieren, dass man in jedem Suchschritt den Nachfolgeknoten nach dem stärksten Anstieg
der Kostenfunktion auswählen. Dieses Verfahren nennt sich auch <strong>Hill-Climbing</strong> (bzw.
Gradientensuche).</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/TcTF9xdW3WU' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Gradientensuche</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/84cc094fbc7c016ef48c76ac38c0e13cb2ca06dea4843658b362ba917c595f4ba90d4be19bd6374cae8ea7ca0d241a522e93dd099fd735c03668e106e85557ed' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Gradientensuche</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Problematik der lokalen Minima bei Gradientenverfahren</li> <li>(K3) Lokale Suche (Gradientenabstieg)</li></ul>
  </div>
</div>




    <h2 id="unterschiede-in-den-suchproblemen">Unterschiede in den Suchproblemen?</h2>
<div class='columns'>
<div class='column'>
<p><a href="#R-image-bb5230ec5ffbae082c6a0b45a4b5f320" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/graph.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-bb5230ec5ffbae082c6a0b45a4b5f320"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/graph.png?width=80%25&height=auto"></a></p>
</div>
<div class='column'>
<p><a href="#R-image-a692e9e9a369c73eaaa5e671836fa9a5" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/screenshot_stundenplan.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a692e9e9a369c73eaaa5e671836fa9a5"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/screenshot_stundenplan.png?width=80%25&height=auto"></a></p>
</div>
</div>
<p>Bisher betrachtete Suchverfahren:</p>
<ul>
<li>Systematische Erkundung des Suchraums</li>
<li><span class='alert'><strong>Weg</strong></span> zur Lösung wichtig</li>
</ul>
<p>=&gt; Oft aber nur das <span class='alert'><strong>Ziel</strong></span> <strong>an sich</strong> interessant!
(Und nicht, wie man dort hin gelangt.)</p>
<p>Beispiel: Stundenplan</p>
<h2 id="analogie-bergsteigen-ohne-karte-und-pfade">Analogie: Bergsteigen ohne Karte und Pfade</h2>
<p><a href="#R-image-7b396473c06ce3e92f1d459f032bf93e" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/hill-climbing.png?width=50%25&height=auto" style=" height: auto; width: 50%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7b396473c06ce3e92f1d459f032bf93e"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/hill-climbing.png?width=50%25&height=auto"></a></p>
<p><strong>Gradienten-Suche</strong>:
&quot;Gehe in Richtung des <span class='alert'>steilsten Anstiegs</span> der <span class='alert'>Zielfunktion</span>.&quot;</p>
<p>=&gt; Schrittweise Verbesserung des aktuellen Zustands (Lokale Suche)</p>
<ul>
<li>Verschiedene Namen: &quot;Hill-climbing&quot;, &quot;Greedy local search&quot;</li>
<li>Kann auch als Minimierung angewendet werden</li>
</ul>
<h2 id="pseudoalgorithmus-gradientensuche">Pseudoalgorithmus Gradientensuche</h2>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>&quot;<em>Wie Bergsteigen am Mount Everest in dickem Nebel mit Gedächtnisverlust</em>&quot;</p>
</span></span>
</div>
<ol>
<li>Setze <code>currNode</code> auf den Startknoten</li>
<li><code>currNode</code> ist gesuchtes Element: Abbruch, melde &quot;<em>gefunden</em>&quot;
<ul>
<li>Expandiere alle Nachfolger von <code>currNode</code></li>
<li>Setze <code>nextNode</code> auf Nachfolger mit höchster Bewertung</li>
<li>Falls Bewertung von <code>nextNode</code> <span class="math align-center">$\leq$</span> Bewertung von <code>currNode</code>:
Abbruch, melde &quot;<em>nicht gefunden</em>&quot;</li>
<li>Setze <code>currNode</code> auf <code>nextNode</code></li>
</ul>
</li>
<li>Gehe zu Schritt 2</li>
</ol>
<div class='center'>
<p><a href="#R-image-6b4c6447cfd73c03ad3ca21cb1ec7df2" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/tafelbeispiel.png?width=90%25&height=auto" style=" height: auto; width: 90%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-6b4c6447cfd73c03ad3ca21cb1ec7df2"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/tafelbeispiel.png?width=90%25&height=auto"></a></p>
</div>
<h2 id="beispiel-gradientensuche-hahahugoshortcode57s2hbhb-damen">Beispiel Gradientensuche: <span class="math align-center">$n$</span>-Damen</h2>
<ul>
<li><strong>Ziel</strong>: Setze <span class="math align-center">$n$</span> Damen auf ein <span class="math align-center">$n \times n$</span>-Spielfeld ohne Konflikte</li>
<li><strong>Start</strong>: Setze <span class="math align-center">$n$</span> Damen auf ein <span class="math align-center">$n \times n$</span>-Spielfeld (mit Konflikten)</li>
<li><strong>Suche</strong>: Bewege jeweils eine Dame so, daß die Anzahl der Konflikte reduziert wird</li>
</ul>
<p>Schauen Sie sich auch Abb. 4.3 auf Seite 130 im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html#id_Russell2020">[Russell2020]</a> an!</p>
<p><strong>Hinweis</strong>: Alle Damen stehen von Anfang an auf dem Brett und werden nur verschoben
=&gt; &quot;<span class='alert'>vollständige Zustandsformulierung</span>&quot;</p>
<h3 id="eigenschaften-8-damen-problem-hahahugoshortcode57s7hbhb">Eigenschaften 8-Damen-Problem (<span class="math align-center">$n=8$</span>)</h3>
<ul>
<li>Zustandsraum: <span class="math align-center">$8^8 \approx 17$</span> Millionen Zustände!</li>
<li>Beginnend mit zufällig erzeugtem Startzustand:
<ul>
<li>bleibt in 86% der Fälle stecken, d.h.</li>
<li>findet Lösung nur in 14% der Fälle.</li>
</ul>
</li>
<li>Beobachtung: Lösung nach durchschnittlich 4 Schritten, oder Verfahren bleibt
nach durchschnittlich 3 Schritten stecken.</li>
</ul>
<p><span class='origin'>Quelle: nach <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html#id_Russell2020">[Russell2020, p. 131]</a></span></p>
<h2 id="eigenschaften-gradientensuche">Eigenschaften Gradientensuche</h2>
<ul>
<li>Vollständigkeit: nein</li>
<li>Optimalität: nein</li>
<li>Komplexität: linear in der Anzahl der zu expandierenden Knoten</li>
</ul>
<p><span class='alert'><strong>Zielfunktion (Bewertung) nötig!</strong></span></p>
<p><a href="#R-image-416f1ac27250ddadd2e7fa6120f6bf82" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/hill-climbing.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-416f1ac27250ddadd2e7fa6120f6bf82"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient/hill-climbing.png?width=60%25&height=auto"></a></p>
<p><span class='alert'><strong>Problem</strong></span>: lokale Maxima und Plateaus</p>
<ul>
<li>Lokale Maxima/Minima: Algorithmus findet nur eine suboptimale Lösung</li>
<li>Plateaus: Hier muss der Algorithmus mit zufälligen Zügen explorieren</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<p>Lokale Suchverfahren: Nur das Ergebnis zählt!</p>
<ul>
<li>Gradientenverfahren: Gehe in Richtung des stärksten Anstiegs der
Kostenfunktion</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106601&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Gradientensuche (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p>Betrachten Sie folgende Landkarte und Restwegschätzungen:</p>
<p><a href="#R-image-23503ea21e38538cd24eb641e94364d9" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/MapGermanyGraph.svg/476px-MapGermanyGraph.svg.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-23503ea21e38538cd24eb641e94364d9"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/MapGermanyGraph.svg/476px-MapGermanyGraph.svg.png?width=auto&height=auto"></a>
Quelle: <a href="https://commons.wikimedia.org/wiki/File:MapGermanyGraph.svg" rel="external" target="_blank">MapGermanyGraph.svg</a> by <a href="https://de.wikipedia.org/wiki/Benutzer:Regnaron" rel="external" target="_blank">Regnaron</a> and <a href="https://commons.wikimedia.org/wiki/User:Jahobr" rel="external" target="_blank">Jahobr</a> on Wikimedia Commons (<a href="https://en.wikipedia.org/wiki/en:public_domain" rel="external" target="_blank">Public Domain</a>)</p>
<p><a href="#R-image-fbc776b380de50e842329159a6d12219" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/searching/images/challenge.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fbc776b380de50e842329159a6d12219"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/searching/images/challenge.png?width=auto&height=auto"></a></p>
<p>Finden Sie mit der <strong>Gradienten-Suche</strong> jeweils einen Weg von Würzburg nach München. Vergleichen Sie das Ergebnis mit der Best-First-Suche.</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Lokale Suche, GA</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Gradientenabstieg: Abschnitt 4.1.1</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Lokale Suche: Simulated Annealing</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Lokale Suchverfahren: Nur das Ergebnis zählt! Nicht der Weg ist das Ziel, sondern nur
das Erreichen des Ziels.</p>
<p>Das Problem bei der Gradientensuche ist, dass man eine Kostenfunktion benötigt und
diese auch <strong>lokale Minima</strong> enthalten kann. Mit der reinen Gradientensuche würde
man bei Erreichen lokaler Minima die Suche abbrechen (müssen), da es keine weitere
Verbesserung unter den Nachfolgern mehr geben kann. In Anlehnung an das Abkühlen von
Metall kann hier eine Variante der lokalen Suche helfen: <strong>Simulated Annealing</strong>.
Man führt einen &quot;Temperatur&quot;-Parameter ein, der im Laufe der Suche immer kleiner
wird und schließlich gegen Null geht. In Abhängigkeit von dieser &quot;Temperatur&quot; wird
mit einer bestimmten Wahrscheinlichkeit eine Verschlechterung akzeptiert: Bei einer
hohen Temperatur ist diese Wahrscheinlichkeit höher, bei einer niedrigen Temperatur
niedriger, so dass das Verfahren in ein normales Hill-Climbing übergeht. Damit kann
man ein Festfressen in lokalen Minima vermeiden bzw. überwinden.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/vPg7PWuY2bM' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Simulated Annealing</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/5abd7e34f28bedb07ac3c4d9665a493bbf5b5cedfb97e91f9f2d8be0a3bace4e628a0b0ac2530e146ccbb0fd2de2fc99b48604d15c8b864b3f58825e8ca99260' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Simulated Annealing</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Funktionsweise Simulated Annealing</li></ul>
  </div>
</div>




    <h2 id="motivation">Motivation</h2>
<p><a href="#R-image-c4fdbb1bf051937e59378c0dc92cac02" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing/hill-climbing.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-c4fdbb1bf051937e59378c0dc92cac02"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing/hill-climbing.png?width=60%25&height=auto"></a></p>
<p><span class='alert'><strong>Problem</strong></span>: lokale Maxima und Plateaus</p>
<ul>
<li>Lokale Maxima/Minima: Algorithmus findet nur eine suboptimale Lösung</li>
<li>Plateaus: Hier muss der Algorithmus mit zufälligen Zügen explorieren</li>
</ul>
<p>Mögliche Lösungen:</p>
<ul>
<li>Neustart des Algorithmus, wenn kein Fortschritt erzielt wird</li>
<li>Rauschen &quot;injizieren&quot;</li>
</ul>
<h2 id="gedankenexperiment-ausweg-aus-lokalen-minima">Gedankenexperiment: Ausweg aus lokalen Minima</h2>
<ul>
<li>&quot;Drehen der Landschaft&quot;: Minimieren statt Maximieren</li>
<li>Ball wird in Zustandsraum-Landschaft gesetzt.</li>
<li>Folge:
<ul>
<li>rollt steilsten Abstieg hinunter</li>
<li>rollt evtl. in Tal auf halber Höhe (lokales Minimum)
=&gt; bleibt dort gefangen</li>
</ul>
</li>
</ul>
<p>=&gt; &quot;<span class='alert'><strong>Schütteln</strong></span> der Landschaft&quot; -- Ball springt aus dem Tal und
rollt in anderes Tal</p>
<p>Nicht zu stark schütteln -- sonst wird u.U. globales Minimum verlassen!</p>
<h2 id="analogie-härten-von-metall">Analogie Härten von Metall</h2>
<ul>
<li>Metall erhitzen bis Atome frei beweglich</li>
<li>Langsam abkühlen</li>
</ul>
<p>=&gt; stabiles Atomgitter mit minimalem Energiezustand</p>
<h2 id="übertragen-der-idee">Übertragen der Idee</h2>
<ul>
<li>Starkes &quot;Schütteln&quot; (hohe &quot;Temperatur&quot;) am Anfang</li>
<li>Schrittweises &quot;Abkühlen&quot; =&gt; &quot;Schütteln&quot; im Laufe der Zeit
verringern</li>
</ul>
<p>=&gt; <span class='alert'><strong>Simulated Annealing</strong></span></p>
<h2 id="pseudocode-simulated-annealing-minimierungsproblem">Pseudocode Simulated Annealing (Minimierungsproblem)</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">simulated_annealing</span>(problem):
</span></span><span style="display:flex;"><span>    current <span style="color:#f92672">=</span> problem<span style="color:#f92672">.</span>startNode
</span></span><span style="display:flex;"><span>    t <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;  temp <span style="color:#f92672">=</span> schedule(t)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> temp<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        temp <span style="color:#f92672">=</span> schedule(<span style="color:#f92672">++</span>t)
</span></span><span style="display:flex;"><span>        neighbors <span style="color:#f92672">=</span> current<span style="color:#f92672">.</span>expandSuccessors()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> neighbors: <span style="color:#66d9ef">return</span> current
</span></span><span style="display:flex;"><span>        working <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(neighbors)
</span></span><span style="display:flex;"><span>        dE <span style="color:#f92672">=</span> problem<span style="color:#f92672">.</span>value(current) <span style="color:#f92672">-</span> problem<span style="color:#f92672">.</span>value(working)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> dE <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">or</span> probability(math<span style="color:#f92672">.</span>exp(dE<span style="color:#f92672">/</span>temp)):
</span></span><span style="display:flex;"><span>            current <span style="color:#f92672">=</span> working
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> current</span></span></code></pre></div>
<p>Wenn <code>dE</code> positiv ist, dann ist der Nachfolger &quot;besser&quot; (hier: kleiner bewertet) als der aktuelle Knoten und wird immer
als nächster Knoten übernommen.</p>
<p>Wenn <code>dE</code> negativ ist, dann ist der betrachtete Nachfolger &quot;schlechter&quot; (hier: größer bewertet) als der aktuelle Knoten.
Dann wird er mit einer Wahrscheinlichkeit <code>math.exp(dE/temp)</code> als nächster Knoten übernommen. Diese
Wahrscheinlichkeit ist bei hohen Temperaturen <code>temp</code> eher hoch, und sinkt, je niedriger die Temperatur <code>temp</code> wird.</p>
<p>Die Temperatur <code>temp</code> bewegt sich dabei von hohen positiven Werten auf den Wert Null (wird also nicht negativ).</p>
<h2 id="detail-akzeptieren-von-verschlechterungen">Detail: Akzeptieren von Verschlechterungen</h2>
<div class='center'>
<p><a href="#R-image-7ecdd1a245c6ec90b4e871ad703b80bd" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Exp_e.svg/524px-Exp_e.svg.png?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7ecdd1a245c6ec90b4e871ad703b80bd"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Exp_e.svg/524px-Exp_e.svg.png?width=60%25&height=auto"></a></p>
<p><span class='origin'>Quelle: <a href="https://commons.wikimedia.org/wiki/File:Exp_e.svg" rel="external" target="_blank">&quot;Exp e.svg&quot;</a> by Marcel Marnitz, reworked by <a href="https://commons.wikimedia.org/wiki/User:Georg-Johann" rel="external" target="_blank">Georg-Johann</a> on Wikimedia Commons (<a href="https://en.wikipedia.org/wiki/Public_domain" rel="external" target="_blank">Public Domain</a>)</span></p>
</div>
<ul>
<li>Wahrscheinlichkeit zum Akzeptieren einer Verschlechterung: <code>math.exp(dE/temp)</code></li>
<li><span class="math align-center">$dE$</span> negativ =&gt; <span class="math align-center">$\exp\left(\text{dE}/\text{temp}\right) = \exp\left(-\frac{|\text{dE}|}{\text{temp}}\right) = \frac{1}{\exp\left(\frac{|\text{dE}|}{\text{temp}}\right)}$</span></li>
</ul>
<p><span class="math align-center">$\exp(a)$</span> bzw. <span class="math align-center">$e^a$</span>:</p>
<ul>
<li><span class="math align-center">$a<0$</span>: geht gegen 0</li>
<li><span class="math align-center">$a=0$</span>: 1</li>
<li><span class="math align-center">$a>0$</span>: steil (exponentiell) gegen Unendlich ...</li>
</ul>
<p>Wenn <span class="math align-center">$dE$</span> negativ ist, wird <code>math.exp(dE/temp)</code> ausgewertet. Damit ergibt sich wegen <span class="math align-center">$dE$</span> negativ:
<span class="math align-center">$\exp\left(\text{dE}/\text{temp}\right) = \exp\left(-\frac{|\text{dE}|}{\text{temp}}\right) = \frac{1}{\exp\left(\frac{|\text{dE}|}{\text{temp}}\right)}$</span>.
Betrachtung für <span class="math align-center">$dE$</span> (nur negativer Fall!) und <span class="math align-center">$\text{temp}$</span>:</p>
<ul>
<li>Temperatur <span class="math align-center">$\text{temp}$</span> hoch: <span class="math align-center">$a = \frac{|\text{dE}|}{\text{temp}}$</span> ist positiv und klein (nahe Null), d.h. <span class="math align-center">$\exp(a)$</span> nahe 1 (oder
größer), d.h. die Wahrscheinlichkeit <span class="math align-center">$1/\exp(a)$</span> ist nahe 1 (oder kleiner)</li>
<li>Temperatur <span class="math align-center">$\text{temp}$</span> wird kleiner und geht gegen Null: <span class="math align-center">$a = \frac{|\text{dE}|}{\text{temp}}$</span> ist positiv und wird größer, d.h.
<span class="math align-center">$\exp(a)$</span> geht schnell gegen Unendlich, d.h. die Wahrscheinlichkeit <span class="math align-center">$1/\exp(a)$</span> geht gegen 0</li>
</ul>
<h2 id="abkühlungsplan-problemabhängig-wählen">Abkühlungsplan problemabhängig wählen</h2>
<ul>
<li>
<p>Initiale Temperatur: So hoch, daß praktisch jede Änderung akzeptiert wird</p>
</li>
<li>
<p>Abkühlen: <span class="math align-center">$T_k = \alpha T_{k-1}$</span> mit <span class="math align-center">$0.8 \le \alpha \le 0.99$</span></p>
<ul>
<li>Ausreichend langsam abkühlen!</li>
<li>Typisch: jede Stufe so lange halten, daß etwa 10 Änderungen akzeptiert wurden</li>
</ul>
</li>
<li>
<p>Stop: Keine Verbesserungen in 3 aufeinander folgenden Temperaturen</p>
</li>
</ul>
<p>Der Abkühlungsplan muss problemabhängig gewählt werden. Das Beispiel zeigt typische Elementes
eines solchen Abkühlungsplans.</p>
<h2 id="eigenschaften-simulated-annealing">Eigenschaften Simulated Annealing</h2>
<ul>
<li>Vollständigkeit: ja (mit gewisser Wahrscheinlichkeit)</li>
<li>Optimalität: ja (mit gewisser Wahrscheinlichkeit)</li>
</ul>
<p>Voraussetzung: geeigneter Abkühlungsplan</p>
<h2 id="anwendungen-von-simulated-annealing">Anwendungen von Simulated Annealing</h2>
<ul>
<li>Flugplan-Scheduling</li>
<li>Layout-Probleme (Chipentwurf, Leiterplatten)</li>
<li>Produktionsplanung</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<p>Lokale Suchverfahren: Nur das Ergebnis zählt!</p>
<ul>
<li>Gradientenverfahren
<ul>
<li>Analogie Bergsteigen: Gehe in Richtung des stärksten Anstiegs der
Kostenfunktion =&gt; <strong>Hill-Climbing</strong></li>
<li>Achtung: Probleme mit lokalen Minima =&gt; <strong>Simulated Annealing</strong></li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106602&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Simulated Annealing (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Lokale Suche, GA</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Simulated Annealing: Abschnitt 4.1.2</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="genetische-algorithmen">Genetische Algorithmen</h1>

<p>Lokale Suche mit Methoden, die der biologischen Evolution abgeschaut bzw. nachempfunden sind.</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro.html">Einführung Evolutionäre Algorithmen</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga.html">Modellierung mit Genetischen Algorithmen</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Genetische Algorithmen</h1>
<article class="default">
<h1>Einführung Evolutionäre Algorithmen</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Lokale Suchverfahren: Nur das Ergebnis zählt!</p>
<p>Evolutionäre Algorithmen sind lokale Suchverfahren, wobei gleichzeitig an mehreren Stellen im Problemraum
gesucht wird. Sie bedienen sich Mechanismen aus der Evolution: Es gibt eine Population von Individuen,
die jedes das Problem kodieren (&quot;vollständige Zustandsbeschreibung&quot;) und damit im Laufe der Suche zu einer
möglichen Lösung werden können.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/pKdKcPLI7V4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Einführung Evolutionäre Algorithmen</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/7804b1839761e3ca43478cbd5eea342fc6f61539fe481d34efe0d3d64747ef4fe2a6de4c9f079ce806f6ac7194b48b0c4324bd17c3483dd318d027d94242d8f9' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Einführung Evolutionäre Algorithmen</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Problematik der lokalen Minima bei Gradientenverfahren</li> <li>(K2) Überblick über die verschiedenen Strömungen</li> <li>(K2) Prinzipieller Ablauf von Genetischen Algorithmen</li></ul>
  </div>
</div>




    <h2 id="evolution-sehr-erfolgreich-bei-anpassung">Evolution sehr erfolgreich bei Anpassung</h2>
<p><a href="#R-image-8864b2d7aa1375bade2728b3ae210ac6" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://images.unsplash.com/flagged/photo-1552863473-6e5ffe5e052f?fm=png&crop=entropy&cs=tinysrgb?width=60%25&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8864b2d7aa1375bade2728b3ae210ac6"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://images.unsplash.com/flagged/photo-1552863473-6e5ffe5e052f?fm=png&crop=entropy&cs=tinysrgb?width=60%25&height=auto"></a></p>
<p><span class='origin'>Quelle: <a href="https://unsplash.com/photos/aWDgqexSxA0" rel="external" target="_blank">Photo</a> by <a href="https://unsplash.com/@jplenio" rel="external" target="_blank">Johannes Plenio</a> on Unsplash.com (<a href="https://unsplash.com/license" rel="external" target="_blank">Unsplash License</a>)</span></p>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p><span class='alert'><strong>Wie funktioniert's?</strong></span></p>
</span></span>
</div>
<h2 id="ea----zutaten-und-mechanismen">EA -- Zutaten und Mechanismen</h2>
<ul>
<li>
<p>Zutaten:</p>
<ul>
<li><strong>Individuen</strong>: Kodierung möglicher Lösungen</li>
<li><strong>Population</strong> von Individuen</li>
<li><strong>Fitnessfunktion</strong>: Bewertung der Angepasstheit</li>
</ul>
</li>
<li>
<p>Mechanismen (&quot;Operatoren&quot;):</p>
<ul>
<li>Selektion</li>
<li>Rekombination (Crossover)</li>
<li>Mutation</li>
</ul>
</li>
</ul>
<h2 id="ea----allgemeiner-ablauf">EA -- Allgemeiner Ablauf</h2>
<p><a href="#R-image-aaa448b762f294cfcea467e0f6a25802" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro/ea_prinz.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-aaa448b762f294cfcea467e0f6a25802"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro/ea_prinz.png?width=80%25&height=auto"></a></p>
<h2 id="ea----beispiel">EA -- Beispiel</h2>
<p><a href="#R-image-4109fcc67f9b24374d7164b5d033a034" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro/4-queens-example.png?width=40%25&height=auto" style=" height: auto; width: 40%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-4109fcc67f9b24374d7164b5d033a034"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro/4-queens-example.png?width=40%25&height=auto"></a></p>
<p>Jedes Individuum kodiert ein Spielfeld mit einer konkreten Anordnung <strong>aller</strong>
Königinnen =&gt; <strong>Vollständige Zustandsbeschreibung</strong>.</p>
<p>Dabei korrespondiert der Index in das Array des Individuums mit der jeweiligen
Spalte des Spielfelds. Die Zahl an einer Arrayposition gibt dann an, in welcher
Zeile in dieser Spalte eine Königin ist.</p>
<p>Crossover: Die ausgewählten Individuen werden an der selben Stelle aufgetrennt
und die Hälften verkreuzt zu zwei neuen Individuen zusammengesetzt. Es entstehen
zwei neue Anordnungen der Königinnen (zwei neue Spielfelder).</p>
<h2 id="ea----strömungen">EA -- Strömungen</h2>
<ol>
<li>
<p><strong>Genetische Algorithmen</strong> (GA)</p>
<ul>
<li>Holland und Goldberg (ab 1960)</li>
<li>Binäre Lösungsrepräsentation (Bitstring): <span class="math align-center">$\mathbf{g} = (g_1, \dots, g_m)\in \{ 0,1\}^m$</span></li>
<li>Fitnessbasierte stochastische Selektion</li>
<li><span class="math align-center">$\mu$</span> Eltern erzeugen <span class="math align-center">$\mu$</span> Kinder</li>
</ul>
</li>
<li>
<p><strong>Evolutionsstrategien</strong> (ES)</p>
<ul>
<li>Rechenberg und Schwefel (ab 1960)</li>
<li>Kodierung reellwertiger Parameter: <span class="math align-center">$\mathbf{g} = (\mathbf{x}, \mathbf{\sigma})$</span>
mit <span class="math align-center">$\mathbf{x} = (x_1, \dots, x_n) \in \mathbb{R}^n$</span></li>
<li><span class="math align-center">$\mu$</span> Eltern erzeugen <span class="math align-center">$\lambda$</span> Kinder mit <span class="math align-center">$\mu \le \lambda$</span></li>
</ul>
</li>
<li>
<p><strong>Evolutionäre Programmierung</strong> (EP)</p>
</li>
</ol>
<p><em>Hinweis</em>: Häufig finden sich Mischformen, beispielsweise GA mit reellwertigen Parametern</p>
<p><em>Hinweis</em>: Im Folgenden werden <strong>Genetische Algorithmen</strong> (GA) betrachtet. Sie
finden jeweils Hinweise auf die Gestaltung der Operatoren bei ES.</p>
<h2 id="anwendungsbeispiele-für-evolutionäre-algorithmen">Anwendungsbeispiele für Evolutionäre Algorithmen</h2>
<ul>
<li>Berechnung und Konstruktion komplexer Bauteile: beispielsweise
Tragflächenprofile (Flugzeuge), Brücken oder Fahrzeugteile unter
Berücksichtigung bestimmter Nebenbedingungen</li>
<li>Scheduling-Probleme: Erstellung von Stunden- und Raumplänen oder Fahrplänen</li>
<li>Berechnung verteilter Netzwerktopologien: Wasserversorgung, Stromversorgung,
Mobilfunk</li>
<li>Layout elektronischer Schaltkreise</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<p>Lokale Suchverfahren: Nur das Ergebnis zählt!</p>
<ul>
<li>Evolutionäre Algorithmen: Unterschied GA und ES (grober Überblick)</li>
</ul>


    



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Lokale Suche, GA</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Baeck1996'>[Baeck1996] <strong>Evolutionary Algorithms in Theory and Praxis</strong><br>Bäck, T., Oxford University Press, 1996. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-1950-9971-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-1950-9971-3</a>.</li> <li id='id_Michalewicz1996'>[Michalewicz1996] <strong>Genetic Algorithms + Data Structures = Evolution Programs</strong><br>Michalewicz, Z., Springer, 1996. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-5406-0676-5' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-5406-0676-5</a>.</li> <li id='id_Nissen1997'>[Nissen1997] <strong>Einführung in Evolutionäre Algorithmen</strong><br>Nissen, V., Vieweg+Teubner Verlag, 1997. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-5280-5499-1' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-5280-5499-1</a>.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>GA: Abschnitt 4.1.4</em></li> <li id='id_Schwefel1995'>[Schwefel1995] <strong>Evolution and Optimum Seeking</strong><br>Schwefel, H. P., Wiley, 1995. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-4715-7148-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-4715-7148-3</a>.<br><em>Originalarbeit zu Evolutionsstrategien</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Modellierung mit Genetischen Algorithmen</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Lokale Suchverfahren: Nur das Ergebnis zählt!</p>
<p>Evolutionäre Algorithmen sind lokale Suchverfahren, wobei gleichzeitig an mehreren Stellen im Problemraum
gesucht wird. Sie bedienen sich Mechanismen aus der Evolution: Es gibt eine Population von Individuen,
die jedes das Problem kodieren (&quot;vollständige Zustandsbeschreibung&quot;) und damit im Laufe der Suche zu einer
möglichen Lösung werden können.</p>
<p>Die Individuen werden mit Hilfe einer Fitnessfunktion bewertet, wie gut sie bereits an das Problem angepasst
sind (bzw. wie sehr sie bereits der gesuchten Lösung entsprechen). Über eine fitnessproportionale Selektion
werden Individuen ausgewählt, aus denen mittels Rekombination (auch &quot;Crossover&quot; genannt) neue Individuen mit
Eigenschaften der Eltern erzeugt werden. Über eine Mutation werden dann noch Elemente der neuen Individuen
leicht verändert, bevor diese zur neuen Population werden ...</p>
<p>Durch das Anwenden von Rekombination und Mutation springt man im Problemraum umher. Auch wenn als Basis die
fitteren (angepassteren) Individuen dienen, kann es wie bei allen lokalen Suchverfahren vorkommen, dass
sich der Algorithmus in lokalen Minima (bzw. lokalen Maxima, je nach Richtung der Optimierung) festfrisst.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/Sd5AA6LIEOc' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Modellierung mit EA/GA</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/ccdf3f05e3eb4e19644f8505b53eeeb0219cddca7902d5840fae19a0816feebfc2c3800b54f3e95bd2a8f26405ea7c40385303a98f6d7d3c94c34c10f4be43d0' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Modellierung mit EA/GA</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) GA: Kodierung, Fitnessfunktion, Ablauf, Operatoren, Auswertung</li></ul>
  </div>
</div>




    <h2 id="ea----allgemeiner-ablauf">EA -- Allgemeiner Ablauf</h2>
<p><a href="#R-image-f1043e79c3b805a60520fa2fec140049" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga/ea_prinz.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f1043e79c3b805a60520fa2fec140049"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga/ea_prinz.png?width=80%25&height=auto"></a></p>
<h2 id="kodierung-individuen">Kodierung Individuen</h2>
<ul>
<li>
<p>Binäre Lösungsrepräsentation (Bitstring):
<span class="math align-center">$\mathbf{g} = (g_1, \dots, g_m)\in \{ 0,1\}^m$</span></p>
<ul>
<li>String gliedert sich in <span class="math align-center">$n$</span> Elemente (mit <span class="math align-center">$n \le m$</span>)
=&gt; jedes Segment entspricht einer Problemvariablen</li>
<li>Dekodierungsfunktion
<span class="math align-center">$\Gamma : \{0,1\}^m \to \mathbb{R}^n$</span></li>
</ul>
<p>Alle relevanten Aspekte des Problems müssen in die Codierung einfließen!</p>
<p>Bei ES hat man einen Vektor mit reellen Zahlen, wobei jeder Eintrag einen
Parameter des Problems darstellt. Eine Dekodierungsfunktion benötigt man
entsprechend nicht.</p>
<p>Bei der Erzeugung der Startpopulation werden die Individuen <strong>zufällig</strong>
(mit zufälligen Werten) initialisiert.</p>
</li>
<li>
<p>Fitnessfunktion <span class="math align-center">$\Phi$</span> ordnet jedem Individuum <span class="math align-center">$\mathbf{g}_i$</span> eine reelle Zahl zu:
<span class="math align-center">$$\Phi(\mathbf{g}_i) = F(\Gamma(\mathbf{g}_i)) - w\cdot\sum_j(Z_j(\Gamma(\mathbf{g}_i)))^2$$</span></p>
<ul>
<li>Zielfunktion <span class="math align-center">$F$</span>: wie sehr genügt ein Individuum bereits dem Optimierungproblem</li>
<li>Strafterme <span class="math align-center">$Z_j$</span>: Anreicherung der Optimierung mit weiteren Informationen</li>
<li>Gewichte <span class="math align-center">$w$</span>: statisch oder dynamisch (Abkühlen)</li>
</ul>
<p>Die Wahl einer guten Fitnessfunktion ist oft eine Herausforderung, aber
dennoch wichtig, da damit die Suche gesteuert wird!</p>
</li>
</ul>
<h2 id="selektion-erstelle-matingpool-mit-hahahugoshortcode19s10hbhb-individuen">Selektion: Erstelle Matingpool mit <span class="math align-center">$\mu$</span> Individuen</h2>
<ul>
<li>
<p>Fitnessproportionale Selektion (<em>Roulette Wheel Selection</em>):
Auswahlwahrscheinlichkeit für Individuum
<span class="math align-center">$\mathbf{g}_k$</span>:
<span class="math align-center">$$p_{sel}(\mathbf{g}_k) = \frac{\Phi(\mathbf{g}_k)}{\sum_j \Phi(\mathbf{g}_j)}$$</span>
=&gt; Voraussetzung: positive Fitnesswerte</p>
</li>
<li>
<p>Turnier-Selektion (<em>Tournament Selection</em>):</p>
<ul>
<li>Turniergröße <span class="math align-center">$\xi$</span></li>
<li>Turnier: ziehe <span class="math align-center">$\xi$</span> Individuen gleichverteilt (mit Zurücklegen!)
und kopiere fittestes Individuum in den Matingpool</li>
<li>Führe <span class="math align-center">$\mu$</span> Turniere durch</li>
</ul>
</li>
</ul>
<p><em>Hinweis</em>: Es gibt noch viele weitere Selektionsmechanismen. Die vorgestellten
sind in der Praxis am gebräuchlichsten.</p>
<p>Über die Selektion wird der sogenannte &quot;Selektionsdruck&quot; aufgebaut: Wie gut
muss ein Individuum sein (im Vergleich zu den restlichen Individuen in der
Population), damit es eine Chance zur Reproduktion erhält? Dürfen sich nur
die &quot;Guten&quot; fortpflanzen, oder erhalten auch die &quot;Schlechten&quot; eine gewisse
Chance?</p>
<p>Da jedes Individuum einen Punkt im Suchraum darstellt, beeinflusst die Wahl
der Selektion die Geschwindigkeit der Suche, begünstigt u.U. aber auch ein
eventuelles Festfahren in lokalen Minima. Dies kann beispielsweise geschehen,
wenn immer nur die &quot;Guten&quot; selektiert werden, aber die &quot;Guten&quot; der Population
sich in der Nähe eines lokalen Minimums befinden. Dann werden auch die
Nachfolger sich wieder dort aufhalten.</p>
<h2 id="crossover-erzeuge-zwei-nachkommen-aus-zwei-eltern">Crossover: Erzeuge zwei Nachkommen aus zwei Eltern</h2>
<p>Festlegung der Crossover-Wahrscheinlichkeit <span class="math align-center">$p_{cross}$</span>
(typisch: <span class="math align-center">$p_{cross} \ge 0.6$</span>)</p>
<ol>
<li>
<p>Selektiere Eltern <span class="math align-center">$\mathbf{g}_a$</span> und <span class="math align-center">$\mathbf{g}_b$</span> <strong>gleichverteilt</strong> aus Matingpool</p>
</li>
<li>
<p>Zufallsexperiment:</p>
<ul>
<li>
<p>mit <span class="math align-center">$1-p_{cross}$</span>: Kinder identisch zu Eltern (kein Crossover)</p>
</li>
<li>
<p>mit <span class="math align-center">$p_{cross}$</span>: Crossover mit <span class="math align-center">$\mathbf{g}_a$</span> und <span class="math align-center">$\mathbf{g}_b$</span></p>
<ul>
<li>Ziehe <span class="math align-center">$i$</span> gleichverteilt mit <span class="math align-center">$1 < i < m$</span></li>
<li>Kinder aus <span class="math align-center">$\mathbf{g}_a$</span> und <span class="math align-center">$\mathbf{g}_b$</span> zusammenbauen:
<span class="math align-center">$$\mathbf{g}_c = (g_{a,1}, \dots, g_{a,i}, \; g_{b,{i+1}}, \dots, g_{b,m})$$</span>
und
<span class="math align-center">$$\mathbf{g}_d = (g_{b,1}, \dots, g_{b,i}, \; g_{a,{i+1}}, \dots, g_{a,m})$$</span></li>
</ul>
<p>=&gt; Trenne Eltern an gleicher Stelle auf, vertausche Bestandteile</p>
</li>
</ul>
</li>
<li>
<p>Gehe zu Schritt 1, bis insg. <span class="math align-center">$\mu$</span> Nachkommen</p>
</li>
</ol>
<p><em>Anmerkung</em>: Die Eltern werden jeweils in die Ausgangsmenge zurückgelegt.</p>
<p>Mit einer kleinen Wahrscheinlichkeit sind die Kinder also identisch zu den
Eltern. Dies ist im Sinne der lokalen Suche wichtig, um bereits erreichte
gute Positionen im Suchraum nicht zu verlieren: Es könnte sein, dass die
Nachfolger alle schlechter sind ...</p>
<p>Varianten: <span class="math align-center">$N$</span>-Punkt-Crossover, Shuffle-Crossover</p>
<p>Bei ES wird parameterweise gekreuzt. Dabei gibt es verschiedene Möglichkeiten:
Übernahme eines Parameters von einem Elternteil, Verrechnen (beispielsweise
Mitteln) der Werte beider Eltern, ... Bei ES heißt &quot;Crossover&quot; deshalb oft
&quot;Rekombination&quot;.</p>
<h2 id="mutation">Mutation</h2>
<ul>
<li>
<p>Mutationswahrscheinlichkeit <span class="math align-center">$p_{mut}$</span>
(typische Werte: <span class="math align-center">$p_{mut} = 0.01$</span> oder <span class="math align-center">$p_{mut} = 0.001$</span>)</p>
</li>
<li>
<p>Für alle Individuen:</p>
<ul>
<li>
<p>Mutiere jedes Gen eines Individuums mit <span class="math align-center">$p_{mut}$</span>:</p>
<span class="math align-center">$$
        g_i^{(t+1)} = \left\{
        \begin{array}{ll}
            \neg g_i^{(t)} & \mbox{ falls } \chi_i \le p_{mut}\\[5pt]
            \phantom{\neg} g_i^{(t)} & \mbox{ sonst }
        \end{array}
        \right.
        $$</span>
<p>=&gt;<span class="math align-center">$\chi_i$</span> gleichverteilte Zufallsvariable (Intervall <span class="math align-center">$[0,1]$</span>),
für jedes Bit <span class="math align-center">$g_i$</span> neu bestimmen</p>
</li>
</ul>
</li>
</ul>
<p><em>Anmerkung</em>: Die optimale Mutationsrate <span class="math align-center">$p_{mut}^*$</span> ist von Länge <span class="math align-center">$m$</span> des
Bitstrings abhängig; annäherbar durch <span class="math align-center">$p_{mut}^* \approx 1/m$</span>.</p>
<p>Die beim Crossover erstellten Nachfolger liegen im Suchraum in der Nähe der
Eltern. Durch die Mutationsrate bestimmt man, ob und wie weit sich ein Kind
entfernen kann. Dies entspricht dem Bild des &quot;Schüttelns&quot; der
Zustandslandschaft.</p>
<p>Bei ES unterscheidet man Mutationswahrscheinlichkeit und Mutationsrate. Es wird
parameterweise mutiert.</p>
<h2 id="bewertungskriterien">Bewertungskriterien</h2>
<p>Vorsicht: Es handelt sich um Zufallsexperimente. Wenn man nicht nur direkt
nach einer Lösung sucht, sondern beispielsweise Parametereinstellungen oder
die Wahl der Fitnessfunktion für ein Problem vergleichen will, muss man jeweils
mehrere Experimente mit der selben Einstellung machen und Kenngrößen berechnen.</p>
<p><strong>Geschwindigkeit: AES</strong> <em>Average Evaluations to a Solution</em>
<span class="math align-center">$$
    \mbox{AES } = \frac{\sum\limits_{i \in \mbox{erfolgreiche Läufe}} \mbox{Generationen von Lauf } i}{\mbox{Anzahl der erfolgreichen Läufe}}
$$</span></p>
<p>Die AES liegt im Intervall <span class="math align-center">$[0, maxGen]$</span>.</p>
<p><strong>Lösungswahrscheinlichkeit: SR</strong> <em>Success Rate</em>
<span class="math align-center">$$
    \mbox{SR } = \frac{\mbox{Anzahl der erfolgreichen Läufe}}{\mbox{Anzahl aller Läufe}}
$$</span></p>
<p>Die SR liegt im Intervall <span class="math align-center">$[0, 1]$</span>.</p>
<h2 id="typische-läufe">Typische Läufe</h2>
<div class='columns'>
<div class='column'>
<p><a href="#R-image-5b2343f0966e4a4721659f63464141b1" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga/typischerLauf_ritterIII_mG500M15L100_fail.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-5b2343f0966e4a4721659f63464141b1"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga/typischerLauf_ritterIII_mG500M15L100_fail.png?width=80%25&height=auto"></a></p>
</div>
<div class='column'>
<p><a href="#R-image-8fc3168404a24286e8d3ff1c18303271" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga/typischerLauf_ritterIII_mG500M15L100_success.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8fc3168404a24286e8d3ff1c18303271"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga/typischerLauf_ritterIII_mG500M15L100_success.png?width=80%25&height=auto"></a></p>
</div>
</div>
<ul>
<li>Populationsgröße <span class="math align-center">$\mu=15$</span></li>
<li>Anzahl Nachfahren <span class="math align-center">$\lambda=100$</span></li>
<li>Abbruch nach <span class="math align-center">$maxGen=200$</span> Generationen</li>
</ul>
<p>Stochastischer Algorithmus! Ausreichend Wiederholungen durchführen und mitteln!</p>
<p><em>Hinweis</em>: Die Parameter müssen problemabhängig gewählt werden. Zu hohe Werte
für <span class="math align-center">$\mu$</span> und <span class="math align-center">$\lambda$</span> führen dazu, dass man bei kleinen Problemen mit hoher
Wahrscheinlichkeit bereits am Anfang eine Lösung &quot;würfelt&quot;, also gar kein GA
nutzt. Wenn dies allerdings nicht passiert, sorgt eine hohe Populationsgröße
dafür, dass jeder Schritt sehr lange dauert. Die Abbruchgrenze ist ebenfalls
mit Augenmaß zu wählen: Ein zu kleiner Wert sorgt für zu frühen Abbruch (keine
Lösung!), ein zu hoher Wert sorgt beim Festfressen des Algorithmus für eine
unnötige weitere &quot;Suche&quot; ...</p>
<h2 id="wrap-up">Wrap-Up</h2>
<p>Lokale Suchverfahren: Nur das Ergebnis zählt!</p>
<ul>
<li>Evolutionäre Algorithmen:
<ul>
<li>Begriffe: Individuum, Population, Kodierung</li>
<li>Operationen: Selektion, Rekombination, Mutation</li>
<li>Bewertung mit Fitnessfunktion</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106580&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest EA/GA (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Sudoku</strong></p>
<p>Ein <span class="math align-center">$9 \times 9$</span>-<em>Sudoku</em>-Rätsel soll mit einem GA gelöst werden.</p>
<p>Geben Sie für dieses Problem jeweils eine geeignete <strong>Kodierung</strong> der Individuen, passende Operatoren (<strong>Crossover</strong>, <strong>Mutation</strong>) und eine geeignete <strong>Fitnessfunktion</strong> an, damit das Problem mit einem GA gelöst werden kann. Begründen Sie Ihre Wahl!</p>
<p>Was würden Sie noch benötigen, um das Probleme mit Simulated Annealing lösen zu können?</p>
<p><strong>Travelling Salesman Problem</strong></p>
<p>Das <em>Travelling Salesman Problem</em> für 10 Städte, d.h. das Finden der kürzesten Route zwischen 10 Städten, soll mit einem GA gelöst werden.</p>
<p>Geben Sie für dieses Problem jeweils eine geeignete <strong>Kodierung</strong> der Individuen, passende Operatoren (<strong>Crossover</strong>, <strong>Mutation</strong>) und eine geeignete <strong>Fitnessfunktion</strong> an, damit das Problem mit einem GA gelöst werden kann. Begründen Sie Ihre Wahl!</p>
<p>Was würden Sie noch benötigen, um das Probleme mit Simulated Annealing lösen zu können?</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Lokale Suche, GA</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Baeck1996'>[Baeck1996] <strong>Evolutionary Algorithms in Theory and Praxis</strong><br>Bäck, T., Oxford University Press, 1996. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-1950-9971-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-1950-9971-3</a>.</li> <li id='id_Michalewicz1996'>[Michalewicz1996] <strong>Genetic Algorithms + Data Structures = Evolution Programs</strong><br>Michalewicz, Z., Springer, 1996. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-5406-0676-5' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-5406-0676-5</a>.</li> <li id='id_Nissen1997'>[Nissen1997] <strong>Einführung in Evolutionäre Algorithmen</strong><br>Nissen, V., Vieweg+Teubner Verlag, 1997. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-5280-5499-1' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-5280-5499-1</a>.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>GA: Abschnitt 4.1.4</em></li> <li id='id_Schwefel1995'>[Schwefel1995] <strong>Evolution and Optimum Seeking</strong><br>Schwefel, H. P., Wiley, 1995. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0-4715-7148-3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0-4715-7148-3</a>.<br><em>Originalarbeit zu Evolutionsstrategien</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="spiele">Spiele</h1>

<p>Man kann Spiele auch als Suchproblem betrachten und als Ziel die Suche nach dem optimalen Zug definieren.</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro.html">Einführung Optimale Spiele</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax.html">Minimax</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html">Heuristiken</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html">Alpha-Beta-Pruning</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Spiele</h1>
<article class="default">
<h1>Einführung Optimale Spiele</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Spiele können als Suchproblem betrachtet werden. Dabei sind in der Regel mehrere Spieler (&quot;Agenten&quot;) beteiligt. Bei manchen
Spielen ist die Umgebung (der Spielzustand) vollständig einsehbar, bei anderen nur teilweise (Kartenspiele). Bei manchen
Spielen kommt eine Zufallskomponente zum Wirken.</p>
<p>Spiele sind in der KI deshalb so interessant, weil bei der Suche riesige Suchbäume entstehen (bzw. durchsucht werden müssten).
Da die Ressourcen normalerweise begrenzt sind (denken Sie an die Reaktionszeit auf einen Zug des Gegners), muss man hier
intelligente Lösungen finden. (Einige davon werden wir in den folgenden Sitzungen anschauen).</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/wVYhbgtzxhs' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Einführung Optimale Spiele</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/6d02dc45505b838df7eaa0a5d8c4a65d608a2898f699716db3576caa9abf421debb61c3d5fc4c5effb0b5213b76c573df50d6aff203199eba8d66548c3238ba3' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Einführung Optimale Spiele</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Spiele als Suchproblem</li> <li>(K2) Eigenschaften guter Spielalgorithmen</li></ul>
  </div>
</div>




    <h2 id="backgammon-zwei-spieler-was-ist-der-beste-zug">Backgammon: Zwei Spieler, was ist der beste Zug?</h2>
<p><a href="#R-image-5166c07f292945d7255bf8a8205c1d29" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://live.staticflickr.com/3670/11267311625_e4758ff425_o_d.jpg?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-5166c07f292945d7255bf8a8205c1d29"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://live.staticflickr.com/3670/11267311625_e4758ff425_o_d.jpg?width=60%25&height=auto"></a></p>
<p><span class='origin'>Quelle: <a href="https://www.flickr.com/photos/83436399@N04/11267311625" rel="external" target="_blank">&quot;position-backgammon-decembre&quot;</a> by <a href="https://www.flickr.com/photos/83436399@N04" rel="external" target="_blank">serialgamer_fr</a> on Flickr.com (<a href="https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=rich" rel="external" target="_blank">CC BY 2.0</a>)</span></p>
<p>Zwei Spieler, ein Spielstand und ein Würfelergebnis: <span class='alert'><strong>Was ist jetzt der beste Zug?!</strong></span></p>
<h2 id="motivation-unterschied-zu-suche">Motivation: Unterschied zu Suche?!</h2>
<p><a href="#R-image-2e578d179b377adc6820310b558eee74" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro/tttEnd.png?width=30%25&height=auto" style=" height: auto; width: 30%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-2e578d179b377adc6820310b558eee74"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro/tttEnd.png?width=30%25&height=auto"></a></p>
<p>=&gt; Mehrere <span class='alert'><strong>konkurrierende</strong></span> Agenten an Suche beteiligt!</p>
<p>=&gt; (Re-) Aktion des Gegners unbekannt/nicht vorhersehbar.</p>
<h2 id="spiele-und-umgebungen">Spiele und Umgebungen</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">Deterministisch</th>
          <th style="text-align: left">Zufallskomponente</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Voll beobachtbar</td>
          <td style="text-align: left">Schach, Go, ...</td>
          <td style="text-align: left">Backgammon, Monopoly</td>
      </tr>
      <tr>
          <td style="text-align: left">Partiell beobachtbar</td>
          <td style="text-align: left">Schiffe-versenken</td>
          <td style="text-align: left">Bridge, Poker, Skat, ...</td>
      </tr>
  </tbody>
</table>
<p>=&gt; Bis auf Roboterfußball in KI traditionell keine physischen Spiele!</p>
<h2 id="brettspiele-sind-interessant-für-ki">Brettspiele sind interessant für KI</h2>
<ul>
<li>
<p>Brettspiele gut abstrakt darstellbar:</p>
<ul>
<li>Zustände einfach repräsentierbar</li>
<li>Aktionen wohldefiniert (und i.d.R. sehr einfach)</li>
<li>Realisierung als Suchproblem möglich</li>
</ul>
</li>
<li>
<p><strong>Problem</strong>: Suchbäume werden in Praxis riesig</p>
<p>Beispiel <strong>Schach</strong>:</p>
<ul>
<li>Im Mittel 35 Aktionen (<em>branching factor</em>) von jeder Position</li>
<li>Oft mehr als 40 Züge pro Spieler =&gt; Suchbäume mit mehr als 80 Ebenen</li>
<li><span class="math align-center">$35^{80} \approx 10^{123}$</span> mögliche Knoten!</li>
<li>(Aber &quot;nur&quot; rund <span class="math align-center">$10^{40}$</span> <em>verschiedene</em> Zustände)</li>
</ul>
<p><span class='origin'>Quelle: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro.html#id_Russell2020">[Russell2020, pp. 193-196]</a></span></p>
</li>
</ul>
<h2 id="eigenschaften-guter-spielalgorithmen">Eigenschaften guter Spielalgorithmen</h2>
<ul>
<li>
<p>Zeit begrenzt</p>
<ul>
<li>Irgendeine gute Entscheidung treffen! =&gt; Bewertungsfunktion (auch für Zwischenzustände)</li>
</ul>
</li>
<li>
<p>Speicher begrenzt</p>
<ul>
<li>Evaluierungsfunktion für Zwischenzustände</li>
<li>Löschen von irrelevanten Zweigen</li>
</ul>
</li>
<li>
<p>Strategien nötig</p>
<ul>
<li>Vorausschauend spielen (Züge &quot;vorhersehen&quot;)</li>
</ul>
</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Spiele kann man als Suchproblem betrachten</li>
<li>Merkmale:
<ul>
<li>Mehrere Agenten beteiligt</li>
<li>Beobachtbarkeit der Umgebung</li>
<li>Zufallskomponente</li>
<li>Spielstrategie</li>
</ul>
</li>
<li>Problem: Riesige Spielbäume</li>
<li>Umgang mit begrenzten Ressourcen (Zeit, Speicher)</li>
</ul>


    



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Spiele</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Einführung Spiele: Abschnitt 6.1</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Minimax</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Mit dem Minimax-Algorithmus können optimale Züge berechnet werden. Dabei wird von zwei
Spielern <code>Max</code> und <code>Min</code> ausgegangen, die abwechselnd ziehen und beide optimal spielen.
Wenn <code>Max</code> gewonnen hat, wird der Spielausgang mit +1 bewertet, wenn <code>Min</code> gewonnen hat
mit -1, und mit 0 sonst. Damit hat man ein sogenanntes &quot;Nullsummenspiel&quot; (der Gewinn des
einen Spielers ist der Verlust des anderen) und kann den Algorithmus so gestalten, dass
<code>Max</code> stets den Zug wählt, der das Spielergebnis maximiert und <code>Min</code> entsprechend den
Zug wählt, der das Spielergebnis minimiert (daher auch die Namen der Spieler).</p>
<p>Minimax baut den gesamten Spielbaum bis zu den Blättern auf. Die Blätter (Spielausgang)
werden mit einer <code>Utility</code>-Funktion bewertet, und diese Bewertung wird dann im Spielbaum
nach oben gereicht.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/aKtF__lMMsw' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Minimax</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/1d5286179490be0627b5a19793b423753cba79388b4966b654bde224a31af81c9481e85a30057276826b3d8f7836a042d0f9e15c2cb82613a4374050b62ca6d2' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Minimax</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Minimax-Algorithmus</li></ul>
  </div>
</div>




    <h2 id="spiele-als-suchproblem-minimax">Spiele als Suchproblem: Minimax</h2>
<h3 id="terminologie">Terminologie</h3>
<ul>
<li>
<p>Zwei abwechselnd spielende Spieler: <code>MAX</code> und <code>MIN</code>, wobei <code>MAX</code> beginnt</p>
<ul>
<li>Beide Spieler spielen in jedem Zug <span class='alert'>optimal</span></li>
<li>Spielergebnis wird aus Sicht von <code>MAX</code> bewertet:
<ul>
<li><span class="math align-center">$+1$</span>, wenn Spieler <code>MAX</code> gewinnt</li>
<li><span class="math align-center">$-1$</span>, wenn Spieler <code>MIN</code> gewinnt</li>
<li><span class="math align-center">$0$</span>, wenn unentschieden</li>
</ul>
</li>
<li>Spieler <code>MAX</code> versucht, das Spielergebnis zu <strong>maximieren</strong></li>
<li>Spieler <code>MIN</code> versucht, das Spielergebnis zu <strong>minimieren</strong></li>
</ul>
</li>
<li>
<p>Startzustand: Initialer Zustand des Spielbrettes</p>
</li>
<li>
<p>Aktionen: Legale Züge, abhängig vom Spielzustand</p>
</li>
<li>
<p>Zieltest: Ist das Spiel vorbei?</p>
<p>=&gt; Startzustand und anwendbare Aktionen definieren den Zustandsraum.</p>
</li>
<li>
<p>Nutzenfunktion: <span class="math align-center">$\operatorname{UTILITY}(s,p)$</span>: Wert des Spiels für
Spieler <span class="math align-center">$p$</span> im Spielzustand <span class="math align-center">$s$</span></p>
</li>
<li>
<p>Strategie: Spieler benötigen <strong>Strategie</strong>, um zu gewünschtem Endzustand
zu kommen <em>(unabhängig von den Entscheidungen des Gegenspielers)</em>
=&gt; einfacher Pfad von Start zu Ziel reicht nicht</p>
</li>
</ul>
<p><em>Hinweis</em>: Nullsummenspiel! (Der Gewinn des einen Spielers ist der Verlust des
anderen Spielers.)</p>
<p>Eine mit dem Minimax-Algorithmus berechnete Strategie wird auch
<em>Minimax-Strategie</em> genannt. Sie sichert dem betreffenden Spieler den
höchstmöglichen Gewinn, der <strong>unabhängig</strong> von der Spielweise des Gegners
erreichbar ist.</p>
<p>Bei Nicht-Nullsummenspielen, bei denen die Niederlage des Gegners nicht
zwangsläufig mit dem eigenen Gewinn zusammenfällt (d.h. Gewinn des einen
Spielers <span class="math align-center">$\ne$</span> Verlust des anderen Spielers), liefert der Minimax-Algorithmus
nicht unbedingt eine optimale Strategie.</p>
<h3 id="spielbaum-ttt">Spielbaum TTT</h3>
<p><a href="#R-image-982d9b151fa900080b35d9fb92d6188d" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax/tttSpielbaum.png?width=50%25&height=auto" style=" height: auto; width: 50%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-982d9b151fa900080b35d9fb92d6188d"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax/tttSpielbaum.png?width=50%25&height=auto"></a></p>
<h3 id="minimax-idee">Minimax (Idee)</h3>
<ol>
<li>Erzeuge kompletten Suchbaum mit Tiefensuche</li>
<li>Wende Nutzenfunktion (<em>Utility</em>) auf jeden Endzustand an</li>
<li>Ausgehend von Endzuständen =&gt; Bewerte Vorgängerknoten:
<ul>
<li>Knoten ist <code>Min</code>-Knoten:
Nutzen ist das <strong>Minimum</strong> der Kindknoten</li>
<li>Knoten ist <code>Max</code>-Knoten:
Nutzen ist das <strong>Maximum</strong> der Kindknoten</li>
</ul>
</li>
<li>Startknoten: <code>Max</code> wählt den Zug, der zum Nachfolger mit der
höchsten Bewertung führt</li>
</ol>
<p><em>Annahme</em>: Beide spielen perfekt. Fehler verbessern das Resultat für den Gegner.</p>
<h2 id="minimax-algorithmus-funktionen-für-max--und-min-knoten">Minimax-Algorithmus: Funktionen für MAX- und MIN-Knoten</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Max</span><span style="color:#f92672">-</span>Value(state):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> Terminal<span style="color:#f92672">-</span>Test(state): <span style="color:#66d9ef">return</span> Utility(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>INF
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (a, s) <span style="color:#f92672">in</span> Successors(state):
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> MAX(v, Min<span style="color:#f92672">-</span>Value(s))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> v</span></span></code></pre></div>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Min</span><span style="color:#f92672">-</span>Value(state):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> Terminal<span style="color:#f92672">-</span>Test(state): <span style="color:#66d9ef">return</span> Utility(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span> <span style="color:#f92672">+</span>INF
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (a, s) <span style="color:#f92672">in</span> Successors(state):
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> MIN(v, Max<span style="color:#f92672">-</span>Value(s))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> v</span></span></code></pre></div>
<p><strong>Hinweis I</strong>: Auf <a href="https://en.wikipedia.org/wiki/Minimax#Pseudocode" rel="external" target="_blank">wikipedia.org/wiki/Minimax</a>
finden Sie eine Variante mit einem zusätzlichen Tiefenparameter, um bei einer bestimmten
Suchtiefe abbrechen zu können. Dies ist bereits eine erweiterte Version, wo man beim
Abbruch durch das Erreichen der Suchtiefe statt <code>Utility()</code> eine <code>Eval()</code>-Funktion
braucht (vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html">Minimax: Heuristiken</a>).</p>
<p>Wenn man ohne Suchtiefenbeschränkung arbeiten will, braucht man diesen
Parameter nicht! Der Algorithmus terminiert auch ohne Suchtiefenbeschränkung!</p>
<p><strong>Hinweis II</strong>: Im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax.html#id_Russell2020">[Russell2020, S. 196, Abb. 6.3]</a> findet sich eine Variante, die die
auf der nächsten Folien gezeigte Startfunktion mit den hier gezeigten <code>Min-Value()</code>-
und <code>Max-Value()</code>-Funktionen verschmilzt. Dabei wird in den beiden Hilfsfunktionen
nicht nur der <code>min</code>- bzw. <code>max</code>-Wert optimiert, sondern auch der jeweils beste Zug
gespeichert und als Rückgabe zurückgeliefert.</p>
<h2 id="minimax-algorithmus-sonderbehandlung-startknoten">Minimax-Algorithmus: Sonderbehandlung Startknoten</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Minimax</span>(state):
</span></span><span style="display:flex;"><span>    (val, action) <span style="color:#f92672">=</span> (<span style="color:#f92672">-</span>INF, null)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (a, s) <span style="color:#f92672">in</span> Successors(state):
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> Min<span style="color:#f92672">-</span>Value(s)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (val <span style="color:#f92672">&lt;=</span> v):
</span></span><span style="display:flex;"><span>            (val, action) <span style="color:#f92672">=</span> (v, a)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> action</span></span></code></pre></div>
<ul>
<li>Startknoten ist ein MAX-Knoten</li>
<li>Nicht nur Kosten, sondern auch die zugehörige Aktion speichern</li>
</ul>
<h2 id="minimax-beispiel">Minimax Beispiel</h2>
<p><a href="#R-image-0f1d19ea338ff9e9405b2ee88cc96870" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax/minimaxBeispiel.png?width=50%25&height=auto" style=" height: auto; width: 50%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0f1d19ea338ff9e9405b2ee88cc96870"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax/minimaxBeispiel.png?width=50%25&height=auto"></a></p>
<h3 id="aufwand-minimax">Aufwand Minimax</h3>
<ul>
<li>maximale Tiefe des Spielbaums: <span class="math align-center">$m$</span></li>
<li>in jedem Zustand <span class="math align-center">$b$</span> gültige Züge</li>
<li>=&gt; Zeitkomplexität <span class="math align-center">$O(b^m)$</span></li>
</ul>
<p>Gedankenexperiment:</p>
<ul>
<li>erster Zug: <span class="math align-center">$b$</span> Möglichkeiten,</li>
<li>zweiter Zug: jeweils wieder <span class="math align-center">$b$</span> Möglichkeiten <span class="math align-center">$\rightarrow$</span> <span class="math align-center">$b \star b = b^2$</span>,</li>
<li>dritter Zug: jeweils wieder <span class="math align-center">$b$</span> Möglichkeiten <span class="math align-center">$\rightarrow$</span> <span class="math align-center">$b \star (b \star b) = b^3$</span>,</li>
<li>...,</li>
<li><span class="math align-center">$m$</span>. Zug: jeweils wieder <span class="math align-center">$b$</span> Möglichkeiten <span class="math align-center">$\rightarrow$</span> <span class="math align-center">$b^m$</span></li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Minimax: Entwickelt Spielbaum, bewertet Zustände entsprechend <code>Max</code> und <code>Min</code>
<ul>
<li>Gewinn von <code>Max</code>: +1, Gewinn von <code>Min</code>: -1</li>
<li><code>Max</code> wählt das Maximum der möglichen Züge von <code>Min</code></li>
<li><code>Min</code> wählt das Minimum der möglichen Züge von <code>Max</code></li>
<li>Spielbaum wird bis zu den Blättern entwickelt, Bewertung mit <code>Utility</code></li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106582&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Minimax (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Optimale Spiele und MiniMax</strong></p>
<p>Auf einem Tisch liegen nebeneinander 5 Streichhölzer. Es gibt zwei Spieler - Weiß und Schwarz - die abwechselnd ein oder zwei Streichhölzer wegnehmen dürfen (es muss mind. ein Streichholz genommen werden). Wer das letzte Streichholz nehmen muss, hat verloren. Zu Beginn ist Weiß am Zug.</p>
<ol>
<li>Spielbaum</li>
</ol>
<p>Zeichnen Sie den <strong>kompletten</strong> Spielbaum auf. Geben Sie an den Kanten jeweils die Zahl der genommenen und der verbleibenden Hölzer an.</p>
<p><em>Beispiel</em>: Wenn in einem Zug ein Holz genommen wird und 3 Hölzer verbleiben, steht an der entsprechenden Kante &quot;1/3&quot;. Geben Sie jeweils an, welcher Spieler am Zug ist.</p>
<ol start="2">
<li>Minimax</li>
</ol>
<p>Geben Sie die Bewertung aller Spielzustände mit Hilfe des Minimax-Algorithmus an. Bewerten Sie die Endzustände mit +1, wenn Spieler Weiß gewonnen hat, mit -1, falls Schwarz gewonnen hat.</p>
<ol start="3">
<li>Optimaler Zug</li>
</ol>
<p>Mit welchem Zug muss Weiß beginnen, um das Spiel garantiert zu gewinnen (beide Spieler verhalten sich stets optimal)? Argumentieren Sie mit der Minimax-Bewertung.</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Spiele</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Minimax: Abschnitt 6.2</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Heuristiken</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Minimax entwickelt den gesamten Spielbaum. Wenn nicht genug Zeit dafür zur Verfügung steht, kann man die
Suchtiefe begrenzen. Für die Bewertung der Zustände benötigt man eine <code>Eval</code>-Funktion, die die Knoten in
der selben Reihenfolge sortieren sollte wie es in der vollständigen Version über die <code>Utility</code>-Funktion
geschieht. Die <code>Eval</code>-Funktion sollte zudem schnell zu berechnen sein. Typische Varianten für die
<code>Eval</code>-Funktion sind gewichtete Features oder ein Nachschlagen in Spieldatenbanken (Spielzustand plus
Bewertung).</p>
<p>Minimax kann auf Spiele mit mehr als zwei Spielern erweitert werden. Dabei versucht dann jeder Spieler für
sich, das Ergebnis des Spiels (aus seiner Sicht) zu maximieren.</p>
<p>Bei Spielen mit Zufall (Würfelereignisse) kann man jedem Würfelereignis eine Wahrscheinlichkeit zuordnen
und damit den jeweils erreichbaren <code>Max</code>- oder <code>Min</code>-Wert gewichten. Die Summe dieser gewichteten Bewertungen
ist die Bewertung des entsprechenden &quot;Chance&quot;-Knotens, der dann in der darüberliegenden Ebene nach dem
Minimax-Prinzip ausgewertet wird (=&gt; <em>Expectimax</em>).</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/rKqNqYBXuK8' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Heuristiken</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/e5d279fef94d9a37e3b5d15fe9f807e024152e4c65a5a1110bab7871aff45828dba25d086e6a24f6a3a14111304b15f31c9844ff04473788595054d406790a59' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Heuristiken</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Minimax für mehr als zwei Spieler</li> <li>(K2) Minimax mit Zufallskomponente</li> <li>(K2) Optimierungsmöglichkeit: Sortierung der Nachfolger =&gt; Heuristik</li> <li>(K2) Optimierungsmöglichkeit: Suchtiefe beschränken =&gt; Übergang zu Bewertungsfunktion</li> <li>(K2) Optimierungsmöglichkeit: Bewertung über Spieldatenbanken</li> <li>(K3) Minimax-Algorithmus</li> <li>(K3) Tiefenbeschränkung und Bewertungsfunktion bei Minimax</li></ul>
  </div>
</div>




    <h2 id="wenn-die-zeit-nicht-reicht-suchtiefe-begrenzen">Wenn die Zeit nicht reicht: Suchtiefe begrenzen</h2>
<ul>
<li>
<p>Einführung neuer Funktionen:</p>
<ol>
<li>
<p><code>Cutoff-Test</code> statt <code>Terminal-Test</code></p>
<p>Beispielsweise bei erreichter Tiefe oder Zeitüberschreitung</p>
</li>
<li>
<p><code>Eval</code> statt <code>Utility</code></p>
<p>Bewertung der erreichten Position (statt nur Bewertung des Endzustandes)</p>
</li>
</ol>
</li>
<li>
<p>Bedingungen an <code>Eval</code>:</p>
<ol>
<li>Endknoten in selber Reihenfolge wie bei <code>Utility</code></li>
<li>Schnell zu berechnen (!)</li>
</ol>
</li>
</ul>
<h2 id="beispiel-schach">Beispiel Schach</h2>
<ul>
<li>
<p>Mögliche Evaluierungskriterien:</p>
<ul>
<li>Materialwert: Bauer 1, Läufer/Springer 3, Turm 5, Dame 9</li>
<li>Stellungsbewertung: Sicherheit des Königs, Stellung der Bauern</li>
<li>Daumenregeln: 3 Punkte Vorteil =&gt; sicherer Sieg</li>
</ul>
</li>
<li>
<p>Nutzung gewichteter Features
<span class="math align-center">$f_i$</span>: <span class="math align-center">$\operatorname{Eval}(s) = w_1f_1(s) + w_2f_2(s) + \ldots$</span></p>
<ul>
<li>Beispiel: <span class="math align-center">$w_1 = 9$</span> und <span class="math align-center">$f_1(s)$</span> = (# weiße Königinnen) - (# schwarze Königinnen)</li>
</ul>
</li>
<li>
<p><strong>Alternativ</strong>: Speicherung von Positionen plus Bewertung in Datenbanken
=&gt; Lookup mit <span class="math align-center">$\operatorname{Eval}(s)$</span> (statt Berechnung zur Laufzeit)</p>
</li>
</ul>
<h2 id="minimax-mit-mehreren-spielern">Minimax mit mehreren Spielern</h2>
<p><a href="#R-image-01f6aa61d52ce7539b4009cf72bf4706" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics/minimax3.png?width=50%25&height=auto" style=" height: auto; width: 50%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-01f6aa61d52ce7539b4009cf72bf4706"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics/minimax3.png?width=50%25&height=auto"></a></p>
<p>Hier maximiert jeder Spieler sein eigenes Ergebnis. Im Grunde müsste diese
Variante dann besser &quot;Maximax&quot; heissen ...</p>
<p>Wenn es an einer Stelle im Suchbaum mehrere gleich gute (beste) Züge geben
sollte, kann der Spieler Allianzen bilden: Er könnte dann einen Zug auswählen,
der für einen der Mitspieler günstiger ist.</p>
<h2 id="zufallsspiele">Zufallsspiele</h2>
<p><a href="#R-image-5d75be5bb0bb13b68e21e498af1c49ec" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://live.staticflickr.com/3670/11267311625_e4758ff425_o_d.jpg?width=60%25&height=auto" style=" height: auto; width: 60%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-5d75be5bb0bb13b68e21e498af1c49ec"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://live.staticflickr.com/3670/11267311625_e4758ff425_o_d.jpg?width=60%25&height=auto"></a></p>
<p><span class='origin'>Quelle: <a href="https://www.flickr.com/photos/83436399@N04/11267311625" rel="external" target="_blank">&quot;position-backgammon-decembre&quot;</a> by <a href="https://www.flickr.com/photos/83436399@N04" rel="external" target="_blank">serialgamer_fr</a> on Flickr.com (<a href="https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=rich" rel="external" target="_blank">CC BY 2.0</a>)</span></p>
<p>Backgammon: Was ist in dieser Situation der optimale Zug?</p>
<h2 id="minimax-mit-zufallsspielen-zufalls-knoten">Minimax mit Zufallsspielen: ZUFALLS-Knoten</h2>
<p><a href="#R-image-f7b43d205e65c9415f63c087a4e816c3" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics/expectimax.png?width=50%25&height=auto" style=" height: auto; width: 50%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f7b43d205e65c9415f63c087a4e816c3"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics/expectimax.png?width=50%25&height=auto"></a></p>
<p>Zusätzlich zu den MIN- und MAX-Knoten führt man noch Zufalls-Knoten ein, um
das Würfelergebnis repräsentieren zu können. Je möglichem Würfelergebnis <span class="math align-center">$i$</span>
gibt es einen Ausgang, an dem die Wahrscheinlichkeit <span class="math align-center">$P(i)$</span> dieses Ausgangs
annotiert wird.</p>
<p>=&gt; Für Zufallsknoten <strong>erwarteten</strong> Minimax-Wert (<em>Expectimax</em>) nutzen</p>
<h2 id="minimax-mit-zufall-expectimax">Minimax mit Zufall: Expectimax</h2>
<p>Expectimax-Wert für Zufallsknoten <span class="math align-center">$C$</span>:</p>
<span class="math align-center">$$
    \operatorname{Expectimax}(C) = \sum_i P(i) \operatorname{Expectimax}(s_i)
$$</span>
<ul>
<li><span class="math align-center">$i$</span> mögliches Würfelergebnis</li>
<li><span class="math align-center">$P(i)$</span> Wahrscheinlichkeit für Würfelergebnis</li>
<li><span class="math align-center">$s_i$</span> Nachfolgezustand von <span class="math align-center">$C$</span> gegeben Würfelergebnis <span class="math align-center">$i$</span></li>
</ul>
<p>Für die normalen Min- und Max-Knoten liefert <code>Expectimax()</code> die üblichen
Aufrufe von <code>Min-Value()</code> bwz. <code>Max-Value()</code>.</p>
<p>Auf <a href="https://en.wikipedia.org/wiki/Expectiminimax" rel="external" target="_blank">wikipedia.org/wiki/Expectiminimax</a>
finden Sie eine Variante mit einem zusätzlichen Tiefenparameter, um bei einer bestimmten
Suchtiefe abbrechen zu können. Dies ist bereits eine erweiterte Version, wo man beim
Abbruch durch das Erreichen der Suchtiefe statt <code>Utility()</code> eine <code>Eval()</code>-Funktion
braucht. Zusätzlich kombiniert der dort gezeigte Algorithmus die Funktionen
<code>Expectimax()</code>, <code>Min-Value()</code> und <code>Max-Value()</code> in eine einzige Funktion.</p>
<p>Eine ähnliche geschlossene Darstellung finden Sie im <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html#id_Russell2020">[Russell2020, S. 212]</a>.</p>
<p><strong>Hinweis</strong>: Üblicherweise sind die Nachfolger der Zufallsknoten gleich wahrscheinlich.
Dann kann man einfach mit dem Mittelwert der Bewertung der Nachfolger arbeiten.</p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Minimax:
<ul>
<li>Kriterien zur Begrenzung der Suchtiefe, Bewertung <code>Eval</code> statt <code>Utility</code></li>
<li>Erweiterung auf <span class="math align-center">$>2$</span> Spieler</li>
<li>Erweiterung auf Spiele mit Zufall: <em>Expectimax</em></li>
</ul>
</li>
</ul>


    



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Spiele</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Erweiterungen und Heuristiken: Abschnitte 6.2.2, 6.3, 6.5</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Alpha-Beta-Pruning</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Minimax entwickelt den gesamten Spielbaum. Wenn man dabei die bisher besten Werte für MAX und MIN als
<span class="math align-center">$\alpha$</span> und <span class="math align-center">$\beta$</span> mitführt, beobachtet man, dass ein <span class="math align-center">$\alpha$</span>-Wert nie kleiner wird und ein <span class="math align-center">$\beta$</span>-Wert
nie größer wird. Dies kann man ausnutzen und das Entwickeln von Pfaden abbrechen, wenn in einem MIN-Knoten
der <span class="math align-center">$\beta$</span>-Wert kleiner wird als der <span class="math align-center">$\alpha$</span>-Wert des MAX-Vorgängers: (a) kann der <span class="math align-center">$\beta$</span>-Wert bei der
weiteren Untersuchung der verbleibenden Nachfolger im MIN-Knoten nur noch kleiner werden, und (b) würde der
MAX-Vorgänger diesen MIN-Knoten nie als Nachfolger in Betracht ziehen, da er bereits einen besseren Zug
gesehen hat (da sein <span class="math align-center">$\alpha$</span> größer ist als das <span class="math align-center">$\beta$</span> im Nachfolger). Deshalb kann man hier sofort die
Untersuchung der verbleibenden Nachfolger im MIN-Knoten abbrechen (&quot;Pruning&quot;). Eine analoge Überlegung gilt
für einen MAX-Nachfolger unter einem MIN-Knoten.</p>
<p>Dabei bleibt das Endergebnis erhalten. Man schneidet nur Suchpfade weg, die das Ergebnis von Minimax nicht
verändern.</p>
<p>Die mögliche Effizienzsteigerung ist sehr stark abhängig von Sortierung der Nachfolger! Deshalb stellt man
häufig Heuristiken zur &quot;richtigen&quot; Sortierung der Nachfolger auf (&quot;Killer-Moves&quot;).</p>
<p>Zusätzlich kann man wie bei Minimax auch die Suchtiefe begrenzen und eine Bewertungsfunktion (statt der
Nutzenfunktion) einsetzen. Auch hier kann die Bewertungsfunktion wieder gewichtete Features nutzen und/oder
Positionen mit in Datenbanken gespeicherten Positionen und Bewertungen abgleichen.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/_Hq-GCl__bU' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Alpha-Beta-Pruning</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/f7c9b90ff9732933c8899548d0f89eb320b59efeee015383a765494b60b29578aae796e92bf7df5a621ed1f8a6dc50649e3820fcc74c698cd3f25e6987882a53' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Alpha-Beta-Pruning</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Optimierungsmöglichkeit: Sortierung der Nachfolger =&gt; Heuristik</li> <li>(K2) Optimierungsmöglichkeit: Suchtiefe beschränken =&gt; Übergang zu Bewertungsfunktion</li> <li>(K2) Optimierungsmöglichkeit: Bewertung über Spieldatenbanken</li> <li>(K3) alpha-beta-Pruning</li></ul>
  </div>
</div>




    <h2 id="verbesserung-minimax-algorithmus">Verbesserung Minimax-Algorithmus</h2>
<p><a href="#R-image-91b33b1324aa548253804005ea9c8e83" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta/minimax.png?width=40%25&height=auto" style=" height: auto; width: 40%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-91b33b1324aa548253804005ea9c8e83"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta/minimax.png?width=40%25&height=auto"></a></p>
<p>=&gt; <strong>Minimax-Baum</strong>: <span class='alert'>Verbesserungen möglich?</span></p>
<h2 id="alpha-beta-pruning">Alpha-beta-Pruning</h2>
<p>Minimax-Algorithmus mit zusätzlichen Informationen:</p>
<ul>
<li><span class="math align-center">$\alpha$</span>: bisher bester Wert für MAX (höchster Wert)</li>
<li><span class="math align-center">$\beta$</span>: bisher bester Wert für MIN (kleinster Wert)</li>
</ul>
<p>=&gt; Beobachtungen:</p>
<ol>
<li><span class="math align-center">$\alpha$</span> für MAX-Knoten wird nie kleiner</li>
<li><span class="math align-center">$\beta$</span> für MIN-Knoten wird nie größer</li>
</ol>
<h2 id="pruning-regeln">Pruning-Regeln</h2>
<ol>
<li>
<p>Schneide (unter) MIN-Knoten ab, deren <span class="math align-center">$\beta$</span> <span class="math align-center">$\le$</span> dem
<span class="math align-center">$\alpha$</span> des MAX-Vorgängers ist.</p>
</li>
<li>
<p>Schneide (unter) MAX-Knoten ab, deren <span class="math align-center">$\alpha$</span> <span class="math align-center">$\ge$</span> dem
<span class="math align-center">$\beta$</span> des MIN-Vorgängers ist.</p>
</li>
</ol>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p><span class='alert'>Abbruch, wenn kein Platz mehr zwischen Alpha und Beta</span></p>
</span></span>
</div>
<h2 id="alpha-beta-pruning----der-algorithmus-skizze">Alpha-beta-Pruning -- Der Algorithmus (Skizze)</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Max</span><span style="color:#f92672">-</span>Value(state, alpha, beta):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> Terminal<span style="color:#f92672">-</span>Test(state): <span style="color:#66d9ef">return</span> Utility(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>INF
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (a, s) <span style="color:#f92672">in</span> Successors(state):
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> MAX(v, Min<span style="color:#f92672">-</span>Value(s, alpha, beta))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (v <span style="color:#f92672">&gt;=</span> beta): <span style="color:#66d9ef">return</span> v
</span></span><span style="display:flex;"><span>        alpha <span style="color:#f92672">=</span> MAX(alpha, v)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> v</span></span></code></pre></div>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Min</span><span style="color:#f92672">-</span>Value(state, alpha, beta):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> Terminal<span style="color:#f92672">-</span>Test(state): <span style="color:#66d9ef">return</span> Utility(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span> <span style="color:#f92672">+</span>INF
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (a, s) <span style="color:#f92672">in</span> Successors(state):
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> MIN(v, Max<span style="color:#f92672">-</span>Value(s, alpha, beta))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (v <span style="color:#f92672">&lt;=</span> alpha): <span style="color:#66d9ef">return</span> v
</span></span><span style="display:flex;"><span>        beta <span style="color:#f92672">=</span> MIN(beta, v)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> v</span></span></code></pre></div>
<p>Initialer Aufruf von <code>Max-Value()</code> mit <span class="math align-center">$\alpha = -\infty$</span> und <span class="math align-center">$\beta = +\infty$</span></p>
<p><strong>Achtung</strong>: Es kursieren Varianten von diesem Algorithmus, bei denen auf die
Hilfsvariable <code>v</code> verzichtet wird und stattdessen <code>alpha</code> bzw. <code>beta</code> direkt
modifiziert werden und als Rückgabewert dienen. Das <em>kann</em> zu anderen oder falschen
Ergebnissen führen! Sie können das in der Aufgabe auf Blatt 03 gut beobachten.</p>
<h2 id="alpha-beta-pruning----eigenschaften">Alpha-beta-Pruning -- Eigenschaften</h2>
<ol>
<li>
<p>Pruning beeinflusst nicht das Endergebnis!</p>
</li>
<li>
<p>Sortierung der Nachfolger spielt große Rolle</p>
</li>
<li>
<p>Perfekte Sortierung: <span class="math align-center">$O(b^{d/2})$</span> =&gt; Verdopplung der Suchtiefe möglich</p>
</li>
</ol>
<p>Für Schach immer noch zu aufwändig ...</p>
<h2 id="verbesserungen-für-alpha-beta-pruning">Verbesserungen für Alpha-beta-Pruning</h2>
<ul>
<li>
<p>&quot;Killer-Move&quot;: Maximale Effizienz nur wenn <strong>optimaler Zug immer zuerst</strong> untersucht
=&gt; Zu untersuchende Züge <strong>sortieren/priorisieren</strong>, zb. Schach:
a)  Figuren schlagen
b)  Drohen
c)  Vorwärts ziehen
d)  Rückwärts ziehen</p>
</li>
<li>
<p>Verändern der Suchtiefe nach Spielsituation</p>
</li>
<li>
<p>Bewertungsfunktion <code>Eval</code>:</p>
<ul>
<li>Datenbanken mit Spielsituationen und Expertenbewertung:
<ul>
<li>Eröffnungsspiele (besonders viele Verzweigungen)</li>
<li>Endspiele</li>
</ul>
</li>
<li>Lernen der optimalen Gewichte für <code>Eval</code>-Funktion</li>
<li>Berücksichtigung von Symmetrien</li>
</ul>
</li>
</ul>
<h2 id="beispiel-deepblue-ibm-1997">Beispiel DeepBlue (IBM, 1997)</h2>
<ul>
<li>Alpha-beta-Pruning mit Tiefenbeschränkung: ca. 14 Halbzüge</li>
<li>Dynamische Tiefenbeschränkung (stellungsabhängig, max. ca. 40 Züge)</li>
<li>Heuristische Stellungsbewertung <code>Eval</code>:
<ul>
<li>mehr als 8.000 Features</li>
<li>ca. 4.000 Eröffnungsstellungen</li>
<li>ca. 700.000 Spielsituationen (von Experten bewertet)</li>
<li>Endspiel-Datenbank: alle Spiele mit 5 Steinen, viele mit 6 Steinen</li>
</ul>
</li>
</ul>
<p><span class='origin'>Quelle: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html#id_Russell2014">[Russell2014, p. 185]</a></span></p>
<h2 id="beispiel-alphago-google-2016">Beispiel AlphaGo (Google, 2016)</h2>
<ul>
<li>
<p>Beschränkung der Suchtiefe: Bewertung der Stellung durch <em>&quot;Value Network&quot;</em></p>
</li>
<li>
<p>Beschränkung der Verzweigungsbreite: Bestimmung von Zugkandidaten durch
<em>&quot;Policy Network&quot;</em></p>
</li>
<li>
<p>Training dieser <em>&quot;Deep Neural Networks&quot;</em>:</p>
<ul>
<li>Überwachtes Lernen: &quot;Analyse&quot; von Spiel-Datenbanken</li>
<li>Reinforcement-Lernen: Self-Play, Bewertung am Ende
<ul>
<li>Züge mit Monte-Carlo-Baumsuche ausgewählt</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span class='origin'>Quelle: <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html#id_Silver2016">[Silver2016]</a>, siehe auch <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="external" target="_blank">deepmind.com/research/alphago/</a></span></p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>
<p>Alpha-beta-Pruning:</p>
<ul>
<li>Mitführen der bisher besten Werte für MAX und MIN: <span class="math align-center">$\alpha$</span> und <span class="math align-center">$\beta$</span></li>
<li>Abschneiden von Pfaden, die Verschlechterung bewirken würden</li>
<li>Endergebnis bleibt erhalten</li>
<li>Effizienzsteigerung abhängig von Sortierung der Nachfolger</li>
</ul>
</li>
<li>
<p>Viele Verbesserungen denkbar:</p>
<ul>
<li>Zu untersuchende Züge &quot;richtig&quot; sortieren (Heuristik)</li>
<li>Suchtiefe begrenzen und Bewertungsfunktion (statt Nutzenfunktion)</li>
<li>Positionen mit Datenbank abgleichen</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106584&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Alpha-Beta-Pruning (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Optimale Spiele und MiniMax</strong></p>
<p>Auf einem Tisch liegen nebeneinander 5 Streichhölzer. Es gibt zwei Spieler - Weiß und Schwarz - die abwechselnd ein oder zwei Streichhölzer wegnehmen dürfen (es muss mind. ein Streichholz genommen werden). Wer das letzte Streichholz nehmen muss, hat verloren. Zu Beginn ist Weiß am Zug.</p>
<ol>
<li>Spielbaum</li>
</ol>
<p>Zeichnen Sie den <strong>kompletten</strong> Spielbaum auf. Geben Sie an den Kanten jeweils die Zahl der genommenen und der verbleibenden Hölzer an.</p>
<p><em>Beispiel</em>: Wenn in einem Zug ein Holz genommen wird und 3 Hölzer verbleiben, steht an der entsprechenden Kante &quot;1/3&quot;. Geben Sie jeweils an, welcher Spieler am Zug ist.</p>
<ol start="2">
<li>Minimax</li>
</ol>
<p>Geben Sie die Bewertung aller Spielzustände mit Hilfe des Minimax-Algorithmus an. Bewerten Sie die Endzustände mit +1, wenn Spieler Weiß gewonnen hat, mit -1, falls Schwarz gewonnen hat.</p>
<ol start="3">
<li>Alpha-Beta-Pruning</li>
</ol>
<p>Wenden Sie Alpha-Beta-Pruning auf den Spielbaum an. Werden damit mehr oder weniger oder gleich viele Spielzüge wie mit Minimax entwickelt? Begründen Sie Ihre Antwort.</p>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Spiele</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
                    
                
            
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.</li> <li id='id_Russell2014'>[Russell2014] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2014. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-1-292-02420-2' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-1-292-02420-2</a>.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Alpha-beta-Pruning: Abschnitt 6.2.3, Erweiterungen: Abschnitt 6.3</em></li> <li id='id_Silver2016'>[Silver2016] <a href='https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Mastering the game of Go with deep neural networks and tree search</strong></a><br>Silver, D. und Huang, A. und Maddison, C. und Guez, A. und Sifre, L. und Van Den Driessche, G. und Schrittwieser, I., J. Schrittwieser und Panneershelvam, V. und Lanctot, M. und others, Nature Publishing Group, 2016. DOI <a href='https://doi.org/10.1038/nature16961' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1038/nature16961</a>.</li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="constraintsolving">Constraintsolving</h1>

<p>Was haben <a href="https://en.wikipedia.org/wiki/Type_inference" rel="external" target="_blank">Typ-Inferenz</a>,
<a href="https://en.wikipedia.org/wiki/Sudoku" rel="external" target="_blank">Sudoku</a>, das
<a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle" rel="external" target="_blank">8-Queens-Problem</a>, das
<a href="https://en.wikipedia.org/wiki/Graph_coloring" rel="external" target="_blank">Einfärben von Landkarten</a> und
das <a href="https://en.wikipedia.org/wiki/Resource_allocation" rel="external" target="_blank">Erstellen von Stundenplänen</a>
gemeinsam?</p>
<p>Es handelt sich um eine bestimmte Art von Suchproblemen, wobei den Parametern
(Variablen) Werte so zugewiesen werden müssen, dass Einschränkungen bzw.
Relationen zwischen den Variablen gelten.</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro.html">Einführung Constraints</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch.html">Lösen von diskreten CSP</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html">Heuristiken</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html">Kantenkonsistenz und AC-3</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Constraintsolving</h1>
<article class="default">
<h1>Einführung Constraints</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Ein Constraintproblem (CSP) besteht aus Variablen, die über Einschränkungen (&quot;Constraints&quot;)
verbunden sind. Jeder Variable wird eine Domäne (Wertebereich) zugeordnet.</p>
<p>Die Constraints können sich auf eine einzelne Variable beziehen (&quot;unäre Constraints&quot;),
auf zwei Variablen (&quot;binäre Constraints&quot;) oder auf mehr Variablen. Ein CSP kann als Graph
dargestellt werden.</p>
<p>Eine &quot;Belegung&quot; ist eine Zuweisung von Werten an Variablen aus deren Domäne. Eine
&quot;konsistente&quot; Belegung erfüllt die Constraints, eine &quot;vollständige&quot; Belegung belegt
alle Variablen.</p>
<p>Eine Lösung für ein CSP ist eine vollständige und konsistente Belegung.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/eFyo4Xh59ns' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Intro CSP</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/a269f53bff001a84bec4435261543b846ccca4290f211cf1634f9007d1285513c8bae4f89a0225a489e1ffda6cac455e264e022c35f46e8ffe80b4ddcd86d137' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Intro CSP</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K1) Definitionen: Variable, Domäne, Constraint, Arität, CSP, Zuweisung</li> <li>(K3) Formulierung von CSP</li></ul>
  </div>
</div>




    <h2 id="motivation-einfärben-von-landkarten">Motivation: Einfärben von Landkarten</h2>
<p><a href="#R-image-567aec55a1565ed4ad6cd3dd9bd0199d" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro/map.png?width=50%25&height=auto" style=" height: auto; width: 50%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-567aec55a1565ed4ad6cd3dd9bd0199d"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro/map.png?width=50%25&height=auto"></a></p>
<p>Die Skizze soll eine Landkarte mit verschiedenen Ländern darstellen. Die Aufgabe
lautet: Färbe jedes Land mit einer Farbe ein, um die Übersichtlichkeit zu
erhöhen. Verwende dabei so wenig wie möglich unterschiedliche Farben. Aneinander
grenzende Länder müssen unterschiedliche Farben bekommen (=&gt; <em>Constraint</em>).</p>
<h2 id="einfärben-von-landkarten-formalisierung">Einfärben von Landkarten: Formalisierung</h2>
<ul>
<li>
<p><strong>Variablen</strong>: A, B, C, D, E, F</p>
</li>
<li>
<p><strong>Werte</strong>: <span class="math align-center">$\lbrace red, green, blue \rbrace$</span></p>
</li>
<li>
<p><strong>Constraints</strong>: Benachbarte Regionen müssen unterschiedliche Farben haben</p>
</li>
<li>
<p><strong>Mögliche Lösung</strong>: Zuweisung an Variablen (&quot;Belegung&quot;)
<span class="math align-center">$\lbrace \operatorname{A} = red, \operatorname{B} = blue, \operatorname{C} = green,
    \operatorname{D} = red, \operatorname{E} = blue, \operatorname{F} = blue \rbrace$</span></p>
</li>
</ul>
<h2 id="definition-constraint-satisfaction-problem-csp">Definition: Constraint Satisfaction Problem (CSP)</h2>
<ul>
<li>
<p>Ein CSP <span class="math align-center">$\langle V, D, C \rangle$</span> besteht aus:</p>
<ul>
<li>Menge von <strong>Variablen</strong> <span class="math align-center">$V = \lbrace V_1, V_2, \ldots, V_n \rbrace$</span></li>
<li>Je <span class="math align-center">$V_i$</span> nicht leere <strong>Domäne</strong> <span class="math align-center">$D_i = \lbrace d_{i,1}, d_{i,2}, \ldots, d_{i,m_i} \rbrace$</span></li>
<li>Menge von <strong>Constraints</strong> <span class="math align-center">$C = \lbrace C_1, C_2, \ldots, C_p \rbrace$</span>
(Randbedingungen, Abhängigkeiten zwischen Variablen)</li>
</ul>
</li>
<li>
<p>Zuweisung/Belegung (<em>Assignment</em>) <span class="math align-center">$\alpha$</span>:</p>
<ul>
<li>Zuweisung von Werten an (einige/alle) Variablen:
<span class="math align-center">$\alpha = \lbrace X=a, Y=b, \ldots \rbrace$</span>
(aus den jeweiligen Wertebereichen)</li>
<li><strong>Konsistente Belegung</strong>: Randbedingungen sind nicht verletzt</li>
<li><strong>Vollständige Belegung</strong>: Alle Variablen sind belegt</li>
</ul>
</li>
<li>
<p><strong>Lösung</strong> eines CSP: Vollständige und konsistente Belegung</p>
</li>
</ul>
<h2 id="constraint-graph">Constraint-Graph</h2>
<p><a href="#R-image-7610e75052c856f7f53a14adc693a98e" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro/map_graph.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7610e75052c856f7f53a14adc693a98e"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro/map_graph.png?width=80%25&height=auto"></a></p>
<p>Ein CSP kann man auch als Constraint-Graph darstellen. Die Variablen werden zu Knoten im
Graph, die Constraints zu Kanten zwischen den Knoten. Dadurch kann man die aus dem Problemlösen
bekannten Algorithmen anwenden ...</p>
<h2 id="constraints----arität">Constraints -- Arität</h2>
<p>Die <em>Arität</em> betrifft hier die &quot;Stelligkeit&quot;: Wie viele Variablen stehen in
einem Constraint miteinander in Beziehung? (Also wie viele Parameter hat
ein Constraint?)</p>
<ul>
<li>
<p><strong>unär</strong>: betrifft einzelne Variablen
Beispiel: <span class="math align-center">$\operatorname{A} \neq red$</span></p>
</li>
<li>
<p><strong>binär</strong>: betrifft Paare von Variablen
Beispiel: <span class="math align-center">$\operatorname{A} \neq \operatorname{B}$</span></p>
</li>
<li>
<p><strong>höhere Ordnung</strong>: betrifft 3 oder mehr Variablen</p>
</li>
<li>
<p><strong>Präferenzen</strong>: &quot;soft constraints&quot;
Beispiel: &quot;rot ist besser als grün&quot;</p>
<p>Abbildung über Gewichtung =&gt; Constraint-Optimierungsproblem (COP)</p>
</li>
</ul>
<h2 id="constraints----wertebereiche">Constraints -- Wertebereiche</h2>
<ul>
<li>
<p><strong>Endliche Domänen</strong>: <span class="math align-center">$d$</span> Werte =&gt; <span class="math align-center">$O(d^n)$</span> mögliche Zuweisungen
(exponentiell in der Zahl der Variablen)</p>
</li>
<li>
<p><strong>Unendliche Domänen</strong>: reelle Zahlen, natürliche Zahlen
=&gt; Keine Auflistung der erlaubten Wertekombinationen mehr möglich
=&gt; Übergang zu Gleichungen/Ungleichungen: <span class="math align-center">$job_1+5<job_2$</span></p>
<ul>
<li>lineare Constraints</li>
<li>nichtlineare Constraints</li>
</ul>
</li>
</ul>
<p><strong>Historische Unterscheidung</strong>:</p>
<ul>
<li><strong>Constraint Satisfaction</strong>: endliche Domänen, kombinatorische Methoden</li>
<li><strong>Constraint Solving</strong>: unendliche Domänen</li>
</ul>
<h2 id="csp-sind-überall-">CSP sind überall ...</h2>
<ul>
<li>Stundenpläne (Klassen, Räume, Zeiten)</li>
<li>Konfiguration (Computer, Autos, ...)</li>
<li>Fahrpläne (Zug, Flug, ...)</li>
<li>Planung von komplexen Projekten</li>
<li>Sudoku :-)</li>
<li>...</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Definitionen und Begriffe:
<ul>
<li>Variable, (un-) endliche Domänen, Wertemenge</li>
<li>Constraint, Arität, CSP</li>
<li>Zuweisung, Lösung, ...</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106572&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Intro CSP (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Constraints</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Bartak2001'>[Bartak2001] <strong>Theory and Practice of Constraint Propagation</strong><br>Barták, R., 2001.</li> <li id='id_Kumar1992'>[Kumar1992] <strong>Algorithms for Constraint Satisfaction Problems: A Survey</strong><br>Kumar, V., 1992.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>CSP: Abschnitt 5.1</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Lösen von diskreten CSP</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>
CSP's mit endlichen Domänen lassen sich mit einer Backtracking-Suche lösen. Dabei wird
schrittweise eine Variablen ausgewählt und dann ein Wert aus deren Wertebereich für
die Belegung ausgewählt. Danach ruft sich die Backtracking-Suche rekursiv auf. Falls
dabei keine Lösung gefunden werden kann, erfolgt Backtracking und die Belegung wird
schließlich rückgängig gemacht und durch die nächste Möglichkeit ersetzt.
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/DIY7F2ycyqA' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL BT-Suche für CSP</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/34f066985dfe7258c153bce523c3f876b94959c8a31bbf316d7c57d4253d45f9fbde7045b99b1ec25d9e459eae84124405d76cc5cd3a32bd6f7c14206651816e' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL BT-Suche für CSP</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Lösung von CSP mit endlichen Domänen mit Hilfe der BT-Suche</li></ul>
  </div>
</div>




    <h2 id="einfärben-von-landkarten-als-csp">Einfärben von Landkarten als CSP</h2>
<p><a href="#R-image-a6d8d09c252258d9d9ba9a6cf4f41bf3" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch/map_graph.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a6d8d09c252258d9d9ba9a6cf4f41bf3"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch/map_graph.png?width=80%25&height=auto"></a></p>
<h2 id="endliche-domänen-formulierung-als-suchproblem">Endliche Domänen: Formulierung als Suchproblem</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">BT_Search</span>(assignment, csp):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> complete(assignment): <span style="color:#66d9ef">return</span> assignment
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    var <span style="color:#f92672">=</span> VARIABLES(csp, assignment)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> value <span style="color:#f92672">in</span> VALUES(csp, var):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> consistent(value, var, assignment, csp):
</span></span><span style="display:flex;"><span>            assignment <span style="color:#f92672">+=</span> {var <span style="color:#f92672">=</span> value}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> INFERENCE(csp, assignment, var) <span style="color:#f92672">!=</span> failure:
</span></span><span style="display:flex;"><span>                result <span style="color:#f92672">=</span> BT_Search(assignment, csp)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> result <span style="color:#f92672">!=</span> failure: <span style="color:#66d9ef">return</span> result
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            assignment <span style="color:#f92672">-=</span> {var <span style="color:#f92672">=</span> value}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> failure</span></span></code></pre></div>
<p><span class='origin'>Quelle: Eigener Code basierend auf einer Idee nach <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch.html#id_Russell2020">[Russell2020, p. 176, fig. 5.5]</a></span></p>
<p>Hierbei handelt es sich um eine etwas angepasste Tiefensuche: Starte mit leerem
Assignment und weise schrittweise Variablen passende Werte zu und mache notfalls
Backtracking.</p>
<h2 id="bt-suche-für-csp-am-beispiel-landkartenfärbeproblem">BT-Suche für CSP am Beispiel Landkartenfärbeproblem</h2>
<p><a href="#R-image-fdff9cc498f2c22a523b0d206440480a" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch/map_progress.png?width=80%25&height=auto" style=" height: auto; width: 80%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fdff9cc498f2c22a523b0d206440480a"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch/map_progress.png?width=80%25&height=auto"></a></p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Lösung von CSP mit endlichen Domänen mit Hilfe der Backtracking-Suche</li>
</ul>


    



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Constraints</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Bartak2001'>[Bartak2001] <strong>Theory and Practice of Constraint Propagation</strong><br>Barták, R., 2001.</li> <li id='id_Kumar1992'>[Kumar1992] <strong>Algorithms for Constraint Satisfaction Problems: A Survey</strong><br>Kumar, V., 1992.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>CSP, Backtracking: Abschnitt 5.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Heuristiken</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>CSP's mit endlichen Domänen lassen sich mit einer Backtracking-Suche lösen. Dabei
gibt es einige Freiheitsgrade: Auswahl der nächsten Variable und Wahl des nächsten
Werts. Hier können Heuristiken die Suche beschleunigen.</p>
<p>Zur Wahl der als nächstes zu betrachtenden Variable kann die <strong>Minimum Remaining
Values (MRV)</strong>-Heuristik eingesetzt werden: Wähle die Variable mit wenigsten freien
Werten. Bei Gleichstand bei der MRV kann man mit der <strong>Gradheuristik</strong> die Variable
mit den meisten Constraints zu offenen (noch nicht belegten) Variablen wählen.</p>
<p>Bei der Wahl des Wertes kann die <strong>Least Constraining Value (LCV)</strong>-Heuristik genutzt
werden: Wähle den Wert, der für die verbleibenden Variablen die wenigsten Werte ungültig
macht.</p>
<p>Während die MRV relativ leicht umzusetzen ist, muss man für die LCV alle Constraints zu
den Nachbarn auswerten.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/pgXf0oV8lhE' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CSP, Heuristiken</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/66689545e201ad90f6f2007f472f3b430ec37ebaa5321315764ae687983bbcb049bc217a1b0852e2d5364eae1223153d567558533246bd58b0db5cc1fa3278c5' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CSP, Heuristiken</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K3) Verbesserung der BT-Suche mit Heuristiken: MRV, Gradheuristik, LCV</li></ul>
  </div>
</div>




    <h2 id="variables-variablen-sortierung-welche-variable-soll-betrachtet-werden">VARIABLES: Variablen-Sortierung, Welche Variable soll betrachtet werden?</h2>
<p><a href="#R-image-0c5298a945a1987ba6412dd7444ddfbb" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics/bt_search_mrv.png?width=65%25&height=auto" style=" height: auto; width: 65%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0c5298a945a1987ba6412dd7444ddfbb"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics/bt_search_mrv.png?width=65%25&height=auto"></a></p>
<p><span class='alert'><strong>VARIABLES</strong></span>: Welche Variable zuerst ausprobieren?</p>
<p><strong>Minimum Remaining Values (MRV)</strong>: (vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html#id_Russell2020">[Russell2020, S. 177]</a>)</p>
<ul>
<li>
<p>Wähle Variable mit wenigsten freien Werten (die am meisten eingeschränkte Variable)</p>
<p>=&gt; reduziert den Verzweigungsgrad</p>
</li>
</ul>
<p>Beispiel:</p>
<ol>
<li>Freie Auswahl, alle haben gleich viele freie Werte (jeweils 3) =&gt; wähle A</li>
<li>B und C haben nur noch zwei freie Werte =&gt; wähle B (oder C)</li>
<li>C hat nur noch einen Wert, D noch zwei, der Rest drei =&gt; wähle C</li>
</ol>
<h2 id="variables-gleichstand-bei-mrv">VARIABLES: Gleichstand bei MRV</h2>
<p><a href="#R-image-a250652ceabee66a8fb3b82bd36e0169" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics/bt_search_mrv.png?width=65%25&height=auto" style=" height: auto; width: 65%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a250652ceabee66a8fb3b82bd36e0169"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics/bt_search_mrv.png?width=65%25&height=auto"></a></p>
<p><span class='alert'><strong>VARIABLES</strong></span>: Welche Variable zuerst ausprobieren?</p>
<p><strong>Gradheuristik</strong>: Erweiterung von <em>MRV</em> bei <em>Gleichstand</em> (vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html#id_Russell2020">[Russell2020, S. 177]</a>)</p>
<ul>
<li>
<p>Wähle Variable mit meisten Constraints auf offene (noch nicht zugewiesene) Variablen</p>
<p>=&gt; reduziert den Verzweigungsgrad in späteren Schritten</p>
</li>
</ul>
<p>Beispiel:</p>
<ol>
<li>MRV: Alle haben gleich viele freie Werte (jeweils 3) =&gt; Gradheuristik: B, C und D haben
die meisten Verbindungen (Constraints) auf offene Variablen =&gt; wähle B (oder C oder D)</li>
<li>MRV: A, C und D haben nur noch zwei freie Werte =&gt; Gradheuristik: C und D haben
je zwei Constraints auf noch offene Variablen =&gt; wähle C (oder D)</li>
<li>MRV: A und D haben beide nur noch einen Wert =&gt; Gradheuristik: D hat
die meisten Verbindungen (Constraints) auf offene Variablen =&gt; wähle D</li>
</ol>
<h2 id="values-werte-sortierung-welchen-wert-soll-ich-ausprobieren">VALUES: Werte-Sortierung, Welchen Wert soll ich ausprobieren?</h2>
<p><a href="#R-image-fff4048ad19891b36bc375a7ced45b73" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics/bt_search_lcv.png?width=65%25&height=auto" style=" height: auto; width: 65%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fff4048ad19891b36bc375a7ced45b73"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics/bt_search_lcv.png?width=65%25&height=auto"></a></p>
<p><span class='alert'><strong>VALUES</strong></span>: Welchen Wert zuerst ausprobieren?</p>
<p><strong>Least Constraining Value (LCV)</strong>: (vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html#id_Russell2020">[Russell2020, S. 177]</a>)</p>
<ul>
<li>
<p>Wähle Wert, der für verbleibende Variablen die wenigsten Werte
ungültig macht</p>
<p>=&gt; verringert die Wahrscheinlichkeit für Backtracking</p>
</li>
</ul>
<p>Beispiel:</p>
<ol>
<li>Sei A gewählt: Alle Werte machen in den anderen Variablen einen Wert ungültig
=&gt; freie Wahl des Wertes =&gt; wähle beispielsweise rot</li>
<li>Sei B gewählt: Alle Werte machen in den anderen Variablen einen Wert ungültig
=&gt; freie Wahl des Wertes =&gt; wähle beispielsweise grün</li>
<li>Sei D gewählt: Verbleibende Werte rot und blau
<ul>
<li>Wahl von rot würde für C einen Wert übrig lassen (blau)</li>
<li>Wahl von blau würde für C <strong>keinen</strong> Wert übrig lassen
=&gt; LCV: Wahl von rot!</li>
</ul>
</li>
</ol>
<p><strong>Hinweis</strong>: Diese Heuristik ist in der Praxis sehr aufwändig zu berechnen! Man müsste für
jeden Wert die noch offenen Constraints anschauen und berechnen, wie viele Werte damit jeweils
ungültig gemacht werden. Die Idee ist aber dennoch interessant, und möglicherweise kann man
sie für ein reales Problem so adaptieren, dass bei der Umsetzung nur wenig zusätzlicher
Aufwand entsteht.</p>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Verbesserung der BT-Suche mit Heuristiken: MRV, Gradheuristik, LCV</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106573&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest CSP, Heuristiken (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p>Sei <span class="math align-center">$D=\lbrace 0, \ldots, 5 \rbrace$</span>, und ein Constraintproblem definiert durch</p>
<span class="math align-center">$$\langle
    \lbrace v_1, v_2, v_3, v_4 \rbrace,
    \lbrace D_{v_1} = D_{v_2} = D_{v_3} = D_{v_4} = D \rbrace,
    \lbrace c_1, c_2, c_3, c_4 \rbrace
\rangle$$</span>
<p>mit</p>
<ul>
<li><span class="math align-center">$c_1=\left((v_1,v_2), \lbrace (x,y) \in D^2 | x+y = 3 \rbrace\right)$</span>,</li>
<li><span class="math align-center">$c_2=\left((v_2,v_3), \lbrace (x,y) \in D^2 | x+y \le 3 \rbrace\right)$</span>,</li>
<li><span class="math align-center">$c_3=\left((v_1,v_3), \lbrace (x,y) \in D^2 | x \le y \rbrace\right)$</span> und</li>
<li><span class="math align-center">$c_4=\left((v_3,v_4), \lbrace (x,y) \in D^2 | x \ne y \rbrace\right)$</span>.</li>
</ul>
<ol>
<li>Zeichen Sie den Constraint-Graph.</li>
<li>Welche Variable würde bei der Anwendung von <em>MRV</em> und <em>Gradheuristik</em> im ersten Schritt bei der Suche mit der BT-Search ausgewählt?</li>
<li>Geben Sie eine Lösung für das Problem an.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Constraints</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Bartak2001'>[Bartak2001] <strong>Theory and Practice of Constraint Propagation</strong><br>Barták, R., 2001.</li> <li id='id_Kumar1992'>[Kumar1992] <strong>Algorithms for Constraint Satisfaction Problems: A Survey</strong><br>Kumar, V., 1992.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>CSP, Backtracking/Heuristiken: Abschnitt 5.3</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Kantenkonsistenz und AC-3</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Bei der Backtracking-Suche werden schrittweise Variablen belegt. Dabei kann eine Belegung eine Lösung
im weiteren Verlauf der Suche unmöglich machen, so dass (viel) Backtracking notwendig wird.</p>
<p>Beim <strong>Forward Checking</strong> entfernt man nach der Belegung einer Variablen in allen Nachbarvariablen
die durch die aktuelle Belegung inkonsistent gewordenen Werte. Wenn dabei ein Wertebereich leer wird,
führt die aktuelle Belegung nicht zu einer Lösung und kann sofort zurückgenommen werden. Allerdings
findet man mit Forward Checking nicht alle Inkonsistenzen.</p>
<p>Bei der <strong>Kantenkonsistenz</strong> prüft man, ob zu jedem Wert aus dem Wertebereich einer Variablen in den
Nachbarvariablen mindestens ein passender (konsistenter) Wert existiert. Dabei werden die Constraints
nacheinander betrachtet (nicht gleichzeitig). Wenn dies nicht der Fall ist, wird der Wert aus dem
Wertebereich der betrachteten Variablen entfernt. Der AC-3-Algorithmus erzeugt schrittweise Kantenkonsistenz
für ein CSP.</p>
<p>Man kann den AC-3 als Vorverarbeitung nutzen und die Wertemengen <em>vor</em> der BT-Suche reduzieren. Eventuell
findet man dabei bereits eine Lösung oder kann eine Lösung ausschließen. Man kann den AC-3 auch als
Inferenzschritt in die BT-Suche einbetten (&quot;MAC&quot;).</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/TvF78iVDwKM' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CSP, AC-3</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/8fa264520ed3ce80f71936e084254d14c579ff19e2c724e914a6df761d12c3c7d22d62ebf625cc9181f29e288922785522e7cc60f9e7ce6cb369a3148b115ca7' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL CSP, AC-3</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Forward Checking (<em>FC</em>)</li> <li>(K2) Erweiterung von <em>FC</em> auf alle Kanten: <em>Kantenkonsistenz</em></li> <li>(K2) Kantenkonsistenz bedeutet nicht globale Konsistenz</li> <li>(K3) AC-3 Algorithmus</li></ul>
  </div>
</div>




    <h2 id="problem-bei-bt-suche">Problem bei BT-Suche</h2>
<p>Zuweisung eines Wertes an Variable <span class="math align-center">$X$</span>:</p>
<ul>
<li>Passt zu aktueller Belegung</li>
<li>Berücksichtigt aber nicht <strong>restliche</strong> Constraints
=&gt; macht weitere Suche u.U. unmöglich/schwerer</li>
</ul>
<p><strong>Lösung</strong>: Nach Zuweisung alle <em>nicht zugewiesenen Nachbarvariablen</em> prüfen</p>
<h2 id="inference-vorab-prüfung-forward-checking">INFERENCE: Vorab-Prüfung (Forward Checking)</h2>
<p><a href="#R-image-a7f57894f982d79f8d3b15481887939a" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3/bt_search_inference.png?width=65%25&height=auto" style=" height: auto; width: 65%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a7f57894f982d79f8d3b15481887939a"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3/bt_search_inference.png?width=65%25&height=auto"></a></p>
<p><span class='alert'><strong>Inference</strong></span>: Frühzeitiges Erkennen von Fehlschlägen! (vgl. <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html#id_Russell2020">[Russell2020, S. 178]</a>)</p>
<p>Nach Zuweisung eines Wertes an Variable <span class="math align-center">$X$</span>:</p>
<ul>
<li>Betrachte alle nicht zugewiesenen Variablen <span class="math align-center">$Y$</span>:
<ul>
<li>Falls Constraints zw. <span class="math align-center">$X$</span> und <span class="math align-center">$Y$</span>, dann ...</li>
<li>... entferne alle inkonsistenten Werte aus dem Wertebereich von <span class="math align-center">$Y$</span>.</li>
</ul>
</li>
</ul>
<p>Beispiel:</p>
<ol>
<li>Sei A auf rot gesetzt =&gt; entferne rot in B und C</li>
<li>Sei D auf grün gesetzt =&gt; entferne grün in B und C und E</li>
</ol>
<p>Problem: Für B und C bleibt nur noch blau; sind aber benachbart!</p>
<h2 id="forward-checking-findet-nicht-alle-inkonsistenzen">Forward Checking findet nicht alle Inkonsistenzen!</h2>
<p><a href="#R-image-19d41663790772506d910eddef5aeed4" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3/forward_checking.png?width=55%25&height=auto" style=" height: auto; width: 55%;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-19d41663790772506d910eddef5aeed4"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3/forward_checking.png?width=55%25&height=auto"></a></p>
<ul>
<li>Nach <span class="math align-center">$\lbrace A=red, D=green \rbrace$</span> bleibt für B und C nur noch blue</li>
<li>B und C sind aber benachbart</li>
</ul>
<h2 id="übergang-von-forward-checking-zu-kantenkonsistenz">Übergang von Forward Checking zu Kantenkonsistenz</h2>
<ul>
<li>
<p>Forward Checking erzeugt Konsistenz für alle Constraints der
<strong>gerade betrachteten (belegten) Variablen</strong>.</p>
</li>
<li>
<p>Idee: Ausdehnen auf alle Kanten ... =&gt; Einschränken der Wertemengen</p>
</li>
</ul>
<h2 id="definition-kantenkonsistenz-arc-consistency">Definition Kantenkonsistenz (Arc Consistency)</h2>
<blockquote>
<p>Eine Kante von <span class="math align-center">$X$</span> nach <span class="math align-center">$Y$</span> ist &quot;<span class='alert'>konsistent</span>&quot;, wenn für jeden Wert
<span class="math align-center">$x \in D_X$</span> und für alle Constraints zwischen <span class="math align-center">$X$</span> und <span class="math align-center">$Y$</span> jeweils ein Wert
<span class="math align-center">$y \in D_Y$</span> existiert, so dass der betrachtete Constraint durch <span class="math align-center">$(x,y)$</span>
erfüllt ist.</p>
</blockquote>
<p>Ein CSP ist kanten-konsistent, wenn für alle Kanten des CSP Konsistenz herrscht.</p>
<h2 id="beispiel-kantenkonsistenz">Beispiel Kantenkonsistenz</h2>
<span class="math align-center">$V = \lbrace a,b,c,d,e \rbrace$</span>
<span class="math align-center">$\mathrm{C} = \lbrace ((a,b), \ne), ((b,c), \ne), ((a,c), \ne), ((c,d), =), ((b,e), <) \rbrace$</span>
<p><span class="math align-center">$D_a=D_b=D_c=\lbrace 1,2,3 \rbrace$</span>, <span class="math align-center">$D_d=\lbrace 1,2 \rbrace$</span>, <span class="math align-center">$D_e=\lbrace 1,2,3 \rbrace$</span></p>
<p>Einschränkung der Ausgangswertemengen (kanten-konsistent)</p>
<p><span class="math align-center">$D_a=\lbrace 1,2,3 \rbrace$</span>, <span class="math align-center">$D_b=\lbrace 1,2 \rbrace$</span>, <span class="math align-center">$D_c=\lbrace 1,2 \rbrace$</span>, <span class="math align-center">$D_d=\lbrace 1,2 \rbrace$</span>, <span class="math align-center">$D_e=\lbrace 2,3 \rbrace$</span></p>
<div style="text-align:center;">
<span class="badge cstyle primary"><span class="badge-content"><p>=&gt; Kantenkonsistenz ist nur <strong>lokale</strong> Konsistenz!</p>
</span></span>
</div>
<p><em>Anmerkung</em>: <span class="math align-center">$((a,b), \ne)$</span> ist Kurzform für
<span class="math align-center">$\left((a,b), \lbrace (x,y) \in D_a \times D_b | x \ne y \rbrace\right)$</span></p>
<h2 id="ac-3-algorithmus-herstellen-von-kantenkonsistenz">AC-3 Algorithmus: Herstellen von Kantenkonsistenz</h2>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">AC3</span>(csp):
</span></span><span style="display:flex;"><span>    queue <span style="color:#f92672">=</span> Queue(csp<span style="color:#f92672">.</span>arcs)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> queue<span style="color:#f92672">.</span>isEmpty():
</span></span><span style="display:flex;"><span>        (x,y) <span style="color:#f92672">=</span> queue<span style="color:#f92672">.</span>dequeue()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> ARC_Reduce(csp,x,y):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> D_x<span style="color:#f92672">.</span>isEmpty(): <span style="color:#66d9ef">return</span> false
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> z <span style="color:#f92672">in</span> x<span style="color:#f92672">.</span>neighbors(): queue<span style="color:#f92672">.</span>enqueue(z,x)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> true
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ARC_Reduce</span>(csp, x, y):
</span></span><span style="display:flex;"><span>    change <span style="color:#f92672">=</span> false
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> v <span style="color:#f92672">in</span> D_x:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> (any w <span style="color:#f92672">in</span> D_y <span style="color:#f92672">and</span> csp<span style="color:#f92672">.</span>C_xy(v,w)):
</span></span><span style="display:flex;"><span>            D_x<span style="color:#f92672">.</span>remove(v);  change <span style="color:#f92672">=</span> true
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> change</span></span></code></pre></div>
<p><span class='origin'>Quelle: Eigener Code basierend auf einer Idee nach <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html#id_Russell2020">[Russell2020, p. 171, fig. 5.3]</a></span></p>
<p><em>Anmerkung</em>: Die Queue in AC-3 ist wie eine (mathematische) Menge zu betrachten: Jedes Element
kann nur genau einmal in einer Menge enthalten sein. D.h. wenn man bei <code>queue.enqueue(z,x)</code> die
Rückkanten von den Nachbarn in die Queue aufnimmt, sorgt die Queue eigenständig dafür, dass es
keine doppelten Vorkommen einer Kante in der Queue gibt. (Falls die verwendete Queue in einer
Programmiersprache das nicht unterstützt, müsste man bei <code>queue.enqueue(z,x)</code> stets abfragen, ob
die Kante <code>(z,x)</code> bereits in der Queue ist und diese dann nicht erneut hinzufügen.)
AC-3 hat eine Laufzeit von <span class="math align-center">$O(d^3n^2)$</span> (<span class="math align-center">$n$</span> Knoten, maximal <span class="math align-center">$d$</span> Elemente pro Domäne). Leider findet
auch AC-3 nicht alle Inkonsistenzen ... (NP-hartes Problem).</p>
<p><em>Hinweis</em>: In gewisser Weise kann man Forward Checking als ersten Schritt bei der
Herstellung von Kantenkonsistenz interpretieren.</p>
<h2 id="einsatz-des-ac-3-algorithmus">Einsatz des AC-3 Algorithmus</h2>
<ol>
<li>
<p>Vorverarbeitung: Reduktion der Wertemengen <em>vor</em> BT-Suche</p>
<ul>
<li>Nach AC-3 evtl. bereits Lösung gefunden (oder ausgeschlossen)</li>
</ul>
</li>
<li>
<p>Propagation: Einbetten von AC-3 als Inferenzschritt in BT-Suche
(<strong>MAC</strong> -- Maintaining Arc Consistency)</p>
<ul>
<li>Nach jeder Zuweisung an <span class="math align-center">$X_i$</span> Aufruf von AC-3-Variante:
<ul>
<li>Initial nur Kanten von <span class="math align-center">$X_i$</span> zu allen noch nicht zugewiesenen Nachbarvariablen</li>
</ul>
</li>
<li>Anschließend rekursiver Aufruf von BT-Suche</li>
</ul>
</li>
</ol>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Anwendung von Forward Checking und ...</li>
<li>... die Erweiterung auf alle Kanten: AC-3, Kantenkonsistenz</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106574&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest CSP, AC-3 (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Fingerübungen</strong></p>
<p>Ist die Kante zwischen a und b konsistent?</p>
<p><a href="#R-image-f40c80145a177fbbb5534518505d2c82" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/csp/images/csp_challenge_a.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f40c80145a177fbbb5534518505d2c82"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/csp/images/csp_challenge_a.png?width=auto&height=auto"></a></p>
<p>Wann ist der Graph lokal konsistent?</p>
<p><a href="#R-image-8dcd076405a70f1c5cb66f2f0c78a920" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/csp/images/csp_challenge_b.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8dcd076405a70f1c5cb66f2f0c78a920"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/csp/images/csp_challenge_b.png?width=auto&height=auto"></a></p>
<ul>
<li>a {1,2}; b {2,3}; c {1,2,3}; d {1,2,3}</li>
<li>a {1,2}; b {2,3}; c {3}; d {1,2}</li>
<li>a {1,3}; b {2,3}; c {1,3}; d {1,2,3}</li>
<li>a {1,2}; b {2,3}; c {1,3}; d {1,2,3}</li>
</ul>
<p>Wie sieht die Queue im nächsten Schritt mit AC3 aus?</p>
<p><a href="#R-image-0c4a27eaa98aa4478e54c349e7daf2ca" class="lightbox-link"><img class="noborder lazy lightbox noshadow figure-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/csp/images/csp_challenge_c.png?width=auto&height=auto" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0c4a27eaa98aa4478e54c349e7daf2ca"><img class="noborder lazy lightbox noshadow lightbox-image" loading="lazy" src="https://raw.githubusercontent.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/master/lecture/csp/images/csp_challenge_c.png?width=auto&height=auto"></a></p>
<p>Aktuelle Queue: [ab, ac, ba, bc, ca, cb]</p>
<ul>
<li>[bc, ba, ca, cb, ab, ac]</li>
<li>[ab, ac, ba, bc, ca, cb]</li>
<li>[ac, ba, bc, ca, cb]</li>
<li>[ac, ba, bc, ca, cb, ba]</li>
</ul>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Constraints</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                
            
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Bartak2001'>[Bartak2001] <strong>Theory and Practice of Constraint Propagation</strong><br>Barták, R., 2001.</li> <li id='id_Kumar1992'>[Kumar1992] <strong>Algorithms for Constraint Satisfaction Problems: A Survey</strong><br>Kumar, V., 1992.</li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>CSP, AC-3: Abschnitt 5.2</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          <article class="default">
            <header class="headline">
            </header>

<h1 id="naive-bayes">Naive Bayes</h1>

<p>Ich habe Symptome beobachtet. Kann ich die Ursache (also die Krankheit)
bestimmen, wenn ich etwas Hintergrundwissen habe:</p>
<ul>
<li>Wie häufig treten verschieden Krankheiten auf</li>
<li>Welche Krankheit zeigt welche Symptome (und wie oft treten die dann auf)</li>
</ul>
<p>Kann ich aus diesen Daten einen Klassifikator lernen?</p>
<ul class="children children-li children-sort-">
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb1-probability.html">Wiederholung Wahrscheinlichkeitstheorie</a></li>
  <li><a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb2-naivebayes.html">Klassifikation mit Naive Bayes</a></li>
</ul>

            <footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Naive Bayes</h1>
<article class="default">
<h1>Wiederholung Wahrscheinlichkeitstheorie</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Diese Sitzung ist eine (relativ oberflächliche) Einführung/Wiederholung in die/der
Grundlagen der Wahrscheinlichkeitstheorie.</p>
<p>Wir schauen uns die möglichen Ausgänge eines Zufallsexperiments an (&quot;Ereignisse&quot;).
Wenn diese Ereignisse sich gegenseitig ausschließen und alle denkbaren Ergebnisse
abdecken, dann nennt man diese Ereignisse auch <strong>Elementarereignisse</strong>. Die
Wahrscheinlichkeit für ein Ereignis kann man angeben als Anzahl der möglichen
Ergebnisse, die für dieses Ereignis günstig sind, geteilt durch die Anzahl aller
Ausgänge. Über die Kolmogorov Axiome bekommt man die typischen Rechenregel für
die Wahrscheinlichkeit.</p>
<p>Man kann eine <strong>Verbundwahrscheinlichkeit</strong> <span class="math align-center">$P(A,B) = P(B,A)$</span> angeben, das ist
die Wahrscheinlichkeit, dass <span class="math align-center">$A$</span> und <span class="math align-center">$B$</span> gleichzeitig auftreten.</p>
<p>Die <strong>bedingte</strong> Wahrscheinlichkeit für <span class="math align-center">$A$</span> gegeben <span class="math align-center">$B$</span> ist <span class="math align-center">$P(A|B)$</span> und berechnet
sich <span class="math align-center">$P(A|B) = \frac{P(A,B)}{P(B)}$</span>.</p>
<p>Daraus kann man die <strong>Bayes-Regel</strong> ableiten: <span class="math align-center">$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$</span>
Dabei nennt man</p>
<ul>
<li><span class="math align-center">$P(A)$</span> <strong>&quot;Prior&quot;</strong> oder <strong>&quot;A-priori-Wahrscheinlichkeit&quot;</strong>
(die Wahrscheinlichkeit für <span class="math align-center">$A$</span> ohne weiteres Wissen),</li>
<li><span class="math align-center">$P(B|A)$</span> <strong>&quot;Likelihood&quot;</strong>
(Wie wahrscheinlich ist das Auftreten von <span class="math align-center">$B$</span>, gegeben <span class="math align-center">$A$</span>?),</li>
<li><span class="math align-center">$P(A|B)$</span> <strong>&quot;Posterior&quot;</strong> oder <strong>&quot;A-posteriori-Wahrscheinlichkeit&quot;</strong>
(Wie wahrscheinlich ist <span class="math align-center">$A$</span>, wenn <span class="math align-center">$B$</span> eingetreten ist?), und</li>
<li><span class="math align-center">$P(B)$</span> ist ein Normierungsfaktor
(Wie wahrscheinlich ist <span class="math align-center">$B$</span> an sich?).</li>
</ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/p_Yy5rkl4CA' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Wahrscheinlichkeiten</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/91611d22ad88ef1adf04c3956746256d14c609274091660652ff567645a537a8983f61cf6c3406110e27dc6cb56b65b599d72d60ab37f45977b6732f6610830d' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Wahrscheinlichkeiten</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Elementarereignisse und Wahrscheinlichkeit</li> <li>(K2) Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit</li> <li>(K2) (Bedingte) Unabhängigkeit</li> <li>(K3) Rechenregeln</li> <li>(K3) Marginalisierung</li> <li>(K3) Bayes'sche Regel</li></ul>
  </div>
</div>




    <h2 id="ereignisse-und-wahrscheinlichkeit">Ereignisse und Wahrscheinlichkeit</h2>
<p><strong>Hinweis</strong>: Die folgende Darstellung zur Einführung in die
Wahrscheinlichkeitstheorie dient dem Verständnis des Naive Bayes
Klassifikationsalgorithmus und ist teilweise eher oberflächlich gehalten.
Sie kann und soll keine entsprechende mathematische Einführung ersetzen!</p>
<h3 id="ereignisse">Ereignisse</h3>
<ul>
<li>
<p><strong>Ereignisse</strong> <span class="math align-center">$\Omega = \lbrace \omega_1, \omega_2, \ldots, \omega_n \rbrace$</span>:
endliche Menge der Ausgänge eines Zufallsexperiments</p>
</li>
<li>
<p><strong>Elementarereignis</strong>: Die <span class="math align-center">$\omega_i \in \Omega$</span></p>
<ul>
<li>decken <em>alle</em> möglichen Versuchsergebnisse ab, und</li>
<li>schließen sich gegenseitig aus</li>
</ul>
</li>
</ul>
<h3 id="regeln">Regeln</h3>
<ul>
<li>Wenn <span class="math align-center">$A$</span> und <span class="math align-center">$B$</span> Ereignisse sind, dann auch <span class="math align-center">$A \cup B$</span></li>
<li><span class="math align-center">$\Omega$</span> wird als <strong>sicheres Ereignis</strong> bezeichnet: Enthält
definitionsgemäß <strong>alle</strong> Versuchsausgänge, d.h. <em>ein</em> in der Menge
enthaltenes Ereignis <em>muss</em> auftreten</li>
<li>Die leere Menge <span class="math align-center">$\emptyset$</span> wird als <strong>unmögliches Ereignis</strong> bezeichnet</li>
<li>Die Variablen <span class="math align-center">$A$</span> und <span class="math align-center">$B$</span> heißen auch <strong>Zufallsvariablen</strong></li>
</ul>
<p>Im Rahmen dieser Veranstaltung betrachten wir nur diskrete Zufallsvariablen mit
endlichem Wertebereich!</p>
<h3 id="wahrscheinlichkeit">Wahrscheinlichkeit</h3>
<ul>
<li>
<p><strong>Wahrscheinlichkeit</strong>:</p>
<p>Sei <span class="math align-center">$\Omega = \lbrace \omega_1, \omega_2, \ldots, \omega_n \rbrace$</span> endlich.
Die Wahrscheinlichkeit <span class="math align-center">$P(A)$</span> für ein Ereignis <span class="math align-center">$A$</span> ist dann
definiert als</p>
<span class="math align-center">$$
    P(A) = \frac{|A|}{|\Omega|} =
    \frac{\text{Anzahl der für A günstigen Fälle}}{\text{Anzahl der möglichen Fälle}}
    $$</span>
<p>Man könnte auch schreiben: <span class="math align-center">$P(A) = \sum_{\omega \in A} P(\omega)$</span></p>
<p><em>Hinweis</em>: Diese Definition von Wahrscheinlichkeit geht von gleich
wahrscheinlichen Elementarereignissen aus! Die allgemeine Definition
geht über einen entsprechenden Grenzwert.</p>
</li>
</ul>
<h3 id="verteilung">Verteilung</h3>
<p>Den Vektor mit den Wahrscheinlichkeiten aller Elementarereignisse
nennt man auch <em>Verteilung</em>.</p>
<p>Beispiel: <span class="math align-center">$\mathbf{P}(A) = (P(A=1), P(A=2), \ldots, P(A=6)) = (1/6, 1/6, \ldots, 1/6)$</span></p>
<p><em>Hinweis</em>: Wir betrachten hier nur diskrete Zufallsvariablen. Für
kontinuierliche Variablen wird die Verteilung mit Hilfe einer
<strong>Dichtefunktion</strong> dargestellt, beispielsweise der Gauss'schen Funktion.</p>
<h3 id="beispiel">Beispiel</h3>
<ul>
<li>Einmaliges Würfeln mit einem Spielwürfel: <span class="math align-center">$\Omega = \lbrace 1,2,3,4,5,6 \rbrace$</span></li>
<li>Elementarereignisse: <span class="math align-center">$\lbrace 1,2,3,4,5,6 \rbrace$</span></li>
<li>Das Würfeln einer geraden Zahl (<span class="math align-center">$A = \lbrace 2,4,6 \rbrace$</span>) ist <em>kein</em>
Elementarereignis, ebenso wie das Würfeln einer Zahl kleiner 5
(<span class="math align-center">$B = \lbrace 1,2,3,4 \rbrace$</span>), da <span class="math align-center">$A \cap B = \lbrace 2,4 \rbrace \ne \emptyset$</span></li>
<li>Wahrscheinlichkeit, eine 1 zu würfeln: <span class="math align-center">$P(A \in \lbrace 1 \rbrace) = P(A=1) = \frac{1}{6}$</span>.
<em>Anmerkung</em>: Man schreibt statt <span class="math align-center">$P(A \in \lbrace 1 \rbrace)$</span> oft einfach <span class="math align-center">$P(1)$</span>.</li>
<li>Wahrscheinlichkeit, eine gerade Zahl zu würfeln:
<span class="math align-center">$P(A \in \lbrace 2,4,6 \rbrace) = P(A=2 \vee A=4 \vee A=6) = \frac{|\lbrace 2,4,6 \rbrace|}{|\lbrace 1,2,3,4,5,6 \rbrace|} = \frac{3}{6} = 0.5$</span></li>
</ul>
<h2 id="rechenregeln-kolmogorov-axiome">Rechenregeln: Kolmogorov Axiome</h2>
<p>Sei <span class="math align-center">$A$</span> ein Ereignis, also <span class="math align-center">$A \subseteq \Omega$</span>:</p>
<ul>
<li>
<span class="math align-center">$0 \le P(A) \le 1$</span>
</li>
<li>
<p><span class="math align-center">$\Omega = \lbrace \omega_1, \omega_2, \ldots, \omega_n \rbrace$</span>: <span class="math align-center">$\sum_{i} P(\omega_i) = 1$</span>
(Normierungsbedingung: Summe über die Wahrscheinlichkeiten aller Elementarereignisse ist immer 1)</p>
</li>
<li>
<span class="math align-center">$P(A \cup B) = P(A) + P(B) - P(A \cap B)$</span>
</li>
</ul>
<p>Daraus folgt (u.a.):</p>
<ul>
<li>
<span class="math align-center">$P(\Omega) = 1$</span>
</li>
<li>
<span class="math align-center">$P(\emptyset) = 0$</span>
</li>
<li>
<span class="math align-center">$P(A) = 1- P(\neg A)$</span>
</li>
<li>
<p><span class="math align-center">$A$</span> und <span class="math align-center">$B$</span> <em>unabhängig</em>: <span class="math align-center">$P(A \cup B) = P(A) + P(B)$</span></p>
</li>
<li>
<p><span class="math align-center">$P(A \cap B)$</span> ist leer, wenn <span class="math align-center">$A$</span> und <span class="math align-center">$B$</span> sich nicht überlappen</p>
</li>
<li>
<p><span class="math align-center">$A \subseteq B$</span>: <span class="math align-center">$P(A) \le P(B)$</span></p>
</li>
</ul>
<h2 id="verbundwahrscheinlichkeiten">Verbundwahrscheinlichkeiten</h2>
<span class="math align-center">$$P(A,B) = P(B,A) = \text{ Wahrscheinlichkeit, dass A und B gleichzeitig auftreten }$$</span>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">Halsschmerzen</th>
          <th style="text-align: left"><span class="math align-center">$\neg$</span> Halsschmerzen</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Schnupfen</td>
          <td style="text-align: left">0.04</td>
          <td style="text-align: left">0.06</td>
      </tr>
      <tr>
          <td style="text-align: left"><span class="math align-center">$\neg$</span> Schnupfen</td>
          <td style="text-align: left">0.01</td>
          <td style="text-align: left">0.89</td>
      </tr>
  </tbody>
</table>
<ul>
<li><span class="math align-center">$P(S,H) = 0.04$</span></li>
</ul>
<p>Die Tabelle kann man so lesen: In 4 von 100 Fällen tritt das Ereignis &quot;Schnupfen&quot;
gleichzeitig mit dem Ereignis &quot;Halsschmerzen&quot; auf, in 6 von 100 Fällen tritt
&quot;Schupfen&quot; ohne Halsschmerzen auf. ... In Summe kommt man wieder auf 100 Fälle
(100 Prozent).</p>
<p>Nach diesen Zahlen liegt also die Verbundwahrscheinlichkeit für die Ereignisse
&quot;Schnupfen&quot; und &quot;Husten&quot;, d.h. <span class="math align-center">$P(S,H)$</span>, bei 4 Prozent.</p>
<p><strong>Hinweis</strong>: Die gezeigten Zahlen und Zusammenhänge sind <strong>fiktiv</strong>
und dienen lediglich zur Verdeutlichung der Wahrscheinlichkeitsbegriffe!</p>
<h2 id="bedingte-wahrscheinlichkeit">Bedingte Wahrscheinlichkeit</h2>
<p><strong>Definition</strong>:
Bedingte Wahrscheinlichkeit für <span class="math align-center">$A$</span> gegeben <span class="math align-center">$B$</span>:</p>
<span class="math align-center">$$P(A|B) = \frac{P(A,B)}{P(B)}$$</span>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">Halsschmerzen</th>
          <th style="text-align: left"><span class="math align-center">$\neg$</span> Halsschmerzen</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Schnupfen</td>
          <td style="text-align: left">0.04</td>
          <td style="text-align: left">0.06</td>
      </tr>
      <tr>
          <td style="text-align: left"><span class="math align-center">$\neg$</span> Schnupfen</td>
          <td style="text-align: left">0.01</td>
          <td style="text-align: left">0.89</td>
      </tr>
  </tbody>
</table>
<ul>
<li><span class="math align-center">$P(\text{Schnupfen } | \text{ Halsschmerzen}) = \frac{P(S,H)}{P(H)} = \frac{0.04}{0.04+0.01} = 0.8$</span></li>
<li><span class="math align-center">$P(\text{Halsschmerzen } | \text{ Schnupfen}) = \frac{P(H,S)}{P(S)} = \frac{0.04}{0.04+0.06} = 0.4$</span></li>
</ul>
<p>Wegen <span class="math align-center">$P(A|B) = \dfrac{P(A,B)}{P(B)}$</span> ist <span class="math align-center">$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$</span>
(<strong>Produkt-Regel</strong>)!</p>
<h2 id="marginalisierung">Marginalisierung</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">Halsschmerzen</th>
          <th style="text-align: left"><span class="math align-center">$\neg$</span> Halsschmerzen</th>
          <th style="text-align: left"><span class="math align-center">$\sum$</span></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Schnupfen</td>
          <td style="text-align: left">0.04</td>
          <td style="text-align: left">0.06</td>
          <td style="text-align: left"><em>0.1</em></td>
      </tr>
      <tr>
          <td style="text-align: left"><span class="math align-center">$\neg$</span> Schnupfen</td>
          <td style="text-align: left">0.01</td>
          <td style="text-align: left">0.89</td>
          <td style="text-align: left"><em>0.9</em></td>
      </tr>
      <tr>
          <td style="text-align: left"><span class="math align-center">$\sum$</span></td>
          <td style="text-align: left"><em>0.05</em></td>
          <td style="text-align: left"><em>0.95</em></td>
          <td style="text-align: left"><em>1</em></td>
      </tr>
  </tbody>
</table>
<span class="math align-center">$P(S) = P(S,H) + P(S, \neg H)$</span>
<p>Allgemein:
Seien <span class="math align-center">$B_1, \ldots, B_n$</span> Elementarereignisse mit <span class="math align-center">$\bigcup_i B_i = \Omega$</span>.
Dann ist <span class="math align-center">$$P(A) = \sum_i P(A,B_i) = \sum_i P(A|B_i)P(B_i)$$</span></p>
<p>Diesen Vorgang nennt man <strong>Marginalisierung</strong>. Die resultierende Verteilung
<span class="math align-center">$P(A)$</span> nennt man auch <em>&quot;Randverteilung&quot;</em>, da sie mit einer Projektion eines
Quaders auf eine Seitenfläche vergleichbar ist.</p>
<h2 id="kettenregel">Kettenregel</h2>
<ul>
<li>
<p><strong>Produktregel</strong>: Wegen <span class="math align-center">$P(A|B) = \dfrac{P(A,B)}{P(B)}$</span>
gilt <span class="math align-center">$P(A,B) = P(A|B)P(B)$</span></p>
</li>
<li>
<p>Verallgemeinerung (<strong>Kettenregel</strong>):
<span class="math align-center">$$
    \begin{array}{rcl}
    P(A_1,A_2,\ldots,A_n) &=& P(A_n,\ldots,A_2,A_1)\\
        & = & P(A_n|A_{n-1},\ldots,A_1)P(A_{n-1},\ldots,A_1)\\
        & = & P(A_n|A_{n-1},\ldots,A_1)P(A_{n-1}|A_{n-2},\ldots,A_1)P(A_{n-2},\ldots,A_1)\\
        & = & \ldots\\
        & = & P(A_n|A_{n-1},\ldots,A_1) \ldots P(A_2|A_1)P(A_1)\\
        & = & \prod_i P(A_i|A_1,\ldots,A_{i-1})
    \end{array}
    $$</span></p>
</li>
</ul>
<h2 id="bayes-regel">Bayes-Regel</h2>
<p>Bedingte Wahrscheinlichkeit: <span class="math align-center">$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$</span></p>
<span class="math align-center">$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$</span>
<ul>
<li><span class="math align-center">$P(A)$</span> nennt man <strong>&quot;Prior&quot;</strong> oder <strong>&quot;A-priori-Wahrscheinlichkeit&quot;</strong>
(Das ist die Wahrscheinlichkeit für <span class="math align-center">$A$</span> ohne weiteres Wissen)</li>
<li><span class="math align-center">$P(B|A)$</span> nennt man <strong>&quot;Likelihood&quot;</strong>
(Wie wahrscheinlich ist das Auftreten von <span class="math align-center">$B$</span>, gegeben <span class="math align-center">$A$</span>?)</li>
<li><span class="math align-center">$P(A|B)$</span> nennt man <strong>&quot;Posterior&quot;</strong> oder <strong>&quot;A-posteriori-Wahrscheinlichkeit&quot;</strong>
(Wie wahrscheinlich ist <span class="math align-center">$A$</span>, wenn <span class="math align-center">$B$</span> eingetreten ist?)</li>
<li><span class="math align-center">$P(B)$</span> ist ein Normierungsfaktor</li>
</ul>
<p>Wenn man (siehe später: Naive Bayes Klassifikator) <span class="math align-center">$A$</span> als Klasse und <span class="math align-center">$B$</span> als
Daten betrachtet:</p>
<ul>
<li><span class="math align-center">$P(A)$</span>: Wie wahrscheinlich ist eine bestimmte Klasse an sich
(A-priori-Wahrscheinlichkeit der Klassen)?</li>
<li><span class="math align-center">$P(B|A)$</span>: Wie wahrscheinlich sind bestimmte Daten, gegeben die Klasse <span class="math align-center">$A$</span>?
(Likelihood der Daten)</li>
<li><span class="math align-center">$P(A|B)$</span>: Gegeben die Daten <span class="math align-center">$B$</span>, wie wahrscheinlich ist die Klasse <span class="math align-center">$A$</span>?
(Posterior)</li>
</ul>
<p>In der Medizin hat sucht man i.d.R. die Ursache für beobachtete Symptome:
<span class="math align-center">$$
P(\text{Ursache}|\text{Symptome}) = \frac{P(\text{Symptome}|\text{Ursache})P(\text{Ursache})}{P(\text{Symptome})}
$$</span></p>
<p>Aus der A-priori-Wahrscheinlichkeit für bestimmte Krankheiten und der
Likelihood der Symptome (wie wahrscheinlich sind Symptome, gegeben eine
Krankheit) kann man die Wahrscheinlichkeit für das Vorliegen einer Erkrankung
gegeben bestimmte Symptome berechnen.</p>
<h2 id="beispiel-bayes">Beispiel Bayes</h2>
<ul>
<li>Bei Arthrose wird in 80 Prozent der Fälle ein steifes Gelenk beobachtet</li>
<li>Eine von 10.000 Personen hat Arthrose</li>
<li>Eine von 10 Personen hat ein steifes Gelenk</li>
</ul>
<p>=&gt; Ich habe ein steifes Gelenk. Habe ich Arthrose?</p>
<ul>
<li>Gegeben: <span class="math align-center">$P(A) = 0.0001,   P(S) = 0.1,   P(S|A) = 0.8$</span></li>
<li>Gesucht: <span class="math align-center">$P(A|S)$</span></li>
</ul>
<span class="math align-center">$$
P(A|S) = \frac{P(S|A)P(A)}{P(S)} = \frac{0.8 \times 0.0001}{0.1} = 0.0008 = 0.08\%
$$</span>
<p>Wenn ein steifes Gelenk vorliegt, ist die Wahrscheinlichkeit, dann an Arthrose
erkrankt zu sein, bei nur 0.08%. Kein Grund zur Sorge in diesem Fall :-)</p>
<p>=&gt; Wie wahrscheinlich ist ein steifes Gelenk ohne Arthrose, also <span class="math align-center">$P(S|\neg A$</span>)?</p>
<p>Mit Marginalisierung: <span class="math align-center">$P(S) = P(S|A)P(A) + P(S|\neg A)P(\neg A)$</span>,
d.h. <span class="math align-center">$0.1 = 0.8 \times 0.0001 + P(S|\neg A) \times (1-0.0001)$</span>,
d.h. <span class="math align-center">$P(S|\neg A) = 0.0999$</span></p>
<p>In knapp 10 Prozent der Fälle würde man im obigen Beispiel bei der Diagnose
&quot;keine Arthrose&quot; ein steifes Gelenk beobachten.</p>
<p><strong>Hinweis</strong>: Die genannten Zahlen und Zusammenhänge sind rein fiktional und sollen
lediglich zur Veranschaulichung der Bayes-Regel dienen!</p>
<p>Schauen Sie sich auch das Beispiel 7.9 in <a href="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb1-probability.html#id_Ertel2017">[Ertel2017, Ex. 7.9, S. 135]</a> an!</p>
<h2 id="unabhängige-ereignisse">Unabhängige Ereignisse</h2>
<ul>
<li>
<span class="math align-center">$P(\text{Halsschmerzen},\text{ Regen}) = P(\text{Regen }|\text{ Halsschmerzen})P(\text{Halsschmerzen})$</span>
</li>
<li>
<p><span class="math align-center">$P(\text{Regen }|\text{ Halsschmerzen}) = \text{ ?? }$</span> <span class="math align-center">$= P(\text{Regen})$</span></p>
</li>
<li>
<p>Zwei Ereignisse <span class="math align-center">$A$</span> und <span class="math align-center">$B$</span> sind <strong>unabhängig</strong>, wenn
<span class="math align-center">$$ P(A|B) = P(A) $$</span></p>
<p>=&gt; <span class="math align-center">$P(A,B) = P(A|B)P(B) = P(A)P(B)$</span></p>
</li>
</ul>
<p>Dies kann man verallgemeinern (<strong>bedingte Unabhängigkeit</strong>):</p>
<blockquote>
<p><span class="math align-center">$X$</span> und <span class="math align-center">$Y$</span> sind <em>bedingt unabhängig</em> (gegeben <span class="math align-center">$Z$</span>),
wenn <span class="math align-center">$P(X|Y,Z) = P(X|Z)$</span> bzw. <span class="math align-center">$P(Y|X,Z) = P(Y|Z)$</span></p>
</blockquote>
<p>Daraus folgt:</p>
<span class="math align-center">$$ P(X,Y|Z) = P(X|Y,Z)P(Y|Z) = P(X|Z)P(Y|Z) $$</span>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Grundlagen der Wahrscheinlichkeitstheorie
<ul>
<li>Elementarereignisse und Wahrscheinlichkeit</li>
<li>Rechenregeln</li>
<li>Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit</li>
<li>Marginalisierung</li>
<li>(Bedingte) Unabhängigkeit</li>
<li>Bayes'sche Regel</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106587&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Wahrscheinlichkeiten (ILIAS)</a></li></ul>
  </div>
</div>



    



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nb.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Naive Bayes</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Abschnitt 7.1</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Kapitel 12</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

<article class="default">
<h1>Klassifikation mit Naive Bayes</h1>



    



    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-graduation-cap"></i> TL;DR
  </div>
  <div class="box-content">
<p>Mit Hilfe der (verallgemeinerten) Bayes-Regel kann man Klassifikation durchführen.
Dazu werden beim &quot;Training&quot; die bedingten Wahrscheinlichkeiten aus den Trainingsdaten
geschätzt. Die Anwendung (Klassifikation) erfolgt dann durch die Nutzung der beim
&quot;Training&quot; berechneten bedingten Wahrscheinlichkeiten:</p>
<span class="math align-center">$$
h_{MAP} = \operatorname{argmax}_{h \in H} P(h|D_1,  \ldots, D_n) =
\operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i|h)
$$</span>
<p>Für jede Hypothese <span class="math align-center">$h$</span>, d.h. für jede Klasse, wird der Posterior <span class="math align-center">$P(h|D_1,  \ldots, D_n)$</span>
ausgerechnet. Die Klasse, deren Wert dabei am höchsten ist, &quot;gewinnt&quot;, d.h. die Klasse
mit dem größten Posterior wird ausgegeben. (Deshalb wird das Verfahren oft auch &quot;MAP&quot;
-- <em>Maximum a Posteriori</em> -- genannt.)</p>
<p>Bei der Berechnung wird angenommen, dass die betrachteten Merkmale (bedingt) unabhängig
sind (dies geht in die obige Formel ein). Diese Annahme trifft aber oft nicht zu, deshalb
auch der Name &quot;<em>Naive</em> Bayes Klassifikation&quot;. Man berechnet in diesem Fall falsche Werte.
Dennoch zeigt der Algorithmus in der Praxis sehr gute Ergebnisse.</p>
<p>Durch den Einsatz der bedingten Wahrscheinlichkeiten in der Produktformel ergeben sich
einige Schwierigkeiten:</p>
<ol>
<li>Wenn beim &quot;Training&quot; Ausprägungen fehlen, ist die bedingte Wahrscheinlichkeit Null.
Dadurch wird das gesamte Produkt Null. Zur Abhilfe kann man den <strong>Laplace-Schätzer</strong>
nutzen, der (gesteuert über einen Parameter) gewissermaßen virtuelle Trainingsbeispiele
beisteuert.</li>
<li>Durch das Produkt vieler kleiner Werte kann es schnell zu <em>Floating Point</em>-Underflows
kommen. Hier kann man einen Trick nutzen: Man berechnet den Logarithmus der Produktformel.
Dadurch ändern sich zwar die absoluten Werte, die Reihenfolge der Hypothesen bleibt aber
erhalten. Da wir nur nach der Hypothese suchen, die einen höheren Wert als die anderen
hat, und nicht den absoluten Wert an sich benötigen, kann man so vorgehen. Durch den
Logarithmus wird aus dem Produkt eine Summe, wo die kleinen Werte der bedingten
Wahrscheinlichkeiten nicht so starke Auswirkungen haben wie im Produkt.</li>
</ol>
<p>Oft nimmt man zusätzlich an, dass für alle Hypothesen (Klassen) <span class="math align-center">$h$</span> der Prior <span class="math align-center">$P(h)$</span> gleich
ist. Dann kann man diesen Faktor ebenfalls aus der Berechnung entfernen. Dieses Verfahren
nennt man auch &quot;<strong>Maximum Likelihood</strong>&quot;.</p>
<p>Der NB-Klassifikator wird gern für die Textklassifikation eingesetzt. Hier muss man einem
Text ein Label zuordnen. In einer Vorverarbeitung wird zunächst eine Menge der relevanten
Wörter über alle Trainingstexte gebildet (<em>Bag-of-Words</em>). Der Bag-of-Words entspricht einem
Merkmalsvektor, wobei die Merkmale die einzelnen Wörter sind. Dann kann jeder Text der
Trainingsmenge über so einen Merkmalsvektor dargestellt werden: Entweder man gibt pro Merkmal
an, ob es da (1) oder nicht da (0) ist oder man zählt die Häufigkeit des Auftretens. Dann kann
man mit dem NB-Klassifikator die bedingten Wahrscheinlichkeiten schätzen und einen neuen Text
klassifizieren.</p>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (YouTube)
  </div>
  <div class="box-content">
<ul> <li><a href='https://youtu.be/qfX4zp1i-Co' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Naive Bayes Klassifikation</a></li></ul>
  </div>
</div>




    
    
    
    





    
    
        
        
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-podcast"></i> Videos (HSBI-Medienportal)
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/medienportal/m/d84605cc9bccc5d1d8d589c2968726d32539aa629bff06e28096a834730bfd8c1bef1604fd09ee1aab906d30272c5fd1f31b11418fa2bdb2e2710a23dd382d1c' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>VL Naive Bayes Klassifikation</a></li></ul>
  </div>
</div>




    
    
    
    






    
    





    

    

    
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-lightbulb"></i> Lernziele
  </div>
  <div class="box-content">
<ul> <li>(K2) Annahme von Unabhängigkeit =&gt; 'Naive' Bayes Klassifikation</li> <li>(K2) Naivität der Annahme, dennoch sehr gute Erfolge in Praxis</li> <li>(K2) Probleme mit niedrigen Wahrscheinlichkeiten</li> <li>(K3) Schätzen der bedingten Wahrscheinlichkeiten aus den Trainingsdaten</li> <li>(K3) Klassifikation mit Naive Bayes durch Nutzung der geschätzten Wahrscheinlichkeiten</li></ul>
  </div>
</div>




    <h2 id="medizinische-diagnostik-mit-nb">Medizinische Diagnostik mit NB</h2>
<ul>
<li>Bei Arthrose wird in 80 Prozent der Fälle ein steifes Gelenk beobachtet: <span class="math align-center">$P(S|A) = 0.8$</span></li>
<li>Eine von 10.000 Personen hat Arthrose: <span class="math align-center">$P(A) = 0.0001$</span></li>
<li>Eine von 10 Personen hat ein steifes Gelenk: <span class="math align-center">$P(S) = 0.1$</span></li>
</ul>
<p>=&gt; Ich habe ein steifes Gelenk. Habe ich Arthrose?</p>
<h2 id="textklassifikation-mit-nb">Textklassifikation mit NB</h2>
<ul>
<li>
<p>Mails, manuell markiert:</p>
<ul>
<li>D1: (&quot;Sieben Zwerge fraßen sieben Ziegen&quot;, OK)</li>
<li>D2: (&quot;Sieben Ziegen traten sieben Wölfe&quot;, SPAM)</li>
<li>D3: (&quot;Sieben Wölfe fraßen sieben Böcke&quot;, OK)</li>
<li>D4: (&quot;Sieben Böcke traten sieben Zwerge&quot;, SPAM)</li>
</ul>
</li>
<li>
<p>Neue Mails:</p>
<ul>
<li>T1: (&quot;Sieben Zwerge fraßen sieben Wölfe&quot;)</li>
<li>T2: (&quot;Sieben Zwerge traten sieben Ziegen&quot;)</li>
</ul>
</li>
</ul>
<p>Lernen Sie mit Hilfe der Trainingsmenge einen Naive-Bayes-Klassifikator und
wenden Sie diesen auf die beiden Test-Dokumente an.</p>
<h2 id="naive-bayes">Naive Bayes</h2>
<ul>
<li>
<p>Verallgemeinerte Bayes Regel
<span class="math align-center">$$
    P(H|D_1, \ldots, D_n) = \frac{P(D_1, \ldots, D_n | H)P(H)}{P(D_1, \ldots, D_n)}
    $$</span></p>
</li>
<li>
<p>Annahme: <span class="math align-center">$D_i$</span> sind bedingt unabhängig
<span class="math align-center">$$
    P(D_1, \ldots, D_n | H) = P(D_1 | H) \cdot \ldots \cdot P(D_n | H) = \prod_i P(D_i | H)
    $$</span></p>
</li>
<li>
<p>Beobachtung: <span class="math align-center">$P(D_1, \ldots, D_n)$</span> für alle Hypothesen <span class="math align-center">$h \in H$</span> gleich</p>
</li>
<li>
<p><strong>Naive Bayes Klassifikator</strong> bzw. <strong>MAP</strong> (&quot;Maximum a Posteriori&quot;)
<span class="math align-center">$$
    h_{MAP} = \operatorname{argmax}_{h \in H} P(h | D_1, \ldots, D_n)
    = \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i | h)
    $$</span></p>
<p>Naive Bayes: Wähle die plausibelste Hypothese, die von den Daten
unterstützt wird.</p>
</li>
</ul>
<h2 id="bayessches-lernen">Bayes'sches Lernen</h2>
<span class="math align-center">$$
h_{MAP} = \operatorname{argmax}_{h \in H} P(h | D_1, \ldots, D_n)
= \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i | h)
$$</span>
<p><strong>Training</strong>: Bestimme die Wahrscheinlichkeiten aus Trainingsdaten <span class="math align-center">$\mathbf{S}$</span></p>
<ul>
<li>Für jede Klasse <span class="math align-center">$h$</span>:
<ul>
<li>Schätze <span class="math align-center">$P(h) = \dfrac{|S(h)|}{|S|}$</span></li>
<li>Für jedes Attribut <span class="math align-center">$D_i$</span> und jede Ausprägung <span class="math align-center">$x \in D_i$</span>:
Schätze <span class="math align-center">$P(D_i=x | h) = \dfrac{|S_{D_i}(x) \cap S(h)|}{|S(h)|}$</span></li>
</ul>
</li>
</ul>
<p><strong>Klassifikation</strong>: Wähle wahrscheinlichste Klasse <span class="math align-center">$h_{MAP}$</span> für Vektor <span class="math align-center">$\mathbf{x}$</span></p>
<ul>
<li><span class="math align-center">$h_{MAP} = \operatorname{argmax}_{h \in H} P(h) \prod_{x \in \mathbf{x}} P(x | h)$</span></li>
</ul>
<h2 id="beispiel-klassifikation-mit-nb">Beispiel Klassifikation mit NB</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Nase läuft</th>
          <th style="text-align: left">Husten</th>
          <th style="text-align: left">Gerötete Haut</th>
          <th style="text-align: left">Fieber</th>
          <th style="text-align: left">Klasse</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">krank</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">krank</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">1</td>
          <td style="text-align: left">krank</td>
      </tr>
      <tr>
          <td style="text-align: left">1</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">gesund</td>
      </tr>
      <tr>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">0</td>
          <td style="text-align: left">gesund</td>
      </tr>
  </tbody>
</table>
<ul>
<li>Eingabe: Person mit Husten und Fieber</li>
</ul>
<p>Gesucht: <span class="math align-center">$P(\text{krank})$</span>, <span class="math align-center">$P(\text{gesund})$</span>, <span class="math align-center">$P(\text{Nase=0}|\text{krank})$</span>,
<span class="math align-center">$P(\text{Nase=0}|\text{gesund})$</span>, ...</p>
<p>Wähle Klasse
<span class="math align-center">$$
\begin{array}{rl}
h_{MAP} = \operatorname{argmax}_{h \in \lbrace \text{gesund, krank} \rbrace} & P(h) \cdot P(\text{Nase=0}|h) \cdot P(\text{Husten=1}|h) \\
    & \cdot P(\text{Haut=0}|h) \cdot P(\text{Fieber=1}|h)
\end{array}
$$</span></p>
<p><strong>Ergebnis</strong>: (nur die für den zu klassifizierenden Beispiel-Vektor nötigen
Werte, die restlichen müssten aber auch beim &quot;Training&quot; berechnet werden!)</p>
<pre><code>P(gesund) = 2/5 = 0.4
P(krank)  = 3/5 = 0.6

P(Nase=0 | gesund) = 1/2 = 0.5
P(Nase=0 | krank)  = 1/3 = 0.333

P(Husten=1 | gesund) = 0/2 = 0
P(Husten=1 | krank)  = 2/3 = 0.667

P(Haut=0 | gesund) = 2/2 = 1
P(Haut=0 | krank)  = 1/3 = 0.333

P(Fieber=1 | gesund) = 0/2 = 0
P(Fieber=1 | krank)  = 1/3 = 0.333

h = gesund: P(gesund) * P(Nase=0 | gesund) * P(Husten=1 | gesund) * P(Haut=0 | gesund) * P(Fieber=1 | gesund) = 0.4*0.5*0*1*0              = 0
h = krank:  P(krank)  * P(Nase=0 | krank)  * P(Husten=1 | krank)  * P(Haut=0 | krank)  * P(Fieber=1 | krank)  = 0.6*0.333*0.667*0.33*0.333 = 0.015
</code></pre>
<p>=&gt; Klasse &quot;krank&quot; gewinnt ...</p>
<h2 id="textklassifikation-mit-nb-1">Textklassifikation mit NB</h2>
<ul>
<li>
<p>Texte als Trainingsmenge:</p>
<ul>
<li>Text zerlegen in Terme (Wörter, sonstige relevante Token)</li>
<li>ggf. Entfernen von Stoppwörtern (beispielsweise Artikel u.ä.)</li>
<li>ggf. Stemming und Lemmatisierung für restliche Terme</li>
<li>ggf. weitere Vorverarbeitungsschritte (Groß-Klein-Schreibung, ...)</li>
<li>Terme zusammenfassen als Menge: <em>&quot;Bag of Words&quot;</em> (mit Häufigkeit)</li>
</ul>
</li>
<li>
<p>Naive Bayes &quot;trainieren&quot;:</p>
<ul>
<li>
<p>A-priori-Wahrscheinlichkeit der Klassen:
<span class="math align-center">$P(c) = \dfrac{N_c}{N} = \dfrac{\text{Anzahl Dokumente in Klasse c}}{\text{Anzahl Dokumente}}$</span></p>
</li>
<li>
<p>Likelihood der Daten (Terme):</p>
<ul>
<li>
<p><span class="math align-center">$P(t|c) = \dfrac{\operatorname{count}(t,c)}{\sum_{v \in V} \operatorname{count}(v,c)}$</span>
mit <span class="math align-center">$\operatorname{count}(t,c)$</span> Anzahl der Vorkommen von Term <span class="math align-center">$t$</span> in allen Dokumenten
der Klasse <span class="math align-center">$c$</span> und <span class="math align-center">$V$</span> die Vereinigung aller Terme aller Dokumente
(als Menge)</p>
</li>
<li>
<p>Variante mit Laplace-Glättung (s.u.):
<span class="math align-center">$P(t|c) = \dfrac{\operatorname{count}(t,c) + 1}{\sum_{v \in V} \operatorname{count}(v,c) + |V|}$</span></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="naivität-im-naive-bayes">Naivität im Naive Bayes</h2>
<ul>
<li>
<p>Unabhängigkeit der Attribute oft nicht gegeben</p>
<p>=&gt; <span class="math align-center">$P(D_1, \ldots, D_n | H) \ne \prod_i P(D_i | H)$</span></p>
</li>
<li>
<p>A-posteriori-Wahrscheinlichkeiten oft unrealistisch nah an 1 oder 0</p>
</li>
<li>
<p>Praxis: Dennoch häufig sehr gute Ergebnisse</p>
<p>Wichtig: Solange die <strong>Maximierung</strong> über alle Hypothesen die selben Ergebnisse
liefert, müssen die konkreten Schätzungen/Werte nicht exakt stimmen ...</p>
</li>
</ul>
<p>Wenn Attribute nicht (bedingt) unabhängig sind, kann sich der NB verschätzen,
d.h. es kommt dann u.U. zu einer höheren Fehlerrate, da bestimmte Eigenschaften
in der Trainingsmenge zu hoch gewichtet werden.</p>
<h2 id="laplace-schätzer">Laplace-Schätzer</h2>
<ul>
<li>
<p>Problem: Attribut-Ausprägung für bestimmte Klasse nicht in Trainingsmenge:</p>
<ul>
<li>=&gt; Bedingte Wahrscheinlichkeit ist 0</li>
<li>=&gt; Produkt gleich 0</li>
</ul>
</li>
<li>
<p>Lösung: &quot;Laplace-Schätzer&quot; (auch &quot;Laplace-Glättung&quot;)</p>
<p>Statt <span class="math align-center">$P(D_i=x | h) = \dfrac{|S_{D_i}(x) \cap S(h)|}{|S(h)|}$</span></p>
<p>nutze <span class="math align-center">$P(D_i=x|h) = \dfrac{|S_{D_i}(x) \cap S(h)| + m \cdot p_i}{|S(h)| + m}$</span></p>
<ul>
<li>
<p>mit <span class="math align-center">$m$</span>: frei wählbarer Faktor, und</p>
</li>
<li>
<p><span class="math align-center">$p_i$</span>: A-priori-Wahrscheinlichkeit für <span class="math align-center">$P(D_i=x|h)$</span></p>
<p>Hintergrundwissen oder einfach <em>uniforme Verteilung der Attributwerte</em>:
<span class="math align-center">$p_i = 1/|D_i|$</span> (Wahrscheinlichkeit für eine Attributausprägung
ist 1/(Anzahl der Ausprägungen des Attributs))</p>
</li>
</ul>
<p>=&gt; &quot;virtuelle&quot; Trainingsbeispiele (<span class="math align-center">$m$</span> ist die Zahl der virtuellen Trainingsbeispiele)</p>
</li>
</ul>
<h2 id="probleme-mit-floating-point-underflow">Probleme mit Floating Point Underflow</h2>
<ul>
<li>
<p>MAP berechnet Produkt mit vielen Termen</p>
</li>
<li>
<p>Problem: Bei kleinen Zahlen kann <strong>Floating Point Underflow</strong> auftreten!</p>
</li>
<li>
<p>Lösung: Logarithmus maximieren (Produkt geht in Summe über)</p>
<p>Erinnerung: <span class="math align-center">$\log(x \cdot y) = \log(x) + \log(y)$</span> und Logarithmus streng monoton</p>
<span class="math align-center">$$
    \begin{array}{rcl}
    h_{MAP} &=& \operatorname{argmax}_{h \in H} P(h|D_1, \ldots, D_n) \\[5pt]
            &=& \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i | h) \\[5pt]
            &=& \operatorname{argmax}_{h \in H} [\log(P(h)) + \sum_i \log(P(D_i | h))]
    \end{array}
    $$</span>
</li>
</ul>
<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<ul>
<li>
<p><strong>Maximum a Posteriori</strong>
<span class="math align-center">$$
    h_{MAP} = \operatorname{argmax}_{h \in H} P(h | D_1, \ldots, D_n)
    = \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i | h)
    $$</span></p>
</li>
<li>
<p>Annahme: Klassen uniform verteilt =&gt; <span class="math align-center">$P(h_i) = P(h_j)$</span></p>
<p><strong>Maximum Likelihood</strong>
<span class="math align-center">$$
    h_{ML} = \operatorname{argmax}_{h \in H} \prod_i P(D_i | h)
    $$</span></p>
<p>=&gt; Maximiere die Likelihood der Daten</p>
</li>
</ul>
<h2 id="ausblick-kontinuierliche-attribute">Ausblick: Kontinuierliche Attribute</h2>
<p>Bisher sind wir von diskreten Attributen ausgegangen. Bei kontinuierlichen
Attributen hat man zwei Möglichkeiten:</p>
<ul>
<li>Diskretisierung der Attribute: Aufteilung in Intervalle und Bezeichnung
der Intervalle mit einem Namen</li>
<li>Einsatz einer Verteilungsannahme und deren Dichtefunktion, beispielsweise
Annahme von <strong>normalverteilten</strong> Daten mit der Dichtefunktion
<span class="math align-center">$$
    f(x) = \frac{1}{\sqrt{2 \pi \sigma}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
    $$</span>
wobei <span class="math align-center">$\mu$</span> der Mittelwert und <span class="math align-center">$\sigma^2$</span> die Varianz der Daten sind.</li>
</ul>
<h2 id="hinweis-zum-sprachgebrauch">Hinweis zum Sprachgebrauch</h2>
<p>In Abhängigkeit von der Verteilung der <span class="math align-center">$P(D_i | h)$</span> spricht man von</p>
<ul>
<li>&quot;multinominalem&quot; NB: Attribute umfassen mehrere Kategorien (verschiedene
Ausprägungen, wie im &quot;Wahlkampf&quot;-Beispiel: Attribut &quot;Bildung&quot; hat
die Ausprägungen &quot;Abitur&quot;, &quot;Bachelor&quot; und &quot;Master&quot;)</li>
<li>Bernoulli NB: Attribute sind binär (Ausprägung 0 oder 1), typischerweise
bei der Textklassifikation</li>
<li>Gauss'sches NB: Annahme einer Normalverteilung der Attribut-Ausprägungen</li>
</ul>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Klassifikation mit Naive Bayes
<ul>
<li>Annahme von Unabhängigkeit =&gt; &quot;Naive&quot; Bayes Klassifikation</li>
<li>Schätzen der bedingten Wahrscheinlichkeiten aus den Trainingsdaten</li>
<li>Klassifikation durch Nutzung der geschätzten Wahrscheinlichkeiten</li>
<li>Hinweis auf Naivität der Annahme, dennoch sehr gute Erfolge in Praxis</li>
<li>Hinweis auf Probleme mit niedrigen Wahrscheinlichkeiten</li>
</ul>
</li>
</ul>


    



    
    
        
        
        
            
        
    
    

    <div class="box notices cstyle tip">
  <div class="box-label">
    <i class="fas fa-user-check"></i> Quizzes
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/goto.php?target=tst_1106588&client_id=FH-Bielefeld' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Selbsttest Naive Bayes Klassifikation (ILIAS)</a></li></ul>
  </div>
</div>



    



    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-puzzle-piece"></i> Challenges
  </div>
  <div class="box-content">
<p><strong>Textklassifikation</strong></p>
<p>Betrachten Sie die folgenden Aussagen:</p>
<blockquote>
<ul>
<li>Patient A hat weder Husten noch Fieber und ist gesund.</li>
<li>Patient B hat Husten, aber kein Fieber und ist gesund.</li>
<li>Patient C hat keinen Husten, aber Fieber. Er ist krank.</li>
<li>Patient D hat Husten und kein Fieber und ist krank.</li>
<li>Patient E hat Husten und Fieber. Er ist krank.</li>
</ul>
</blockquote>
<p>Aufgaben:</p>
<ol>
<li>Trainieren Sie auf diesem Datensatz einen Klassifikator mit NB.</li>
<li>Ist Patient F krank? Er hat Husten, aber kein Fieber.</li>
</ol>
  </div>
</div>



    




    
    
        
        
        

        
            
            
            
        
    
    

    <div class="box notices cstyle note">
  <div class="box-label">
    <i class="fas fa-laptop-code"></i> Übungsblätter/Aufgaben
  </div>
  <div class="box-content">
<ul> <li><a href='https://www.hsbi.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nb.html' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>Übungsblatt: Naive Bayes</a></li></ul>
  </div>
</div>



    




    
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
                    
                
            
            
                
            
            
        
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            
            
                
            
            
                
                
                    
                
                
            
            
                
            
            
        
    
    

    <div class="box notices cstyle info">
  <div class="box-label">
    <i class="fas fa-book-reader"></i> Quellen
  </div>
  <div class="box-content">
<ul> <li id='id_Ertel2017'>[Ertel2017] <strong>Introduction to Artificial Intelligence</strong><br>Ertel, W., Springer, 2017. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-3-319-58487-4</a>. DOI <a href='https://doi.org/10.1007/978-3-319-58487-4' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>10.1007/978-3-319-58487-4</a>.<br><em>Abschnitt 8.7</em></li> <li id='id_Russell2020'>[Russell2020] <a href='http://aima.cs.berkeley.edu' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'><strong>Artificial Intelligence: A Modern Approach</strong></a><br>Russell, S. und Norvig, P., Pearson, 2020. ISBN <a href='https://fhb-bielefeld.digibib.net/openurl?isbn=978-0134610993' class='icon reading' target='_blank' rel='nofollow noopener noreferrer'>978-0134610993</a>.<br><em>Kapitel 12</em></li></ul>
  </div>
</div>






<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin-left: 4rem; margin-right: 4rem; margin-top: 6rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>

          </section>
          </section>
        </div>
      </main>
    </div>
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/clipboard.min.js?1737742242" defer></script>
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/perfect-scrollbar.min.js?1737742242" defer></script>
    <script>
      function useMathJax( config ){
        window.MathJax = Object.assign( window.MathJax || {}, {
          tex: {
            inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
            displayMath: [['\\[', '\\]'], ['$$', '$$']], 
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/mathjax/tex-mml-chtml.js?1737742242"></script>
    <script src="/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/theme.js?1737742242" defer></script>
  </body>
</html>
