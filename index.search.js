var relearn_search_index = [
  {
    "breadcrumb": "",
    "content": "Quelle: \"künstliche intelligenz\" by Gerd Altmann (geralt) on Pixabay.com (Pixabay License)\nKursbeschreibung Ausgehend von den Fragen \"Was ist Intelligenz?\" und \"Was ist künstliche Intelligenz?\" werden wir uns in diesem Modul mit verschiedenen Teilgebieten der KI beschäftigen und uns anschauen, welche Methoden und Algorithmen es gibt und wie diese funktionieren. Dabei werden wir auch das Gebiet Machine Learning berühren, aber auch andere wichtige Gebiete betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen Methoden-Baukasten zur Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...\nÜberblick Modulinhalte Problemlösen Zustände, Aktionen, Problemraum Suche (blind, informiert): Breiten-, Tiefensuche, Best-First, Branch-and-Bound, A-Stern Lokale Suche: Gradientenabstieg, Genetische/Evolutionäre Algorithmen (GA/EA) Spiele: Minimax, Alpha-Beta-Pruning, Heuristiken Constraints: Backtracking, Heuristiken, Propagation, AC-3 Maschinelles Lernen Merkmalsvektor, Trainingsmenge, Trainingsfehler, Generalisierung Entscheidungsbäume: CAL2, CAL3, ID3, C4.5 Neuronale Netze Perzeptron, Lernregel Feedforward Multilayer Perzeptron (MLP), Backpropagation, Trainings- vs. Generalisierungsfehler Steuerung des Trainings: Kreuzvalidierung, Regularisierung Ausblick: Support-Vektor-Maschinen Naive Bayes Klassifikator Inferenz, Logik (entfällt im W24) Prädikatenlogik: Modellierung, semantische und formale Beweise, Unifikation, Resolution Ausblick: Anwendung in Prolog Team Carsten Gips (HSBI, Sprechstunde nach Vereinbarung) Canan Yıldız (TDU) Fulya Yenilmez (TDU) Kursformat ​ IFM 3.2 GKI (HSBI, PO23, 3. Semester) IFM 5.14 KI (HSBI, PO18, 5. Semester) INF701 KI (TDU) Vorlesung (2 SWS)\n07.10. - 24.01. Mo, 11:00 - 12:30 Uhr (DE) (online) (Flipped Classroom) Praktikum (2 SWS)\nPraktikumsgruppe 07.10. - 24.01. G1 Mo, 17:00 bis 18:30 Uhr (DE) (online) G2 Mo, 15:15 bis 16:45 Uhr (DE) (online) G3 Di, 09:45 bis 11:15 Uhr (DE) (online) Online-Sitzungen per Zoom (Zugangsdaten siehe ILIAS IFM 3.2 GKI (PO23, 3. Semester)). Sie können hierzu den Raum J101 bzw. J102 (siehe Stundenplan) nutzen.\nVorlesung (2 SWS)\n07.10. - 24.01. Mo, 11:00 - 12:30 Uhr (DE) (online) (Flipped Classroom) Praktikum (2 SWS)\nPraktikumsgruppe 07.10. - 24.01. G1 Mi, 11:30 bis 13:00 Uhr (DE) (online) G2 Mi, 14:00 bis 15:30 Uhr (DE) (online) G3 Fr, 11:30 bis 13:00 Uhr (DE) (online) Online-Sitzungen per Zoom (Zugangsdaten siehe ILIAS IFM 5.14 KI (PO18, 5. Semester)). Sie können hierzu den Raum J101 bzw. J102 (siehe Stundenplan) nutzen.\nVorlesung (2 SWS)\n30.09. - 25.10. 28.10. - 15.01. Mo, 12:00 - 13:30 Uhr (TR) Mo, 13:00 - 14:30 Uhr (TR) online online Durchführung als Flipped Classroom: Sitzungen per Zoom (Zugangsdaten siehe Google Classroom)\nÜbung (2 SWS)\nÜbungsgruppe 30.09. - 15.01. G1 / G2 wird bekanntgegeben G3 / G4 wird bekanntgegeben online Sitzungen per Google Meet (Zugangsdaten siehe Google Classroom)\nFahrplan ​ IFM 3.2 GKI (HSBI, PO23, 3. Semester) IFM 5.14 KI (HSBI, PO18, 5. Semester) INF701 KI (TDU) News 24.01.25 Planung Klausur GKI (04.02.25)\nDie Klausur in GKI wird am Dienstag, den 04.02.25 von 10:00 bis ca. 11:30 Uhr im B40 stattfinden.\n09.12.24 Am 16.12. wird es in der Vorlesung eine Einführung ins Deep Learning geben sowie die bereits angekündigte offene Sprechstunde.\nDas Thema Deep Learning ist nicht prüfungsrelevant (aber trotzdem spannend ;).\n04.12.24 Da die Projektwoche (16.-20.12.2024) mangels Interesse nicht stattfinden wird, biete ich eine zusätzliche Sprechstunde im Vorlesungsslot am 16.12. an.\n29.11.24 Projektwoche: Vom 16.-20.12.2024 findet unsere gemeinsame Projektwoche des ersten und dritten Semesters statt. Sie können in einem Team an spannenden Aufgaben arbeiten. In den teilnehmenden Veranstaltungen entfallen deshalb die Vorlesungen und Übungen - wir nehmen mit \"Grundlagen der KI\" (GKI) an der Projektwoche teil :-)\nWenn Sie in Ihr Projekt nachweislich Inhalte aus \"Grundlagen der KI\" (GKI) einbringen, zählt das wie ein zusätzliches Übungsblatt. Sie haben also eine zusätzliche Chance für das Testat (6 aus 11 Blättern) ...\nMelden Sie sich bis zum 04.12. unter https://www.hsbi.de/elearning/goto.php?target=crs_1460449 an. Dort finden Sie auch weitere Informationen zum Ablauf.\nHier finden Sie einen abonnierbaren Google Kalender IFM 3.2 GKI (PO23, 3. Semester) mit allen Terminen der Veranstaltung zum Einbinden in Ihre Kalender-App.\nAbgabe der Übungsblätter jeweils Montag bis 11:00 Uhr im ILIAS. Vorstellung der Lösung im jeweiligen Praktikum in der Abgabewoche.\nMonat Woche Vorlesung Lead Abgabe Aufgabenblatt Vorstellung Praktikum Oktober 41 07.: Orga (Zoom); Einführung KI, Problemlösen; Machine Learning 101, Perzeptron Carsten, Canan 42 14.: Lineare Regression Canan 14.: Blatt: Perzeptron 14. / 15. 43 21.: Logistische Regression Canan 44 28.: Overfitting, Multilayer Perceptron Canan 28.: Blatt: Regression 28. / 29. November 45 04.: Backpropagation Canan 04.: Blatt: MLP 04. / 05. 46 11.: Training \u0026 Testing, Performanzanalyse Canan 11.: Blatt: Backpropagation 11. / 12. 47 18.: Machine Learning 101, CAL2, Pruning, CAL3, Entropie, ID3 und C4.5 Carsten 48 25.: Tiefensuche, Breitensuche, Branch-and-Bound, Best First, A-Stern Carsten 25.: Blatt: DTL 25. / 26. Dezember 49 02.: Gradientensuche, Simulated Annealing; Intro EA/GA, Genetische Algorithmen Carsten 02.: Blatt: Suche 02. / 03. 50 09.: Optimale Spiele, Games mit Minimax, Minimax und Heuristiken, Alpha-Beta-Pruning Carsten 09.: Blatt: EA/GA 09. / 10. 51 16.: Projektwoche Semester 1+3 Intro Deep Learning und offene Sprechstunde Canan, Carsten 52 23.: Weihnachtspause 01 30.: Weihnachtspause Januar 02 06.: Einführung Constraints, Lösen von diskreten CSP, CSP und Heuristiken, Kantenkonsistenz und AC-3 Carsten 06.: Blatt: Games 06. / 07. 03 13.: Wahrscheinlichkeitstheorie, Naive Bayes Carsten 13.: Blatt: CSP 13. / 14. 04 20.: Rückblick (Zoom), Prüfungsvorbereitung HSBI Carsten 20.: Blatt: Naive Bayes 20. / 21. (Prüfungsphase I) Klausur: Di, 04. Feb 2025, 10-18 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher) (Prüfungsphase II) Klausur: Di, 01. Apr 2025, 10-16 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher) News 24.01.25 Planung Klausur KI (04.02.25)\nDie Klausur in KI wird am Dienstag, den 04.02.25 von 12:00 bis ca. 13:30 Uhr im B40 stattfinden.\n09.12.24 Am 16.12. wird es in der Vorlesung eine Einführung ins Deep Learning geben sowie die bereits angekündigte offene Sprechstunde.\nDas Thema Deep Learning ist nicht prüfungsrelevant (aber trotzdem spannend ;).\nHier finden Sie einen abonnierbaren Google Kalender IFM 5.14 KI (PO18, 5. Semester) mit allen Terminen der Veranstaltung zum Einbinden in Ihre Kalender-App.\nAbgabe der Übungsblätter jeweils Mittwoch bis 11:00 Uhr im ILIAS. Vorstellung der Lösung im jeweiligen Praktikum in der Abgabewoche.\nMonat Woche Vorlesung Lead Abgabe Aufgabenblatt Vorstellung Praktikum Oktober 41 07.: Orga (Zoom); Einführung KI, Problemlösen; Machine Learning 101, Perzeptron Carsten, Canan 42 14.: Lineare Regression Canan 16.: Blatt: Perzeptron 16. / 18. 43 21.: Logistische Regression Canan 44 28.: Overfitting, Multilayer Perceptron Canan November 45 04.: Backpropagation Canan 06.: Blatt: Regression 06. / 08. 46 11.: Training \u0026 Testing, Performanzanalyse Canan 13.: Blatt: MLP 13. / 15. 47 18.: Machine Learning 101, CAL2, Pruning, CAL3, Entropie, ID3 und C4.5 Carsten 20.: Blatt: Backpropagation 20. / 22. 48 25.: Tiefensuche, Breitensuche, Branch-and-Bound, Best First, A-Stern Carsten 27.: Blatt: DTL 27. / 29. Dezember 49 02.: Gradientensuche, Simulated Annealing; Intro EA/GA, Genetische Algorithmen Carsten 04.: Blatt: Suche 04. / 06. 50 09.: Optimale Spiele, Games mit Minimax, Minimax und Heuristiken, Alpha-Beta-Pruning Carsten 11.: Blatt: EA/GA 11. / 13. 51 16.: Projektwoche Semester 1+3 Intro Deep Learning und offene Sprechstunde Canan, Carsten 18.: Blatt: Games 18. / 20. 52 23.: Weihnachtspause 01 30.: Weihnachtspause Januar 02 06.: Einführung Constraints, Lösen von diskreten CSP, CSP und Heuristiken, Kantenkonsistenz und AC-3 Carsten 03 13.: Wahrscheinlichkeitstheorie, Naive Bayes Carsten 15.: Blatt: CSP 15. / 17. 04 20.: Rückblick (Zoom), Prüfungsvorbereitung HSBI Carsten 22.: Blatt: Naive Bayes 22. / 24. (Prüfungsphase I) Klausur: Di, 04. Feb 2025, 10-18 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher) (Prüfungsphase II) Klausur: Di, 01. Apr 2025, 10-16 Uhr (je Klausur 90', Vergabe ca. 2 Wochen vorher) KW Monat Tag Vorlesung Lead Abgabe Übung 40 Sep 30. Orga (Zoom); Einführung KI, Problemlösen Canan, Carsten 41 Okt 07. (12:30 - 13:30 Uhr TR) Machine Learning 101, Perzeptron Canan 42 14. Lineare Regression Canan Blatt: Perzeptron 43 21. Logistische Regression Canan 44 28. Overfitting, Multilayer Perceptron Canan Blatt: Regression 45 Nov 04. Backpropagation Canan Blatt: MLP 46 11. Training \u0026 Testing, Performanzanalyse Canan Blatt: Backpropagation 47 18. Zwischenprüfung 48 25. Tiefensuche, Breitensuche, Branch-and-Bound, Best First, A-Stern Carsten 49 Dez 02. Gradientensuche, Simulated Annealing; Intro EA/GA, Genetische Algorithmen Carsten Blatt: Suche 50 09. Optimale Spiele, Games mit Minimax, Minimax und Heuristiken, Alpha-Beta-Pruning Carsten Blatt: EA/GA 51 16. Vorschau Deep Learning (CNN, RNN) Canan Blatt: Games 52 23. (Google Meet) Prüfungsvorbereitung TDU Canan 01 30. (KEINE Sprechstunde) Machine Learning 101, CAL2, Pruning, CAL3, Entropie, ID3 und C4.5 02 Jan 06. Einführung Constraints, Lösen von diskreten CSP, CSP und Heuristiken, Kantenkonsistenz und AC-3 Carsten Blatt: DTL 03 13. Wahrscheinlichkeitstheorie, Naive Bayes Carsten Blatt: CSP Prüfungsform, Note und Credits ​ IFM 3.2 GKI (HSBI, PO23, 3. Semester) IFM 5.14 KI (HSBI, PO18, 5. Semester) INF701 KI (TDU) Klausur plus Testat, 5 ECTS\nTestat: Vergabe der Credit-Points\nKriterien: Mindestens 6 der 10 Aufgabenblätter erfolgreich bearbeitet.\n(\"erfolgreich bearbeitet\": Bearbeitung individuell (also in 1er Teams), je mindestens 60% bearbeitet, fristgerechte Abgabe der Lösungen im ILIAS, Vorstellung der Lösungen im Praktikum)\nKlausur: =\u003e Modulnote\nSchriftliche Prüfung (\"Klausur\") am Ende des Semesters (in beiden Prüfungszeiträumen; Prüfungsvorbereitung HSBI).\nDie nächste Klausur für \"Grundlagen der KI\" wird am Dienstag, 04. Februar 2025 angeboten. Die Klausur wird als digitale Klausur auf dem Prüfungs-ILIAS der HSBI in Präsenz vor Ort in Minden im Raum B40 durchgeführt. Die Prüfung für GKI beginnt um 10:00 Uhr und dauert 90 Minuten. Ein DIN-A4-Zettel ist als Hilfsmittel zugelassen. Der geprüfte Stoff bezieht sich auf den zuletzt durchgeführten Kurs (Winter 2024/25). Weitere Informationen siehe Prüfungsvorbereitung HSBI.\nKlausur plus Testat, 5 ECTS\nTestat: Vergabe der Credit-Points\nKriterien: Mindestens 6 der 10 Aufgabenblätter erfolgreich bearbeitet.\n(\"erfolgreich bearbeitet\": Bearbeitung individuell (also in 1er Teams), je mindestens 60% bearbeitet, fristgerechte Abgabe der Lösungen im ILIAS, Vorstellung der Lösungen im Praktikum)\nKlausur: =\u003e Modulnote\nSchriftliche Prüfung (\"Klausur\") am Ende des Semesters (in beiden Prüfungszeiträumen; Prüfungsvorbereitung HSBI).\nDie nächste Klausur für \"Künstliche Intelligenz\" wird am Dienstag, 04. Februar 2025 angeboten. Die Klausur wird als digitale Klausur auf dem Prüfungs-ILIAS der HSBI in Präsenz vor Ort in Minden im Raum B40 durchgeführt. Die Prüfung für KI beginnt um 12:00 Uhr und dauert 90 Minuten. Ein DIN-A4-Zettel ist als Hilfsmittel zugelassen. Der geprüfte Stoff bezieht sich auf den zuletzt durchgeführten Kurs (Winter 2024/25). Weitere Informationen siehe Prüfungsvorbereitung HSBI.\nPrüfung Gewicht Zwischenprüfung 40 % Endprüfung 60 % Übung 10 % Bonus für Endprüfung Wenn in der Endprüfung die 40 Punkte Mindestgrenze erreicht wird (Prüfungsnote ≥40), werden 10 % der Übungspunkte als Bonus zu der Prüfungsnote hinzugefügt.\nFür die Vergabe von Übungspunkten ist eine erfolgreiche Teilnahme an der Übung erforderlich. Für Details siehe Prüfung \u0026 Noten @TDU.\nMaterialien \"Artificial Intelligence: A Modern Approach\" (AIMA). Russell, S. und Norvig, P., Pearson, 2020. ISBN 978-0134610993. \"Introduction to Artificial Intelligence\". Ertel, W., Springer, 2017. ISBN 978-3-319-58487-4. DOI 10.1007/978-3-319-58487-4. \"An Introduction to Machine Learning\". Kubat, M., Springer, 2017. ISBN 978-3-319-63913-0. DOI 10.1007/978-3-319-63913-0. Förderungen und Kooperationen Kooperation zw. HSBI und TDU Über das Projekt \"Digital Mobil @ FH Bielefeld\" der Fachhochschule Bielefeld (HSBI) ist im Sommer 2020 eine Kooperation mit der Türkisch-Deutschen Universität in Istanbul (TDU) im Modul \"Künstliche Intelligenz\" gestartet.\nWir werden in diesem Semester die Vorlesungen und auch die Übungen/Praktika wieder im Co-Teaching durchführen. In den Zoom-Sitzungen nehmen deshalb alle Studierenden gemeinsam (TDU und HSBI) teil.\nKooperation mit dem DigikoS-Projekt Diese Vorlesung wurde zudem vom Projekt \"Digitalbaukasten für kompetenzorientiertes Selbststudium\" (DigikoS) unterstützt. Ein vom DigikoS-Projekt ausgebildeter Digital Learning Scout hat insbesondere die Koordination der digitalen Gruppenarbeiten, des Peer-Feedbacks und der Postersessions in ILIAS technisch und inhaltlich begleitet. DigikoS wird als Verbundprojekt von der Stiftung Innovation in der Hochschullehre gefördert.",
    "description": "Quelle: \"künstliche intelligenz\" by Gerd Altmann (geralt) on Pixabay.com (Pixabay License)\nKursbeschreibung Ausgehend von den Fragen \"Was ist Intelligenz?\" und \"Was ist künstliche Intelligenz?\" werden wir uns in diesem Modul mit verschiedenen Teilgebieten der KI beschäftigen und uns anschauen, welche Methoden und Algorithmen es gibt und wie diese funktionieren. Dabei werden wir auch das Gebiet Machine Learning berühren, aber auch andere wichtige Gebiete betrachten. Sie erarbeiten sich im Laufe der Veranstaltung einen Methoden-Baukasten zur Lösung unterschiedlichster Probleme und erwerben ein grundlegendes Verständnis für die Anwendung in Spielen, Navigation, Planung, smarten Assistenten, autonomen Fahrzeugen, ...",
    "tags": [],
    "title": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Was ist Intelligenz? Was ist künstliche Intelligenz? Woran kann man das erkennen? Wie kann man eine Welt (ein Problem) modellieren, um es dann anschließend lösen zu können?\nIntro: Was ist Künstliche Intelligenz? Problemlösen",
    "description": "Was ist Intelligenz? Was ist künstliche Intelligenz? Woran kann man das erkennen? Wie kann man eine Welt (ein Problem) modellieren, um es dann anschließend lösen zu können?\nIntro: Was ist Künstliche Intelligenz? Problemlösen",
    "tags": [],
    "title": "Einführung KI",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Einführung KI",
    "content": "Was ist (künstliche) Intelligenz? Quelle: AvB - RoboCup 2013 - Eindhoven by RoboCup2013 on Flickr.com (CC BY 2.0)\nWas ist (künstliche) Intelligenz? Ist Commander Data intelligent? Woran erkennen Sie das? Definition Intelligenz Intelligenz (von lat. intellegere \"verstehen\", wörtlich \"wählen zwischen ...\" von lat. inter \"zwischen\" und legere \"lesen, wählen\") ist in der Psychologie ein Sammelbegriff für die kognitive Leistungsfähigkeit des Menschen. Da einzelne kognitive Fähigkeiten unterschiedlich stark ausgeprägt sein können und keine Einigkeit besteht, wie diese zu bestimmen und zu unterscheiden sind, gibt es keine allgemeingültige Definition der Intelligenz.\nQuelle: \"Intelligenz\" by Cumtempore and others on Wikipedia (CC BY-SA 3.0)\nDas ist spannend: Es gibt keine allgemeingültige Definition für den Begriff \"Intelligenz\". Damit wird es schwierig, auch \"Künstliche Intelligenz\" zu definieren ...\nAber wir können aus dieser Definition auf Wikipedia mitnehmen, dass es um kognitive Fähigkeiten geht. Verstehen und im weiteren Sinne Denken sind bereits im Begriff enthalten und damit auch Teil der kognitiven Fähigkeiten.\nSchauen wir uns nun noch die Definition von \"kognitiven Fähigkeiten\" genauer an.\nZu den kognitiven Fähigkeiten eines Menschen zählen die Wahrnehmung, die Aufmerksamkeit, die Erinnerung, das Lernen, das Problemlösen, die Kreativität, das Planen, die Orientierung, die Imagination, die Argumentation, die Introspektion, der Wille, das Glauben und einige mehr. Auch Emotionen haben einen wesentlichen kognitiven Anteil.\nQuelle: \"Kognition\" by Arbraxan and others on Wikipedia (CC BY-SA 3.0)\nZu den kognitiven Fähigkeiten und damit zur Intelligenz zählen also eine Reihe von Fähigkeiten, die man Menschen im allgemeinen zuschreibt. Lernen und Problemlösen und Planen sind Dinge, die vermutlich jeder direkt mit dem Begriff Intelligenz verbindet. Interessanterweise gehören auf auch Aufmerksamkeit und Wahrnehmung und Orientierung mit dazu -- Fähigkeiten, die beispielsweise in der Robotik sehr wichtig sind. Kreativität und Vorstellung zählen aber auch mit in den Bereich der kognitiven Fähigkeiten und damit zum Begriff Intelligenz. In der KI werden diese Gebiete zunehmend interessant, etwa beim Komponieren von Musik und beim Erzeugen von Bildern oder Texten. Mit Emotionen beschäftigt sich die KI-Forschung aktuell nur am Rande, etwa bei der Erkennung von Emotionen in Texten. Andere Gebiete der kognitiven Fähigkeiten wie Glaube und Wille spielen derzeit keine Rolle in der KI.\nVersuch einer Definition für \"KI\" Ziel der KI ist es, Maschinen zu entwickeln, die sich verhalten, als verfügten sie über Intelligenz.\n-- John McCarthy (1955)\nEinwand: Braitenberg-Vehikel zeigen so etwas wie \"intelligentes\" Verhalten, sind aber noch lange nicht intelligent! Bezieht sich nur auf Verhalten!\nKI ist die Fähigkeit digitaler Computer oder computergesteuerter Roboter, Aufgaben zu lösen, die normalerweise mit den höheren intellektuellen Verarbeitungsfähigkeiten von Menschen in Verbindung gebracht werden ...\n-- Encyclopedia Britannica\nEinwand: Dann wäre aber auch Auswendig-Lernen oder das Multiplizieren langer Zahlen als intelligent zu betrachten! Bezieht sich vor allem auf \"Denken\"!\nArtificial Intelligence is the study of how to make computers do things at which, at the moment, people are better.\n-- Elaine Rich (\"Artificial Intelligence\", McGraw-Hill, 1983)\nQuelle: nach [Ertel2017, pp. 1-3]\nDazu gehört auch\nAnpassung an sich verändernde Situationen Erkennung von Bildern und Gesichtern und Emotionen Erkennung von Sprache Umgang mit kontextbehafteten, unvollständigen Informationen Ausräumen von Geschirrspülern ;-) Über Emotionen und Empathie verfügen KI-Grundverordnung der EU Die EU hat am 13. Juni 2024 die sogenannte \"KI-Verordnung\" verabschiedet (\"VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES\", Document 32024R1689: Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz) (Text von Bedeutung für den EWR)).\nDort finden Sie unter Artikel 3 \"Begriffsbestimmungen\" unter Absatz 1 eine Begriffsdefinition. Ein \"KI-System\" wird darin als ein maschinengestütztes System definiert, in irgendeiner Art für einen autonomen Betrieb ausgelegt ist und eine gewisse Anpassungsfähigkeit haben kann. Zusätzlich soll das \"KI-System\" aus den Eingaben Vorhersagen und Entscheidungen zu treffen oder auch Inhalte erzeugen und mit der physischen oder digitalen Umwelt interagieren können. Quelle: VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES, Art. 3 Abs. 1\nInteressant ist, dass dabei nicht explizit auf Softwaresysteme eingegangen wird. Später im Text finden sich Hinweise, dass sich ein KI-System vermutlich als Software repräsentiert, auch kommen Modelle und Daten vor. Bei den Modellen kommt der Begriff des Lernens vor, in allen derzeit üblichen Varianten (überwachtes Lernen, unüberwachtes Lernen, Reinforcement Learning). Große Teile des Dokuments beschäftigen sich mit weitreichenden Bestimmungen für Akteure, die ein KI-System zur Verfügung stellen wollen.\nLesen Sie selbst: VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES.\nAlan Turing 1950: Turing-Test (begründet Zweige der KI) Wann verhält sich eine Maschine intelligent? Quelle: Turing Test version 3.png by Bilby on Wikimedia Commons (Public Domain)\nZum Bestehen des Turing-Tests ist (u.a.) erforderlich:\nWissensrepräsentation: Speichern des gesammelten Wissens =\u003e Wissensbasierte Systeme Logisches Schließen: Beantworten von Fragen mit Hilfe des vorhandenen Wissens =\u003e Logik, Resolution Maschinelles Lernen: Anpassung an veränderliches Umfeld =\u003e Musteranalyse und Mustererkennung und Mustervorhersage Verarbeitung natürlicher Sprache: Erfolgreiche Kommunikation, beispielsweise in Englisch =\u003e NLP \"Totaler Turing-Test\": zusätzlich Computer Vision (Erkennen von Objekten) und Robotik (Manipulation von Objekten)\nDamit begründet der Turing-Test die Gebiete der KI.\nProblem: Der Turing-Test ist nicht reproduzierbar, nicht konstruktiv und nicht mathematisch analysierbar ... Außerdem wird durch den Turing-Test im Wesentlichen nur Funktionalität geprüft, nicht ob Intention oder Bewusstsein vorhanden ist.\nStarke vs. schwache KI \"Schwache KI\" Simulation intelligenten Verhaltens Lösung konkreter Probleme Adaptives Verhalten (\"Lernen\") Umgang mit Unsicherheit und unvollständigen Informationen \"Starke KI\" Eigenschaften der \"schwachen KI\" Intelligenz nach menschlichen Maßstäben (auf \"Augenhöhe\") Bewusstsein Emotionen (?) Empathie Frage Wie würden Sie Systeme wie ChatGPT einordnen? Woran machen Sie das fest?\nTypische Ansätze in der KI Untersuchung von\nVerhalten vs. Denken Rational vs. menschlich Motivation dabei\nMenschliche Intelligenz verstehen Intelligente/intelligent wirkende Systeme bauen Damit erhält man vier Kombinationen:\nMenschliches Denken Rationales Denken Rationales Verhalten Menschliches Verhalten Menschliches Denken: Kognitive Modellierung Welche kognitiven Fähigkeiten sind für intelligentes Verhalten nötig? Wie laufen Denkprozesse im Gehirn ab? Erfordert Theorien über interne Aktivitäten des Gehirns Ansätze: top-down: Vorhersage und Test von menschlichem Verhalten bottom-up: Auswertung neurobiologischer Daten Wissenschaftszweige: Kognitionswissenschaft (Verbindung der Computermodelle der KI mit den Experimenten und Theorien der Psychologie), Neurobiologie/-informatik Neuronale bzw. Konnektionistische KI Die Schule der sogenannten \"Neuronalen bzw. Konnektionistischen KI\" verfolgt den Ansatz, die biologischen Prozesse im Gehirn zu verstehen und nachzubauen (bottom-up Ansatz) und auf reale Probleme anzuwenden.\nDank massiver Rechenleistung, riesigen Datenmengen und geeigneten Modellen (Deep Learning) kann diese Tradition aktuell große Erfolge vorzeigen.\nRationales Denken: Aristoteles: Was sind korrekte Argumente und Denkprozesse? =\u003e Wie sollen wir denken? Beispiel:\nFakt: Sokrates ist ein Mensch. Fakt: Alle Menschen sind sterblich. Folgerung: Sokrates ist sterblich. Formalisierte Problembeschreibung Problemlösung durch logische Prozesse Verbindung von moderner KI zur Mathematik und Philosophie Symbolische KI Die Schule der sogenannten \"Symbolische KI\" verfolgt den top-down-Ansatz, mit Hilfe formaler Beweise Schlüsse zu ziehen und damit Fragen zu beantworten bzw. Probleme zu lösen. Dabei wird die betrachtete \"Welt\", also Gedanken, Konzepte und Beziehungen zwischen Objekten durch Symbole und Formeln repräsentiert, ähnlich der Art und Weise, wie Menschen logisch denken und kommunizieren.\nDas Hauptproblem dieser Tradition liegt im Aufwand bei der geeigneten Formalisierung der realen Welt.\nRationales Verhalten: Das \"Richtige\" tun Das \"Richtige\": Verhalten zum Erzielen des besten (erwarteten) Ergebnisses (unter Berücksichtigung der verfügbaren Informationen)\nEin System ist rational, wenn es das seinen Kenntnissen nach \"Richtige\" macht.\nDenken ist nicht unbedingt notwendig (zb. Reflexe können auch rationales Verhalten sein)\nInteressant: \"richtige\" Handlung unter unvollständigen/unsicheren Informationen\nStatistische KI Die Schule der sogenannten \"Statistischen KI\" verfolgt einen Ansatz, der sich stark auf statistische Methoden und Modelle stützt, um Muster in Daten zu erkennen und Entscheidungen zu treffen, also um aus großen Datenmengen Erkenntnisse zu gewinnen und Vorhersagen zu treffen.\nAus der Analyse von Datenpunkten und deren Merkmalen werden Wahrscheinlichkeiten für bestimmte Ereignisse berechnet, beispielsweise in Regressionsanalysen, Klassifizierungsverfahren oder Zeitreihenanalysen.\nDiese Tradition spielt eine zentrale Rolle in zahlreichen Anwendungsbereichen wie Gesundheitswesen, Finanzen, Marketing und vielen weiteren.\nMenschliches Verhalten: Na ja, Sie wissen schon ;-) Modelle, Lernen und Vorhersagen In der Informatik allgemein und auch in der KI versuchen wir, Probleme der realen Welt mit Hilfe von künstlichen Systemen (Algorithmen, Software) zu lösen. Dafür brauchen wir zunächst ein abstraktes mathematisches Modell der Welt, in der wir uns bewegen. Das Modell sollte alle für das zu lösende Problem relevanten Aspekte der Welt repräsentieren - und möglichst nicht mehr als diese, um unnötige Komplexität zu vermeiden. Es kommt häufig vor, dass selbst die relevanten Aspekte zu umfangreich oder teilweise sogar unbekannt sind und nicht vollständig dargestellt werden können. Modelle stellen also eine Abstraktion der echten Welt dar und sind verlustbehaftet. Es gibt viele verschiedene Modelle.\nBeispiel: Wir möchten von Bielefeld nach Minden fahren. Neben den offensichtlichen Parametern (Womit wollen wir fahren? Wo genau ist der Startpunkt, wo genau der Zielpunkt? Wann wollen wir fahren?) spielen in der realen Welt unendlich viele Aspekte eine Rolle: Farben, Gerüche, Licht, Beschaffenheit der einzelnen Straßen, exakte Positionen auf der Straße/im Ort ... Sind diese wirklich relevant für dieses Problem? Am Ende wird es wichtig sein, eine abstrakte Darstellung zu finden, die irgendwie die Städte und Dörfer repräsentiert und die Verbindungen dazwischen. Und vermutlich muss ich wissen, wie lang die Strecken jeweils sind (oder wie lange ich brauche oder wieviel Geld mich das Abfahren kostet). Es scheint also so zu sein, dass eine Abstraktion des Problems als Graph sinnvoll ist: Die Knoten entsprechen den Orten, die Kanten den Straßen (oder Bahnlinien o.ä.). An den Kanten sind Kosten annotiert (Kilometer, Zeit, ...). Damit ignorieren wir die Komplexität der realen Welt und fokussieren uns auf die Aspekte, die zur Lösung des Problems wichtig sind. Behalten Sie im Gedächtnis, dass unser Modell verlustbehaftet ist und wir damit tatsächlich nur das Wegeproblem lösen können! Wenn wir Wege vergessen haben oder falsch bewertet haben, wird unser Algorithmus später möglicherweise nicht die gewünschte Lösung finden! Wir schauen uns das Thema Modellierung am Beispiel des Problemlösens und insbesondere für Suchprobleme in der Lektion Problemlösen noch genauer an.\nEin Modell kann Parameter haben. Im obigen Beispiel wären dies die Werte an den Kanten. Es kann sein, dass diese Werte nicht im Vorfeld bekannt sind, sondern aus einem Datensatz extrahiert werden müssen. Dies nennt man Lernen: Das Modell wird (besser gesagt: die Parameter des Modells werden) an das Problem angepasst. Dafür gibt es unterschiedliche Algorithmen. In der Regel benötigt man ein Ziel für den Adaptionsprozess: eine sogenannte Ziel- oder Kostenfunktion. Anpassen der Modellparameter mit Hilfe von Daten und einer Zielfunktion bedeutet auch, dass man das Ziel möglicherweise nie zu 100% erreicht, sondern nur in die Nähe kommt. Je nach Problem kann man auch nur eine Modellfamilie vorgeben und den konkreten Aufbau des Modells im Trainingsprozess erarbeiten lassen.\nWichtig: Lernen bietet sich immer dann an, wenn eine analytische Lösung nicht möglich ist (fehlende Informationen, Komplexität des Problems). Das bedeutet im Umkehrschluss aber auch: Wenn eine analytische Lösung bekannt ist (oder zu finden ist), dann gibt es keinen Grund für den Einsatz von adaptiven Systemen!\nMit dem Modell der Welt kann nun das Problem gelöst werden. Dazu wird das Modell mit Daten versorgt (im obigen Beispiel: Start und Ziel) und ein passender Algorithmus kann auf dem Modell die Lösung berechnen. Dies kann eine Vorhersage sein, welchen Weg ich nehmen soll, wie lange es dauern wird, welchen Spielzug ich als nächstes machen sollte, ob in einem Bild eine Katze zu sehen ist, ... Es könnte aber auch im Fall von sogenannten generativen Modellen ein erzeugter Text oder ein erzeugtes Bild sein.\nHinweis: In manchen Quellen wird dieser Vorgang auch \"Inferenz\" genannt. Da dieser Begriff aus der Logik stammt und mit bestimmten Prozessen zur Schlussfolgerung verbunden ist, möchte ich in diesem Skript diesen Begriff nicht für das Generieren einer Vorhersage nutzen.\nModellkomplexität In der KI werden sehr unterschiedliche Modelle betrachtet, die auch eine sehr unterschiedliche Komplexität aufweisen.\nBesonders einfache Modelle sind Modelle, die für einen Input direkt einen Output berechnen. Für jeden Input wird eine feste Berechnung durchgeführt, es erfolgt kein Backtracking und es gibt keine (inneren) Zustände. Dies ähnelt dem reflexartigen Verhalten in biologischen Vorbildern, weshalb diese Modell oft auch \"reflex-based models\" genannt werden. In diese Kategorie fallen beispielsweise lineare Regression und das Perzeptron, aber auch Modelle mit vielen Parametern wie ein Multilagen-Perzeptron (MLP) oder die daraus abgeleiteten Deep Neural Networks.\nIn der nächst komplexeren Stufe haben die Modelle einen internen Zustand (\"state-based models\"). Darüber wird ein Zustand der betrachteten Welt modelliert. Zwischen den Zuständen gibt es Übergänge (sogenannte Aktionen), so dass sich hier ein Graph aufspannt (der sogenannte Problemgraph, vgl. Problemlösen). In diese Klasse fallen die typischen Suchprobleme (wie Breitensuche, Tiefensuche, A*), aber auch Spiele mit Zufallskomponente oder mit gegnerischen Mitspielern.\nNoch eine Stufe komplexer sind Modelle mit Variablen (\"variable-based models\"). Während es bei den zustandsbasierten Modellen immer (auch) um den Weg zwischen den Zuständen geht und damit um eine prozedurale Beschreibung, wie von einem Startzustand zu einem Zielzustand zu gelangen ist, steht bei Modellen mit Variablen nur die Lösung im Vordergrund: Das Modell enthält verschiedene Variablen, denen ein passender Wert aus einem Wertebereich zugeordnet werden muss. Wie diese Belegung entsteht, ist am Ende nicht mehr so interessant. Denken Sie beispielsweise an ein Sudoku oder die Erstellung eines Stundenplans. Die Variablen sind entsprechend die einzelnen Felder, gesucht ist eine insgesamt korrekte Belegung aller Felder. In diese Klasse fallen Constraint Satisfaction Probleme (CSP), aber auch Bayes'sche Netze und die sogenannte “lokale Suche”.\nAuf der höchsten Komplexitätsstufe stehen logische Modelle. Hier wird Wissen über die Welt in Form von Fakten und Regeln modelliert, und über eine entsprechende Anfrage wird daraus mit Hilfe von formal definierten Beweisen eine korrekte Antwort generiert. Dies nennt man auch \"Inferenz\". Hier kommt beispielsweise das Prädikatenkalkül zum Einsatz oder die Programmiersprache Prolog.\nKurzer Geschichtsüberblick -- Wichtigste Meilensteine 1943: McCulloch/Pitts: binäres Modell eines Neurons 1950: Turing-Test 1956: Dartmouth Workshop (Minsky, McCarthy, ...) -- McCarthy schlägt den Begriff \"Artificial Intelligence\" vor 1960er: Symbolische KI (Theorembeweiser), Blockwelt, LISP 1970er: Wissensbasierte System (Expertensysteme) 1980er: zunächst kommerzieller Erfolg der Expertensysteme, später Niedergang (\"KI-Eiszeit\"); Zuwendung zu Neuronalen Netzen 1990er: Formalisierung der KI-Methoden, Einführung probabilistischer Methoden (Bayes'sches Schließen), verstärkte mathematische Fundierung heute: friedliche Koexistenz verschiedener Paradigmen und Methoden; Rückkehr zu \"Human-Level AI\" (McCarthy, Minsky, Nilsson, Winston); Rückkehr zu Neuronalen Netzen (CNN/RNN) Siehe auch Abbildung 1.3 in [Ertel2017, S.8] ...\nWrap-Up Definition von \"Intelligenz\" nicht ganz einfach Dimensionen: Denken vs. Verhalten, menschlich vs. rational Ziele der KI: Verständnis menschlicher Fähigkeiten, Übertragen auf künstliche Systeme Schauen Sie sich zur Einführung auch gern das YouTube-Video Overview Artificial Intelligence Course | Stanford CS221 an. (Vorsicht: Das ist recht lang.)",
    "description": "Was ist (künstliche) Intelligenz? Quelle: AvB - RoboCup 2013 - Eindhoven by RoboCup2013 on Flickr.com (CC BY 2.0)\nWas ist (künstliche) Intelligenz? Ist Commander Data intelligent? Woran erkennen Sie das? Definition Intelligenz Intelligenz (von lat. intellegere \"verstehen\", wörtlich \"wählen zwischen ...\" von lat. inter \"zwischen\" und legere \"lesen, wählen\") ist in der Psychologie ein Sammelbegriff für die kognitive Leistungsfähigkeit des Menschen. Da einzelne kognitive Fähigkeiten unterschiedlich stark ausgeprägt sein können und keine Einigkeit besteht, wie diese zu bestimmen und zu unterscheiden sind, gibt es keine allgemeingültige Definition der Intelligenz.",
    "tags": [],
    "title": "Intro: Was ist Künstliche Intelligenz?",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro1-overview.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Einführung KI",
    "content": "Motivation: Roboter in einer Bibliothek Aktionen:\nRight (R) Left (L) Take (T) Drop (D) Wahrnehmungen:\nIn welchem Raum bin ich? Habe ich das Buch? Aufgabe: Das Buch aus der Bibliothek holen und in den Kopiererraum bringen.\nBemerkungen zur Umwelt:\nBeobachtbarkeit der Umwelt kann variieren: \"voll beobachtbar\" bis zu \"unbeobachtbar\" Umwelt kann \"deterministisch\" oder \"stochastisch\" sein: Führt eine Aktion in einem Zustand immer zum selben Folgezustand? Wann erfolgt die Rückmeldung an den Agenten über die Auswirkung der Aktionen? Sofort (\"sequentiell\") oder erst am Ende einer Aktionsfolge (\"episodisch\")? Wird die Umwelt nur durch die Aktionen des Agenten verändert (\"statisch\")? Oder verändert sich die Umwelt zwischen den Aktionen eines Agenten, beispielsweise durch andere Agenten (\"dynamisch\")? Gibt es diskrete Zustände (wie im Beispiel)? Zustände der Bibliotheks-Welt Problem: Gegeben einen Startzustand, wie komme ich zum Ziel?\nWelche Aktionen können in einem Zustand (zb. Nr. 4) angewendet werden? Welche Aktionen können in den Folgezuständen angewendet werden? Ergebnis:\nZustandsraum: Menge aller von den Startzuständen aus erreichbaren Zustände Problemgraph: Repräsentation der Zustände und Aktionen als Knoten und (gerichtete) Kanten Suche im Problemgraphen Durch die Suche im Problemgraphen wird ein Suchbaum aufgespannt Varianten: Zustände können in einem Pfad wiederholt vorkommen vs. Wiederholungen werden ausgeschlossen Definition Zustand und Aktion Zustand: (Formale) Beschreibung eines Zustandes der Welt Aktion: (Formale) Beschreibung einer durch Agenten ausführbaren Aktion\nAnwendbar auf bestimmte Zustände Überführt Welt in neuen Zustand (\"Nachfolge-Zustand\") Geeignete Abstraktionen wählen für Zustände und Aktionen!\nAnmerkung: [Russell2020] unterscheidet zw. Aktionen und Transitionsmodell; hier nur Aktionen! D.h. die Aktionen und das Übergangsmodell aus dem [Russell2020] werden direkt zusammen betrachtet. Bei den hier diskutierten Problemen ist das ohne Nachteile möglich, es wird lediglich etwas Flexibilität genommen bzw. Komplexität vermieden (je nach Sichtweise :-) ...\nDefinition Problem Ein Problem besteht aus:\nStartzustände Menge $S_A \\subset S$ Aktionen Menge von Funktionen $\\operatorname{op}: S \\to S$ Zustandsraum Menge aller Zustände $S$, die durch (wiederholte) Anwendung von Aktionen von den Startzuständen aus erreichbar sind Zieltest Funktion $\\operatorname{goal}: S \\to \\{0,1\\}$ Zielzustände Menge $S_E \\subseteq S$ mit $\\forall x \\in S_E : \\operatorname{goal}(x)=1$ Kosten Gesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf dem aktuellen Weg erreichter Knoten $g(n)$ tatsächliche Kosten für den Weg vom Start bis zu Knoten $n$ $h(n)$ geschätzte Restkosten für den Weg von Knoten $n$ zum Ziel Hinweis: Unterschied Zustand und Knoten bzw. Zustandsraum und Problemgraph Zustände und Aktionen kann man als einen Graph darstellen: Problemgraph Zustände werden als Knoten im Graphen abgebildet Aktionen werden als (gerichtete) Kanten im Graphen abgebildet Unterscheidung \"Zustand\" und \"Knoten\": Zustand: Beschreibung/Modellierung eines Zustandes der Welt Knoten: Datenstruktur, Bestandteil des Graphen, symbolisiert einen Zustand Das bedeutet, dass der Problemgraph eine Repräsentation des Zustandsraumes ist.\nDie beiden Begriffe werden normalerweise synonym verwendet, sofern eindeutig ist, was gemeint ist.\nDefinition Problemlösen Problemlösen Wegesuche im Graph vom Startknoten zu einem Zielknoten\nSpannt den Suchbaum auf Lösung Folge von Aktionen, die Start- in Zielzustand überführen\nErgebnis des Problemlösens\nSuche: Einfache Basisvariante Füge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein Entnehme Knoten aus der Datenstruktur: Knoten ist gesuchtes Element: Abbruch, melde \"gefunden\" Expandiere alle Nachfolger des Knotens und füge diese in die Datenstruktur ein Falls die Datenstruktur leer ist: Abbruch, melde \"nicht gefunden\" Gehe zu Schritt 2 Für die in dieser Veranstaltung betrachteten deterministischen Probleme mit diskreten Zuständen ist diese Basisvariante der Suche eine Art generischer Suchalgorithmus: Durch die Variation der eingesetzten Datenstruktur und durch die Betrachtung unterschiedlicher Kosten erhält man die in den nächsten Sitzungen betrachteten verschiedenen klassischen Suchalgorithmen.\nAnmerkung: Für Handsimulation besserer Überblick, wenn statt der Knoten immer partielle Wege in Datenstruktur gespeichert werden!\nAnmerkung: Im [Russell2020, Abschnitt 3.3.3, S.92] wird ein Algorithmus mit den vorgestellten Eigenschaften als \"tree-like search\" bezeichnet. In Anlehnung an [Russell2020] wird diese Basisvariante der Suche in dieser Lehrveranstaltung kurz als \"Tree-Search\"-Algorithmus bezeichnet.\nAnmerkung: Im [Russell2020] wird für die Datenstruktur, mit der die Suche arbeitet, auch \"Frontier\" genannt. Hier werden alle Knoten gehalten, die in einem der nächsten Schritte betrachtet werden sollen, d.h. diese Knoten bilden die Grenze zwischen dem bereits untersuchten Teil des Graphen und dem noch unbekannten Teil des Graphen (deshalb auch \"Frontier\").\nErweiterung der Suche: Vermeiden von Wiederholungen Füge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein Entnehme Knoten aus der Datenstruktur: Knoten ist gesuchtes Element: Abbruch, melde \"gefunden\" Markiere aktuellen Knoten, und Expandiere alle Nachfolger des Knotens und füge alle unmarkierten Nachfolger, die noch nicht in der Datenstruktur sind, in die Datenstruktur ein Falls die Datenstruktur leer ist: Abbruch, melde \"nicht gefunden\" Gehe zu Schritt 2 Dieser Algorithmus ist eine Erweiterung der einfachen Basisvariante der Suche:\nMan markiert bereits besuchte (expandierte) Knoten und besucht diese nie wieder (man würde diese bei einer Expansion nicht wieder in die Datenstruktur aufnehmen). Außerdem vermeidet man, dass ein Knoten mehrfach in der Datenstruktur vorkommt: Dies würde bedeuten, dass man hier verschiedene Wege vom Start zu diesem Knoten in der Datenstruktur hat, die dann auch alle weiter untersucht werden müssten. In der Regel reicht aber ein Weg vom Start zu einem Zwischenknoten (meist wird der kürzeste genommen, dazu in einer späteren Sitzung mehr). Anmerkung: Für Handsimulation besserer Überblick, wenn statt der Knoten immer partielle Wege in Datenstruktur gespeichert werden!\nAnmerkung: Im [Russell2020, Abschnitt 3.3.3, S.92] wird ein Algorithmus mit den vorgestellten Eigenschaften als \"graph search\" bezeichnet. In Anlehnung an [Russell2020] wird diese erweiterter Variante der Suche in dieser Lehrveranstaltung kurz als \"Graph-Search\"-Algorithmus bezeichnet.\nBewertung von Suchalgorithmen Vollständigkeit Findet der Algorithmus eine Lösung, wenn es eine gibt? Optimalität Findet der Algorithmus die beste Lösung? Zeitkomplexität Wie lange dauert es eine Lösung zu finden? Speicherkomplexität Wieviel Speicher benötigt die Suche? Größen zur Bewertung:\nb: Verzweigungsfaktor d: Ebene (Tiefe) des höchsten Lösungsknotens m: Länge des längsten Pfades Wrap-Up Begriffe \"Problem\", \"Zustand\", \"Aktion\", \"Zustandsraum\", \"Problemgraph\", \"Suchbaum\"\nProblemlösen: Suche in Graphen nach Weg vom Start zum Ziel\nSuche spannt einen Suchbaum auf Unterschiedliche Kostenfunktionen möglich Suchalgorithmen: Einfache Basisvariante, Erweiterung mit Vermeidung von Redundanzen Beurteilung der Suchverfahren: Optimalität, Vollständigkeit, Komplexität",
    "description": "Motivation: Roboter in einer Bibliothek Aktionen:\nRight (R) Left (L) Take (T) Drop (D) Wahrnehmungen:\nIn welchem Raum bin ich? Habe ich das Buch? Aufgabe: Das Buch aus der Bibliothek holen und in den Kopiererraum bringen.\nBemerkungen zur Umwelt:\nBeobachtbarkeit der Umwelt kann variieren: \"voll beobachtbar\" bis zu \"unbeobachtbar\" Umwelt kann \"deterministisch\" oder \"stochastisch\" sein: Führt eine Aktion in einem Zustand immer zum selben Folgezustand? Wann erfolgt die Rückmeldung an den Agenten über die Auswirkung der Aktionen? Sofort (\"sequentiell\") oder erst am Ende einer Aktionsfolge (\"episodisch\")? Wird die Umwelt nur durch die Aktionen des Agenten verändert (\"statisch\")? Oder verändert sich die Umwelt zwischen den Aktionen eines Agenten, beispielsweise durch andere Agenten (\"dynamisch\")? Gibt es diskrete Zustände (wie im Beispiel)? Zustände der Bibliotheks-Welt",
    "tags": [],
    "title": "Problemlösen",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro2-problemsolving.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden) Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.\nEinen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die Klasse.\nMachine Learning 101 CAL2 Pruning CAL3 Entropie ID3 und C4.5",
    "description": "Beim überwachten Lernen soll eine Hypothese aufgebaut werden, die der echten (zu lernenden) Funktion möglichst nahe kommt. Eine Hypothese kann im einfachsten Fall als Entscheidungsbaum dargestellt werden. Die Merkmale bilden dabei die Knoten im Baum, und je Ausprägung gibt es eine Kante zu einem Nachfolgerknoten. Ein Merkmal bildet die Wurzel des Baums, an den Blättern sind die Klassen zugeordnet.\nEinen Entscheidungsbaum kann man zur Klassifikation eines Objekts schrittweise durchlaufen: Für jeden Knoten fragt man die Ausprägung des Merkmals im Objekt ab und wählt den passenden Ausgang aus dem Knoten. Wenn man am Blatt angekommen ist, hat man die Antwort des Baumes auf das Objekt, d.h. üblicherweise die Klasse.",
    "tags": [],
    "title": "Entscheidungsbäume (Decision Tree Learner - DTL)",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Entscheidungsbäume (DTL)",
    "content": "Was ist Lernen? Verhaltensänderung eines Agenten in Richtung der Optimierung eines Gütefunktionals (Bewertungsfunktion) durch Erfahrung.\nWarum Lernen? Nicht alle Situationen vorhersehbar Nicht alle Details modellierbar Lösung oder Lösungsweg unbekannt, nicht explizit programmierbar Data Mining: Entdeckung neuen Wissens durch Analyse der Daten Selbstanpassende Programme =\u003e Lernen wichtige Eigenschaft lebender Wesen :-)\nLearning Agent Feedback während des Lernens Überwachtes Lernen\nLernen durch Beobachtung Vorgabe von Beispielen: Ein- und Ausgabewerte =\u003e Regression, Klassifikation\nUnüberwachtes Lernen\nErkennen von Mustern in den Inputdaten, Clustering Kein Feedback (!) Reinforcement Lernen\nBewertung der Aktionen des Agenten am Ende einer Aktionsfolge Beispiel Kleinkind: Lernen von Klassen/Konzepten durch Beispiele\nZuerst ist alles \"Katze\" (Übergeneralisierung) Differenzierung durch Feedback der Umwelt; Erkennung unterschiedlicher Ausprägungen Beispiel: Kreditrisiko Bankkunde beantragt Kredit\nSoll er aus Sicht der Bank den Kredit bekommen?\nBankangestellter betrachtet (relevante) Merkmale des Kunden:\nAlter, Einkommen, sozialer Status Kundenhistorie bei der Bank Höhe des Kredits Bewertung des Kreditrisikos:\nKlassifikation: Guter oder schlechter Kunde (Binäre Entscheidung: 2 Klassen) Regression: Vorhersage Gewinn/Verlust für die Bank (Höhe des Gewinns/Verlusts interessant) Beispiel: Autoreparatur Gegeben: Eigenschaften eines Autos\n=\u003e Eigenschaften: Ausprägungen der Merkmale\nGesucht: Diagnose und Reparaturanleitung\n=\u003e Hypothese über den Merkmalen (Funktion $\\operatorname{h}$)\nLernen durch Beobachten: Lernen einer Funktion $\\operatorname{f}$ Funktionsapproximation: Lernen einer Funktion $\\operatorname{f}$ anhand von Beispielen\nEin Beispiel ist ein Tupel $(\\mathbf{x}, \\operatorname{f}(\\mathbf{x}))$, etwa $$ (\\mathbf{x}, \\operatorname{f}(\\mathbf{x})) = \\left(\\begin{array}{ccc} O \u0026 O \u0026 X \\\\ . \u0026 X \u0026 . \\\\ X \u0026 . \u0026 . \\end{array}, +1\\right) $$\nAufgabe: Baue Hypothese $\\operatorname{h}$ auf, so dass $\\operatorname{h} \\approx \\operatorname{f}$.\nBenutze dazu Menge von Beispielen =\u003e Trainingsdaten. Ziele:\nKonsistente Hypothese: Übereinstimmung bei Trainingsdaten Generalisierende Hypothese: Korrekte Vorhersage bei unbekannten Daten Anmerkung: Stark vereinfachtes Modell realen Lernens!\nKonstruieren einer konsistenten Hypothese Welcher Zusammenhang ist hier dargestellt? Offenbar eine Art Funktionsverlauf ... Wir haben für einige x-Werte die zugehörigen y-Werte vorgegeben.\nKonstruieren einer konsistenten Hypothese (cnt.) Die einfachste Approximation wäre eine lineare Funktion. Allerdings werden hierbei einige Werte mehr oder weniger stark nicht korrekt widergegeben, d.h. man hat einen relativ hohen (Trainings-) Fehler.\nKonstruieren einer konsistenten Hypothese (cnt.) Die Hyperbel erklärt die Trainingsdaten bis auf den einen Punkt sehr gut. Die Frage ist, ob dieser eine Punkt zum zu lernenden Zusammenhang gehört oder ein Ausreißer ist, den man gefahrlos ignorieren kann?\nKonstruieren einer konsistenten Hypothese (cnt.) Die grüne Hypothese ist von allen bisher gezeigten die komplexeste, erklärt aber alle Datenpunkte. D.h. hier wäre der Trainingsfehler Null. Zwischen den Trainingsdaten zeigt das Modell eine \"glatte\" Approximation, d.h. es wird auch neue Daten, die es beim Training nicht gesehen hat, relativ gut erklären. (Dabei liegt freilich die Annahme zugrunde, dass alle relevanten Daten in der Trainingsmenge vorhanden sind, d.h. dass es insbesondere zwischen den Datenpunkten keine Ausreißer o.ä. gibt.)\nKonstruieren einer konsistenten Hypothese (cnt.) Diese Hypothese erklärt ebenfalls sämtliche Trainingsdaten. Allerdings schwingt die Funktion zwischen den Daten stark hin und her. Vermutlich entspricht dies nicht dem zu lernenden Funktionsverlauf. Der Trainingsfehler wäre wie bei der deutlich einfacheren Hypthese aus dem letzten Schritt Null. Der Generalisierungsfehler (sprich die Abweichung, wenn man das Modell nach Daten zwischen den Trainingspunkten fragt) dürfte erheblich höher liegen.\nD.h. hier hat das Modell einfach die Trainingsdaten auswendig gelernt, aber nicht den Zusammenhang zwischen den Daten! Dies ist in der Regel unerwünscht!\nOccam's Razor Bevorzuge die einfachste konsistente Hypothese!\nWenn es mehrere mögliche Erklärungen für einen Sachverhalt gibt, ist die einfachste Erklärung allen anderen vorzuziehen. Eine Erklärung ist \"einfach\", wenn sie möglichst wenige Variablen und Annahmen enthält und wenn diese in klaren logischen Beziehungen zueinander stehen, aus denen der zu erklärende Sachverhalt logisch folgt. Trainingsdaten und Merkmalsvektoren Lehrer gibt Beispiele vor: Eingabe $\\mathbf{x}$ und passende Ausgabe $\\operatorname{f}(\\mathbf{x})$\nAusgabe: typischerweise Skalar (Funktionswert oder Klasse) =\u003e Beispiel: Bewertung eines Spielstandes bei TicTacToe\nEingabe: (Beschreibung des) Objekt(s) oder Situation, die zur Ausgabe gehört =\u003e Beispiel: Spielstand bei TicTacToe\nMerkmalsvektoren:\nZusammenfassen der relevanten Merkmale zu Vektoren Beispiel: Schwimmen im See Beschreibung der Faktoren, wann ich im See schwimmen möchte:\nScheint die Sonne? Wie warm ist das Wasser? Wie warm ist die Luft? Trainingsbeispiel: Eingabe: Merkmalsvektor (sonnig, warm, warm) Ausgabe: Klasse ja Dabei wird davon ausgegangen, dass jeder Faktor (jedes Merkmal) an einer bestimmten Stelle im Merkmalsvektor aufgeführt ist. Beispielsweise gehört das sonnig zur Frage \"Scheint die Sonne\", warm jeweils zur Wasser- und zur Lufttemperatur.\nDamit hat man in einem Vektor eine Situation komplett beschrieben, d.h. einen Zustand der Welt mit den relevanten Dingen beschrieben. Diesem Zustand kann man beispielsweise ein Label (Klasse) verpassen, hier in diesem Fall \"ja, in dieser Welt möchte ich schwimmen\".\nDie Trainingsmenge baut sich dann beim überwachten Lernen aus vielen solcher Paare (Merkmalsvektor, Klasse) auf, und die Algorithmen sollen diese Zuordnung lernen, d.h. ein Modell für diese Daten erzeugen, welches die Daten gut erklärt und darüber hinaus für neue Daten aus der selben Datenquelle gute Vorhersagen macht.\nTrainingsdaten -- Merkmalsvektoren Generell: Merkmalsvektor für Objekt $v$: $$ \\mathbf{x}(v) = (x_1, x_2, \\ldots, x_n) $$\n$n$ Merkmale (Attribute) Attribut $x_t$ hat $m_t$ mögliche Ausprägungen Ausprägung von $v$ bzgl. $x_t$: $\\quad x_t(v) = i \\quad$ (mit $i = 1 \\ldots m_t$) Anmerkung: Stellen Sie sich den Merkmalsvektor vielleicht wie einen Konstruktor einer Klasse x vor: Die einzelnen Attribute $x_t$ sind die Parameter, aus denen der Merkmalsvektor aufgebaut ist/wird. Jedes der Attribute hat einen Typ und damit eine bestimmte Anzahl erlaubter Werte (\"Ausprägungen\") ...\nTrainingsbeispiel:\nTupel aus Merkmalsvektor und zugehöriger Klasse: $\\left(\\mathbf{x}(v), k\\right)$ Wrap-Up Lernen ist Verhaltensänderung, Ziel: Optimierung einer Gütefunktion\nAufbau einer Hypothese, die beobachtete Daten erklären soll Arten: Überwachtes Lernen, Unüberwachtes Lernen, Reinforcement Lernen Merkmalsvektoren gruppieren Eigenschaften des Problems bzw. der Objekte\nTrainingsdaten: Beispielobjekte (durch Merkmalsvektoren beschrieben) plus Vorgabe vom Lehrer",
    "description": "Was ist Lernen? Verhaltensänderung eines Agenten in Richtung der Optimierung eines Gütefunktionals (Bewertungsfunktion) durch Erfahrung.\nWarum Lernen? Nicht alle Situationen vorhersehbar Nicht alle Details modellierbar Lösung oder Lösungsweg unbekannt, nicht explizit programmierbar Data Mining: Entdeckung neuen Wissens durch Analyse der Daten Selbstanpassende Programme =\u003e Lernen wichtige Eigenschaft lebender Wesen :-)\nLearning Agent Feedback während des Lernens Überwachtes Lernen",
    "tags": [],
    "title": "Machine Learning 101",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl1-mlbasics.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Das Perzeptron kann als die Nachahmung einer biologischen Nervenzelle betrachtet werden. Durch das Zusammenschließen dieser \"künstlichen \"Nervenzellen\" entstehen künstliche Neuronale Netze (NN), die ähnlich wie das Gehirn lernen sollen, komplexe Aufgaben zu bewerkstelligen.\nNN01 - Das Perzeptron NN02 - Lineare Regression und Gradientenabstieg NN03 - Logistische Regression NN04 - Overfitting und Regularisierung NN05 - Multilayer Perzeptron NN06 - Backpropagation NN07 - Training \u0026 Testing NN08 - Performanzanalyse NN10 - Vorschau Deep Learning (CNN, RNN)",
    "description": "Das Perzeptron kann als die Nachahmung einer biologischen Nervenzelle betrachtet werden. Durch das Zusammenschließen dieser \"künstlichen \"Nervenzellen\" entstehen künstliche Neuronale Netze (NN), die ähnlich wie das Gehirn lernen sollen, komplexe Aufgaben zu bewerkstelligen.\nNN01 - Das Perzeptron NN02 - Lineare Regression und Gradientenabstieg NN03 - Logistische Regression NN04 - Overfitting und Regularisierung NN05 - Multilayer Perzeptron NN06 - Backpropagation NN07 - Training \u0026 Testing NN08 - Performanzanalyse NN10 - Vorschau Deep Learning (CNN, RNN)",
    "tags": [],
    "title": "NN: Einführung in Neuronale Netze",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Definition \"Maschinelles Lernen\" Fähigkeit zu lernen, ohne explizit programmiert zu werden. (Arthur Samuel, 1959)\nArten des Lernens Überwachtes Lernen (e.g. Klassifizierung, Regression) Unüberwachtes Lernen (e.g. Clustering, Dimensionsreduktion) Bestärkendes Lernen (e.g. Schach spielen) Formalisierung Zielfunktion $f$ Merkmalraum (input space) Ausgaberaum (output space) Datensatz $\\mathcal{D}$ Hypothesenmenge $\\mathcal{H}$ Lernalgorithmus $\\mathcal{A}$ Das Perzeptron Ein einfaches Modell für die binäre Klassifizierung\nBilde gewichtete Summe (Linearkombination) der Merkmale Vergleiche das Ergebnis mit einem Schwellenwert Positiv, falls über dem Schwellenwert Negativ, falls unter dem Schwellenwert Gewichte und Schwellenwert sind unbekannte Parameter des Modells, die es zu lernen gilt \u003e siehe Perzeptron Lernalgorithmus",
    "description": "Kurze Übersicht Definition \"Maschinelles Lernen\" Fähigkeit zu lernen, ohne explizit programmiert zu werden. (Arthur Samuel, 1959)\nArten des Lernens Überwachtes Lernen (e.g. Klassifizierung, Regression) Unüberwachtes Lernen (e.g. Clustering, Dimensionsreduktion) Bestärkendes Lernen (e.g. Schach spielen) Formalisierung Zielfunktion $f$ Merkmalraum (input space) Ausgaberaum (output space) Datensatz $\\mathcal{D}$ Hypothesenmenge $\\mathcal{H}$ Lernalgorithmus $\\mathcal{A}$ Das Perzeptron Ein einfaches Modell für die binäre Klassifizierung\nBilde gewichtete Summe (Linearkombination) der Merkmale Vergleiche das Ergebnis mit einem Schwellenwert Positiv, falls über dem Schwellenwert Negativ, falls unter dem Schwellenwert Gewichte und Schwellenwert sind unbekannte Parameter des Modells, die es zu lernen gilt \u003e siehe Perzeptron Lernalgorithmus",
    "tags": [],
    "title": "NN01 - Das Perzeptron",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn01-perceptron.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Formalisierung Ausgabe $y$ ist reelle Zahl aus einem stetigen Bereich (zum Beispiel Hauspreis) Die Hypothesenfunktion ist eine gewichtete Summe der Merkmale $x_i$ plus eine Konstante $w_0$: $$h(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n$$ Der Verlust (engl. loss) für einen Datenpunkt $\\mathbf{x}$ ist das Fehlerquadrat: $$\\mathcal{L} = (\\hat{y} - y)^2 = (h(\\mathbf{x}) - y)^2$$ Die Kosten (engl. cost) sind der durchschnittliche Verlust über alle Datenpunkte: $$J = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y} - y)^2 = \\frac{1}{2m} \\sum_{i=1}^{m} (h(\\mathbf{x}) - y)^2$$ Der Gradient Der Gradientenvektor $\\nabla J(\\mathbf{w})$ setzt sich zusammen aus den partiellen Ableitungen der Kostenfunktion $J$ nach den Gewichten $w_i$ und zeigt in jedem Punkt $\\mathbf{w}$ in die Richtung des steilsten Aufstiegs: $$\\nabla J = [ \\partial J / \\partial w_0 \\quad \\partial J / \\partial w_1 \\quad \\ldots \\quad \\partial J / \\partial w_n]^T$$ Schlussfolgerung: In die entgegengesetzte Richtung, i.e. in Richtung $-\\nabla J(\\mathbf{w})$ geht es am steilsten bergab! IDEE: Bewege $\\mathbf{w}$ in Richtung $-\\nabla J(\\mathbf{w})$, um die Kosten $J$ möglichst schnell zu senken. Der Gradientenabstieg (engl. Gradient Descent) Starte mit zufälligen Gewichten $\\mathbf{w}$ Berechne den Gradientenvektor im aktuellen Punkt $\\mathbf{w}$ Gewichtsaktualisierung: Gehe einen kleinen Schritt in Richtung $-\\nabla J(\\mathbf{w})$ $$\\mathbf{w} _{neu} := \\mathbf{w} _{alt} - \\alpha \\cdot \\nabla J(\\mathbf{w} _{alt})$$ ($\\alpha$: Lernrate/Schrittweite). Wiederhole Schritte 2-3, bis das globale Minimum von $J$ erreicht ist. Graphische Übersicht Lineare Regression Perzeptron",
    "description": "Kurze Übersicht Formalisierung Ausgabe $y$ ist reelle Zahl aus einem stetigen Bereich (zum Beispiel Hauspreis) Die Hypothesenfunktion ist eine gewichtete Summe der Merkmale $x_i$ plus eine Konstante $w_0$: $$h(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n$$ Der Verlust (engl. loss) für einen Datenpunkt $\\mathbf{x}$ ist das Fehlerquadrat: $$\\mathcal{L} = (\\hat{y} - y)^2 = (h(\\mathbf{x}) - y)^2$$ Die Kosten (engl. cost) sind der durchschnittliche Verlust über alle Datenpunkte: $$J = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y} - y)^2 = \\frac{1}{2m} \\sum_{i=1}^{m} (h(\\mathbf{x}) - y)^2$$ Der Gradient Der Gradientenvektor $\\nabla J(\\mathbf{w})$ setzt sich zusammen aus den partiellen Ableitungen der Kostenfunktion $J$ nach den Gewichten $w_i$ und zeigt in jedem Punkt $\\mathbf{w}$ in die Richtung des steilsten Aufstiegs: $$\\nabla J = [ \\partial J / \\partial w_0 \\quad \\partial J / \\partial w_1 \\quad \\ldots \\quad \\partial J / \\partial w_n]^T$$ Schlussfolgerung: In die entgegengesetzte Richtung, i.e. in Richtung $-\\nabla J(\\mathbf{w})$ geht es am steilsten bergab! IDEE: Bewege $\\mathbf{w}$ in Richtung $-\\nabla J(\\mathbf{w})$, um die Kosten $J$ möglichst schnell zu senken. Der Gradientenabstieg (engl. Gradient Descent) Starte mit zufälligen Gewichten $\\mathbf{w}$ Berechne den Gradientenvektor im aktuellen Punkt $\\mathbf{w}$ Gewichtsaktualisierung: Gehe einen kleinen Schritt in Richtung $-\\nabla J(\\mathbf{w})$ $$\\mathbf{w} _{neu} := \\mathbf{w} _{alt} - \\alpha \\cdot \\nabla J(\\mathbf{w} _{alt})$$ ($\\alpha$: Lernrate/Schrittweite). Wiederhole Schritte 2-3, bis das globale Minimum von $J$ erreicht ist. Graphische Übersicht Lineare Regression Perzeptron",
    "tags": [],
    "title": "NN02 - Lineare Regression und Gradientenabstieg",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn02-linear-regression.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "Bonus: Möglichkeiten und Grenzen sowie Auswirkungen der KI (2P) Recherchieren Sie, welche Probleme bereits mittels Computer- bzw. Robotereinsatz gelöst werden können und welche aktuell noch ungelöst sind.\nRecherchieren Sie Auswirkungen auf die Gesellschaft durch die KI, etwa durch autonomes Fahren oder durch Large Language Models (LLM).\nThema: Gefühl für bereits realisierbare Aufgaben, Chancen und Risiken, Ethik\nNN.Perzeptron.01: Entscheidungsgrenze (2P) (1P) Betrachten Sie das durch den Gewichtsvektor $(w_0,w_1,w_2)^T = (2,1,1)^T$ gegebene Perzeptron. Zeichnen Sie die Trennebene und markieren Sie den Bereich, der mit $+1$ klassifiziert wird. (1P) Welche der folgenden Perzeptrons haben die selbe Trennebene? Welche weisen exakt die gleiche Klassifikation auf? $(w_0,w_1,w_2)^T = (1, 0.5, 0.5)^T$ $(w_0,w_1,w_2)^T = (200, 100, 100)^T$ $(w_0,w_1,w_2)^T = (\\sqrt{2}, \\sqrt{1}, \\sqrt{1})^T$ $(w_0,w_1,w_2)^T = (-2, -1, -1)^T$ Thema: Verständnis Interpretation Perzeptron (Trennebene/Entscheidungsgrenze)\nNN.Perzeptron.02: Logische Funktionen als Perzeptron (2P) (1.5P) Das Perzeptron kann zur Ausführung zahlreicher logischer Funktionen verwendet werden. Implementieren Sie die binären Logikfunktionen UND, ODER und KOMPLEMENT und demonstrieren Sie Ihre Implementierung in der Übung/im Praktikum. (0.5P) Eine grundlegende Einschränkung des Perzeptrons besteht darin, dass es die EXKLUSIV-ODER-Funktion nicht implementieren kann. Erklären Sie den Grund für diese Einschränkung. Thema: Verständnis Perzeptron\nNN.Perzeptron.03: Perzeptron Lernalgorithmus (6P) Ziel dieser Aufgabe ist es, mit Hilfe eines Experiments ein Gefühl für die Laufzeit des Perzeptron-Lernalgorithmus zu bekommen und eine Art empirische Approximation zu bestimmen.\nDatensatz (1P) Konstruieren Sie Ihren eigenen Datensatz $\\mathcal{D}$ mit $m=10$ gleichförmig verteilten Zufallspunkten aus dem Bereich $\\mathcal{X}=[−1, 1]\\times[−1, 1]$. Wählen Sie auf ähnliche Weise zwei zufällige, gleichmäßig verteilte Punkte aus dem Bereich $[−1, 1]\\times[−1, 1]$. Verwenden Sie die Gerade, die durch diese zwei Punkte verläuft, als die Entscheidungsgrenze Ihrer Zielfunktion $f$. Sie können die positiv beschriftete Seite beliebig festlegen. Werten Sie die Zielfunktion für jeden Datenpunkt $\\mathbf{x}^{(j)}$ aus, um die entsprechenden Beschriftungen (Ausgangslabel) $y^{(j)}$ zu erhalten. Training (3P) Führen Sie nun den Perzeptron-Lernalgorithmus $1000$ mal hintereinander aus. Initialisieren Sie jedes Mal die Gewichte mit $0$. Wählen Sie in jedem Lernschritt einen Punkt $\\mathbf{x}^{(i)}$ zufällig aus der Menge der falsch klassifizierten Punkte und aktualisieren Sie die Gewichte entsprechend der folgenden Formel: $$\\mathbf{w}:=\\mathbf{w}+\\alpha ( y^{(i)} - h(\\mathbf{x}^{(i)}) ) \\mathbf{x}^{(i)}$$\nNehmen Sie $\\alpha=1$ als Lernrate. Halten Sie für jeden Durchlauf fest, wie viele Schritte der Algorithmus benötigt, um zu der endgültigen Hypothese $h^{*}(\\mathbf{x})$ zu konvergieren. Berechnen Sie am Ende die durchschnittliche Anzahl von benötigten Schritten. In welcher Größenordnung liegt sie?\nExperimente (2P) Wiederholen Sie das obige Experiment mit $m=100$ und $m=1000$ Datenpunkten, jeweils ein Mal mit den Lernraten $\\alpha=1$ und $\\alpha=0.1$. In welcher Größenordnung liegt die durchschnittliche Anzahl von benötigten Schritten in diesen Fällen?\nUm eine zuverlässigere Schätzung zu erhalten, können Sie dasselbe Experiment mehrfach mit anderen zufällig generierten Datensätzen derselben Größe $m$ wiederholen und danach den Durchschnitt über alle Wiederholungen betrachten.\nVisualisierung (optional) Halten Sie während des Trainings die Anzahl der falsch klassifizierten Punkte fest und veranschaulichen Sie anschließend den Lernprozess mit Hilfe eines zweidimensionalen Plots. Visualisieren Sie (auf eine geeignete Weise) Meilenstein 2.1, wie sich die Entscheidungsrenze während des Trainings verändert. Sie können das folgende Jupyter Notebook als Startpunkt benutzen.\nIdee nach Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning From Data. AMLBook.",
    "description": "Bonus: Möglichkeiten und Grenzen sowie Auswirkungen der KI (2P) Recherchieren Sie, welche Probleme bereits mittels Computer- bzw. Robotereinsatz gelöst werden können und welche aktuell noch ungelöst sind.\nRecherchieren Sie Auswirkungen auf die Gesellschaft durch die KI, etwa durch autonomes Fahren oder durch Large Language Models (LLM).\nThema: Gefühl für bereits realisierbare Aufgaben, Chancen und Risiken, Ethik\nNN.Perzeptron.01: Entscheidungsgrenze (2P) (1P) Betrachten Sie das durch den Gewichtsvektor $(w_0,w_1,w_2)^T = (2,1,1)^T$ gegebene Perzeptron. Zeichnen Sie die Trennebene und markieren Sie den Bereich, der mit $+1$ klassifiziert wird. (1P) Welche der folgenden Perzeptrons haben die selbe Trennebene? Welche weisen exakt die gleiche Klassifikation auf? $(w_0,w_1,w_2)^T = (1, 0.5, 0.5)^T$ $(w_0,w_1,w_2)^T = (200, 100, 100)^T$ $(w_0,w_1,w_2)^T = (\\sqrt{2}, \\sqrt{1}, \\sqrt{1})^T$ $(w_0,w_1,w_2)^T = (-2, -1, -1)^T$ Thema: Verständnis Interpretation Perzeptron (Trennebene/Entscheidungsgrenze)",
    "tags": [],
    "title": "Übungsblatt: Perzeptron",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-perceptron.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Formalisierung Ausgabe $y$ ist reelle Zahl aus dem stetigen Bereich $(0,1)$\nDie Hypothesenfunktion ist: $$h(\\mathbf{x}) = \\sigma (\\mathbf{w}^T\\mathbf{x}) = \\sigma (w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n) \\tag{1}$$\nDer Kreuzentropie Verlust (engl. Cross-Entropy) für einen Datenpunkt $\\mathbf{x}$: $$\\mathcal{L}(a, y) = - y \\log(a) - (1-y) \\log(1-a)\\tag{2}$$ wobei hier $a := \\hat{y}$ die Vorhersage ist.\nDie Kosten als durchschnittlicher Verlust über alle Datenpunkte $x^{(1)}, \\ldots, x^{(m)}$: $$J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{3}$$\nGradientenabstieg Der Gradient für einen Datenpunkt $\\mathbf{x}$: $$\\frac{\\partial \\mathcal{L}}{\\partial w} = (a-y)x \\tag{4}$$ Der Gradient für alle Datenpunkte $X$ in Matrix-Notation: $$\\nabla J = \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$ Graphische Übersicht Logistische Regression Lineare Regression Perzeptron",
    "description": "Kurze Übersicht Formalisierung Ausgabe $y$ ist reelle Zahl aus dem stetigen Bereich $(0,1)$\nDie Hypothesenfunktion ist: $$h(\\mathbf{x}) = \\sigma (\\mathbf{w}^T\\mathbf{x}) = \\sigma (w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n) \\tag{1}$$\nDer Kreuzentropie Verlust (engl. Cross-Entropy) für einen Datenpunkt $\\mathbf{x}$: $$\\mathcal{L}(a, y) = - y \\log(a) - (1-y) \\log(1-a)\\tag{2}$$ wobei hier $a := \\hat{y}$ die Vorhersage ist.\nDie Kosten als durchschnittlicher Verlust über alle Datenpunkte $x^{(1)}, \\ldots, x^{(m)}$: $$J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{3}$$",
    "tags": [],
    "title": "NN03 - Logistische Regression",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn03-logistic-regression.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Nichtlineare Modelle Einführung von neuen Merkmalen in Form von nichtlienaren Kombinationen der ursprünglichen Merkmale Erhöhung der Komplexität des Modells ermöglicht das Erfassen von nichtlinearen Beziehungen Bemerkung: Die Hypothesenfunktion bleibt linear in den Gewichten, es wird weiterhin logistische Regression in einem erweiterten Merkmalraum durchgeführt. Überanpassung und Regularisierung Die Überanpassung (engl. Overfitting) ist eines der häufigsten und wichtigsten Probleme in ML und DL \"Was im Bereich des maschinellen Lernens Professionelle von Amateuren unterscheidet, ist ihre Fähigkeit mit Überanpassung umzugehen.\" [AbuMostafa2012, S. 119] Anzeichen von Überanpassung sind geringe Trainingskosten und hohe Testkosten (Kosten auf nicht-gesehenen Daten). Regularisierung ist eine Maßnahme gegen Überanpassung. Man kann es sich als eine Reduktion in der Komplexität des Modells vorstellen. Der Regularisierungsparameter $\\lambda$ ist ein Hyperparameter. Je größer der $\\lambda$-Wert, desto größer der Regularisierungseffekt. Die Kostentenfunktion bei regulariserter logistischer Regression: $$J = \\frac{1}{m} \\left\\lbrack \\sum_{i=1}^m \\left( -y^{[i]}log(a^{[i]})-(1-y^{[i]})log(1-a^{[i]}) \\right) + \\frac{\\lambda}{2} \\sum_{j=1}^n (w^2_j) \\right\\rbrack \\tag{1}$$ Die Gewichtsaktualisierung mit Regularisierungsterm: $$w_j := w_j - \\frac{\\alpha}{m} \\left\\lbrack \\sum_{i=1}^m \\left( ( a^{[i]} - y^{[i]} )x_j^{[i]} \\right) + \\lambda w_j \\right\\rbrack \\tag{2}$$",
    "description": "Kurze Übersicht Nichtlineare Modelle Einführung von neuen Merkmalen in Form von nichtlienaren Kombinationen der ursprünglichen Merkmale Erhöhung der Komplexität des Modells ermöglicht das Erfassen von nichtlinearen Beziehungen Bemerkung: Die Hypothesenfunktion bleibt linear in den Gewichten, es wird weiterhin logistische Regression in einem erweiterten Merkmalraum durchgeführt. Überanpassung und Regularisierung Die Überanpassung (engl. Overfitting) ist eines der häufigsten und wichtigsten Probleme in ML und DL \"Was im Bereich des maschinellen Lernens Professionelle von Amateuren unterscheidet, ist ihre Fähigkeit mit Überanpassung umzugehen.\" [AbuMostafa2012, S. 119] Anzeichen von Überanpassung sind geringe Trainingskosten und hohe Testkosten (Kosten auf nicht-gesehenen Daten). Regularisierung ist eine Maßnahme gegen Überanpassung. Man kann es sich als eine Reduktion in der Komplexität des Modells vorstellen. Der Regularisierungsparameter $\\lambda$ ist ein Hyperparameter. Je größer der $\\lambda$-Wert, desto größer der Regularisierungseffekt. Die Kostentenfunktion bei regulariserter logistischer Regression: $$J = \\frac{1}{m} \\left\\lbrack \\sum_{i=1}^m \\left( -y^{[i]}log(a^{[i]})-(1-y^{[i]})log(1-a^{[i]}) \\right) + \\frac{\\lambda}{2} \\sum_{j=1}^n (w^2_j) \\right\\rbrack \\tag{1}$$ Die Gewichtsaktualisierung mit Regularisierungsterm: $$w_j := w_j - \\frac{\\alpha}{m} \\left\\lbrack \\sum_{i=1}^m \\left( ( a^{[i]} - y^{[i]} )x_j^{[i]} \\right) + \\lambda w_j \\right\\rbrack \\tag{2}$$",
    "tags": [],
    "title": "NN04 - Overfitting und Regularisierung",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn04-overfitting.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Multilayer Perzeptron (MLP) Das Perzeptron kann nur linear separable Daten korrekt klassifizieren. Durch das Zusammenschließen von mehreren Perzeptronen kann man ein mehrschichtiges Perzeptron (engl. Multilayer Perceptron) aufstellen, das komplexere Funktionen modellieren kann. Ein MLP wird oft auch als Feed Forward Neural Network oder als Fully Connected Neural Network bezeichnet. Die \"inneren\" Schichten eines solchen Netzwerkes sind sogenannte versteckte Schichten (engl. hidden layer). Das sind alle Schichten ausgenommen die Eingangs- und Ausgangsschicht. Graphische Übersicht und Vorwärtslauf Ein Multi-Layer Perzeptron Ein Vorwärtslauf (forward pass): $$a^{[1]} = ReLU \\left( W^{[1]} \\cdot \\mathbb{x} + b^{[1]} \\right) \\tag{1}$$ $$\\hat{y} := a^{[2]} = \\sigma \\left( W^{[2]} \\cdot a^{[1]} + b^{[2]} \\right) \\tag{2}$$",
    "description": "Kurze Übersicht Multilayer Perzeptron (MLP) Das Perzeptron kann nur linear separable Daten korrekt klassifizieren. Durch das Zusammenschließen von mehreren Perzeptronen kann man ein mehrschichtiges Perzeptron (engl. Multilayer Perceptron) aufstellen, das komplexere Funktionen modellieren kann. Ein MLP wird oft auch als Feed Forward Neural Network oder als Fully Connected Neural Network bezeichnet. Die \"inneren\" Schichten eines solchen Netzwerkes sind sogenannte versteckte Schichten (engl. hidden layer). Das sind alle Schichten ausgenommen die Eingangs- und Ausgangsschicht. Graphische Übersicht und Vorwärtslauf Ein Multi-Layer Perzeptron Ein Vorwärtslauf (forward pass): $$a^{[1]} = ReLU \\left( W^{[1]} \\cdot \\mathbb{x} + b^{[1]} \\right) \\tag{1}$$ $$\\hat{y} := a^{[2]} = \\sigma \\left( W^{[2]} \\cdot a^{[1]} + b^{[2]} \\right) \\tag{2}$$",
    "tags": [],
    "title": "NN05 - Multilayer Perzeptron",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn05-mlp.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "NN.Regression.01: Lineare Regression \u0026 Gradientenabstieg (3P) Es sind folgende Trainingsdaten gegeben:\n$$ ( x^{(1)}, y^{(1)} ) = (1, 1), ( x^{(2)}, y^{(2)} ) = (2, 1), ( x^{(3)}, y^{(3)} ) = (3, 2) $$ Es soll das lineare Regressionsmodell $h(x) = w_0 + w_1 x$ mit diesen Daten trainiert werden, wobei die zu minimierende Kostenfunktion (durchschnittliche Summe der Fehlerquadrate) wie folgt gegeben ist:\n$$ J(\\mathbf{w}) = \\frac{1}{2m} \\sum^{m}_{j=1} (h(x^{(j)}) - y^{(j)} )^2 $$ (1P) Geben Sie $n$ und $m$ an und schreiben Sie die Kostenfunktion für die gegebenen Datenpunkte explizit auf. Berechnen Sie den Gradientenvektor $\\nabla J$ und beschreiben Sie die Bedeutung dieses Vektors.\n(2P) Seien die Gewichte in einem Iterationsschritt $w_0 = 1, w_1 = 1$. Führen Sie für die Lernraten $\\alpha=0.01$, $\\alpha=0.1$ und $\\alpha=1$ jeweils fünf aufeinanderfolgende Iterationen des Gradientenabstieg (Gradient Descent) Algorithmus durch.\nErstellen Sie eine Tabelle mit den Spalten $w_0$, $w_1$, $J(\\mathbf{w})$, $\\nabla J(\\mathbf{w})$, $\\alpha \\cdot \\nabla J(\\mathbf{w})$ und notieren Sie die zugehörigen Werte für jede Iteration. Erklären Sie, wie die Gewichtsaktualisierungen durchgeführt werden und geben Sie die dafür verwendete Formel an.\nWie verändern sich die Kosten während des Gradientenabstieges für die unterschiedlichen Lernraten? Begründen Sie dieses Verhalten.\nSie können das folgende Geogebra-Arbeitsblatt zu Hilfe nehmen.\nThema: Verständnis und Ablauf Gradientenabstieg und Lernrate\nNN.Regression.02: Logistische Regression \u0026 Gradientenabstieg (7P) Datensatz (1P) Konstruieren Sie Ihren eigenen Datensatz $\\mathcal{D}$ mit $m=100$ gleichförmig verteilten Zufallspunkten aus dem Bereich $\\mathcal{X}=[−1, 1]\\times[−1, 1]$. Wählen Sie auf ähnliche Weise zwei zufällige, gleichmäßig verteilte Punkte aus dem Bereich $[−1, 1]\\times[−1, 1]$. Verwenden Sie die Gerade, die durch diese zwei Punkte verläuft, als die Entscheidungsgrenze Ihrer Zielfunktion $f$. Sie können die positiv beschriftete Seite beliebig festlegen. Werten Sie die Zielfunktion für jeden Datenpunkt $\\mathbf{x}^{(j)}$ aus, um die entsprechenden Beschriftungen (Ausgangslabel) $y^{(j)}$ zu erhalten. Training (3P) Trainieren Sie ein logistisches Regressionsmodell auf diesen Daten, um $h^{*}=\\sigma(w^T x)$ zu finden. Verwenden Sie dazu den Gradientenabstieg-Algorithmus. Initialisieren Sie alle Gewichtswerte mit 0 und führen Sie 2000 Iterationen durch. Nehmen Sie $\\alpha=0.1$ als Lernrate. Speichern Sie alle 100 Schritte die berechneten Kosten. Zeichnen Sie am Ende die Kosten als Diagramm über die Anzahl der Iterationen auf.\nExperimente (3P) Wiederholen Sie das obige Experiment mit unterschiedlichen Lernraten, z.B. $\\alpha=0.1$, $\\alpha=0.01$ und $\\alpha=0.001$. Vergleichen Sie die Kosten-Diagramme.\nSie können das folgende Jupyter Notebook als Startpunkt benutzen. Sie können alternativ auch eine andere Programmiersprache und/oder einen anderen Datensatz (z.B. zufällig generierter Datensatz mittels Numpy and Scikit-Learn) verwenden.\nThema: Verständnis und Implementierung Logistische Regression",
    "description": "NN.Regression.01: Lineare Regression \u0026 Gradientenabstieg (3P) Es sind folgende Trainingsdaten gegeben:\n$$ ( x^{(1)}, y^{(1)} ) = (1, 1), ( x^{(2)}, y^{(2)} ) = (2, 1), ( x^{(3)}, y^{(3)} ) = (3, 2) $$ Es soll das lineare Regressionsmodell $h(x) = w_0 + w_1 x$ mit diesen Daten trainiert werden, wobei die zu minimierende Kostenfunktion (durchschnittliche Summe der Fehlerquadrate) wie folgt gegeben ist:\n$$ J(\\mathbf{w}) = \\frac{1}{2m} \\sum^{m}_{j=1} (h(x^{(j)}) - y^{(j)} )^2 $$ (1P) Geben Sie $n$ und $m$ an und schreiben Sie die Kostenfunktion für die gegebenen Datenpunkte explizit auf. Berechnen Sie den Gradientenvektor $\\nabla J$ und beschreiben Sie die Bedeutung dieses Vektors.",
    "tags": [],
    "title": "Übungsblatt: Lineare / Logistische Regression \u0026 Gradientenabstieg",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-regression.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Forwärts- und Rückwärtslauf Im Forwärtslauf (engl. forward pass oder forward propagation) wird ein einzelner Forwärtsschritt von Schicht $[l-1]$ auf Schicht $[l]$ wie folgt berechnet: $$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\tag{1}$$ $$A^{[l]} = g(Z^{[l]}) \\tag{2}$$ Dabei bezeichnet $g$ die Aktivierungsfunktion (z.B. Sigmoid oder ReLU).\nIm Rückwärtslauf (engl. backpropagation) werden in einem einzelnen Rückwärtsschritt von Schicht $[l]$ auf Schicht $[l-1]$ die folgenden Gradienten berechnet:\n$$dZ^{[l]} := \\frac{\\partial J }{\\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) \\tag{3}$$ $$dW^{[l]} := \\frac{\\partial J }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{4}$$ $$db^{[l]} := \\frac{\\partial J }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{5}$$ $$dA^{[l-1]} := \\frac{\\partial J }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{6}$$ Dabei steht \"$*$\" für die elementweise Multiplikation.\nBeachten Sie:\nDer Forwärtsschirtt übernimmt $A^{[l-1]}$ von dem vorherigen Schritt und gibt $A^{[l]}$ an den nächsten Schritt weiter. Der Rückwärtschritt übernimmt $dA^{[l]}$ von dem vorherigen Schritt und gibt $dA^{[l-1]}$ an den nächsten Rückwärtsschritt weiter. Parameteraktualisierung Die Aktualisierung der Parameter in Schicht $l$ erfolgt wie gewohnt durch: $$W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{7}$$ $$b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{8}$$ Dabei bezeichnet $\\alpha$ die Lernrate.",
    "description": "Kurze Übersicht Forwärts- und Rückwärtslauf Im Forwärtslauf (engl. forward pass oder forward propagation) wird ein einzelner Forwärtsschritt von Schicht $[l-1]$ auf Schicht $[l]$ wie folgt berechnet: $$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\tag{1}$$ $$A^{[l]} = g(Z^{[l]}) \\tag{2}$$ Dabei bezeichnet $g$ die Aktivierungsfunktion (z.B. Sigmoid oder ReLU).\nIm Rückwärtslauf (engl. backpropagation) werden in einem einzelnen Rückwärtsschritt von Schicht $[l]$ auf Schicht $[l-1]$ die folgenden Gradienten berechnet:\n$$dZ^{[l]} := \\frac{\\partial J }{\\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) \\tag{3}$$ $$dW^{[l]} := \\frac{\\partial J }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{4}$$ $$db^{[l]} := \\frac{\\partial J }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{5}$$ $$dA^{[l-1]} := \\frac{\\partial J }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{6}$$ Dabei steht \"$*$\" für die elementweise Multiplikation.",
    "tags": [],
    "title": "NN06 - Backpropagation",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn06-backprop.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "NN.MLP.01: Perzeptron-Netze (2P) Konstruieren Sie ein Netz mit drei Perzeptrons, welches für zwei Eingabevariablen $x_1$ und $x_2$ die in der folgenden Abbildung blau-grau dargestellten Bereiche mit +1 klassifiziert. Benutzen Sie die $\\operatorname{sign}$-Funktion als Aktivierungsfunktion.\nAbbildung 1\nNN.MLP.02: Vorwärtslauf im MLP (2P) Gegeben sei ein MLP mit 25 Zellen in der Eingangsschicht, 64 Zellen in der ersten versteckten Schicht, 32 Zellen in der zweiten versteckten Schicht und 4 Zellen in der Ausgabeschicht (die Bias-Zellen nicht mitgezählt). In allen Zellen wird die ReLU Aktivierungsfunktion verwendet.\nWas sind die Dimensionen der Gewichtsmatrizen $W^{[1]}$, $W^{[2]}$ und $W^{[3]}$ und der Bias-Vektoren $b^{[1]}$, $b^{[2]}$ und $b^{[3]}$? Wie wird die Ausgabe berechnet? Schreiben Sie den Vorwärtslauf in Matrix-Notation auf. Wie könnte man die Ausgabe deuten; welches Problem könnte durch dieses Netzwerk möglicherweise gelöst werden? NN.MLP.03: Tensorflow Playground (6P) Benutzen Sie den Neural Network Playground, um die unten gelisteten Experimente durchzuführen. Achten Sie bei allen Experimenten auf das Verhalten der Trainings- und Testkosten. Sie können mit Hilfe der Checkbox unter der Ausgabezelle (ganz rechts, unten) die Testdaten ein- und ausblenden. Der Play-Knopf startet dabei das Training und der Reload-Knopf setzt das Netzwerk zurück.\n(1P) Trainieren Sie ein logistisches Regressionsmodell zunächst auf dem \"Gaussian\" Datensatz (linear separierbarer Datensatz links-unten), danach auf den anderen Datensätzen.\n(3P) Trainieren Sie ein MLP mit\neiner versteckten Schicht mit 2 Neuronen, einer versteckten Schicht mit 3 Neuronen, einer versteckten Schicht mit 5 Neuronen, zwei versteckten Schichten mit jeweils 5 Neuronen pro Schicht drei versteckten Schichten mit jeweils 7 Neuronen pro Schicht vier versteckten Schichten mit jeweils 7 Neuronen pro Schicht auf dem kreisförmigen (Circle) und auf dem spiralförmigen (Spiral) Datensatz, mehrmals mit jeweils den Aktivierungsfunktionen ReLU, tanh und Sigmoid. Hat die Auswahl der Aktivierungsfunktion einen Einfluss auf die Form der Entscheidungsgrenze oder die Geschwindigkeit der Berechnung?\n(2P) Setzen Sie nun den Noise-Level auf 15 und wiederholen Sie die Experimente. Wann kann von einer Überanpassung gesprochen werden?\nSprechen Sie für alle Experimente die folgenden Punkte an:\nWie verhält sich die Entscheidungsgrenze? Was können Sie über Trainings- und Testkosten sagen? Entsteht eine Überanpassung? Wie schnell wird die Entscheidungsgrenze berechnet? Können alle Datenpunkte jedes mal korrekt klassifiziert werden? Warum? Untersuchen und vergleichen Sie die Ausgaben der Zellen in den versteckten Schichten, in dem Sie die Maus über die jeweilige Zelle bewegen. Bemerken Sie einen wesentlichen Unterschied in den Ausgaben der ersten Schicht im Vergleich zu der letzten Schicht?",
    "description": "NN.MLP.01: Perzeptron-Netze (2P) Konstruieren Sie ein Netz mit drei Perzeptrons, welches für zwei Eingabevariablen $x_1$ und $x_2$ die in der folgenden Abbildung blau-grau dargestellten Bereiche mit +1 klassifiziert. Benutzen Sie die $\\operatorname{sign}$-Funktion als Aktivierungsfunktion.\nAbbildung 1\nNN.MLP.02: Vorwärtslauf im MLP (2P) Gegeben sei ein MLP mit 25 Zellen in der Eingangsschicht, 64 Zellen in der ersten versteckten Schicht, 32 Zellen in der zweiten versteckten Schicht und 4 Zellen in der Ausgabeschicht (die Bias-Zellen nicht mitgezählt). In allen Zellen wird die ReLU Aktivierungsfunktion verwendet.",
    "tags": [],
    "title": "Übungsblatt: Overfitting \u0026 MLP",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-mlp.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Training und Testing Der tatsächliche Erfolg eines Modells wird nicht durch niedrige Trainingskosten gemessen, sondern durch geringe Kosten auf ungesehenen Daten, d.h. hohe Vorhersagekraft, gute Generalisierung!\nDie Menge aller gelabelten Daten in Trainingsset und Testset aufteilen, Testset nicht während des Trainings einsetzen!.\n$E_{in}$ bezeichnet den Fehler auf dem Trainingsset, auch in-sample error. $E_{out}$ bezeichnet den Fehler auf dem gesamten Eingaberaum $X$, auch out-of-sample error. $E_{out}$ ist der eigentliche Indikator für den zukünftigen Erfolg des Modells, ist uns aber nicht zugänglich. $E_{test}$ bezeichnet den Fehler auf dem Testset und ist eine Näherung für $E_{out}$. Analogie:\n$E_{in}$ : Erfolg in Übungsaufgaben und Probeprüfungen.\n$E_{test}$ : Erfolg in Endprüfung.\nDie Näherung $E_{test}$ sollte möglichst genau sein, damit es als ein verlässliches Gütesiegel dienen kann.\nDas Testset sollte genug Daten enthalten. Üblicher Anteil an Testdaten: bei $|D| \\approx 100.000 \\rightarrow$ ca. 20% bei $|D| \\approx 10.000.000 \\rightarrow$ ca. 1% Beispiel: Hat man 1000 Beispiele im Testset, wird $E_{test}$ mit $\\ge 98\\%$ Wahrscheinlichkeit in der $\\pm 5\\%$ Umgebung von $E_{out}$ liegen (für theoretische Grundlagen und Herleitung siehe [AbuMostafa2012, S. 39-69]). Trainingsdaten und Testdaten sollten möglichst aus derselben Verteilung kommen, wie die zukünftigen Real-World-Daten. Wichtige Bemerkung:\nTestdaten nicht anfassen, bis das Modell Einsatzbereit ist! Die Testdaten dürfen in keinster Weise bei der Auswahl der endgültigen Hypothese eingesetzt werden, weder bei der Berechnung der Parameter (Training), noch bei der Bestimmung der Hyperparameter (Hyperparameter-Tuning). Sobald der Testfehler die Auswahl der endgültigen Hypothese beeinflusst, kann sie nicht mehr als \"Gütesiegel\" eingesetzt werden.\nCHECK: Hätte man zufällig andere Testdaten gewählt, könnte sich dadurch die endgültige Hypothese ändern? Validierung und Modellauswahl Das Ziel ist es, das Modell mit bester Generalisierung, also kleinstem $E_{out}$ zu bestimmen. $E_{out}$ ist jedoch unbekannt und die Näherung $E_{test}$ darf nicht bei der Modellauswahl eingesetzt werden.\nLÖSUNG: Einen weiteren Teil der Daten als Validierungsset (auch development set) beiseitelegen und nicht für das Training (i.e. Minimierung des Trainingsfehlers $E_{in}$) verwenden!\nBemerkung:\nDas Wort Modell kann je nach Kontext unterschiedliche Bedeutungen annehmen.\nEin Modell im aktuellen Kontext ist als ein Paar $(\\mathcal{H},\\mathcal{A})$ von Hypothesenraum (bzw. Modellarchitektur) und Lernalgorithmus definiert.\nDie Auswahl eines Modells kann aus einer Menge von Modellen unterschiedlicher Art erfolgen (z.B. lineare Modelle, polynomiale Modelle, neuronale Netze), oder von Modellen derselben Art aber mit unterschiedlichen Hyperparametern (z.B. Neuronale Netze mit unterschiedlicher Anzahl von versteckten Schichten). Außerdem kann dieselbe Modellarchitektur $\\mathcal{H}$ mit unterschiedlichen Lernalgorithmen trainiert werden, was wiederum die endgültige Hypothese beeinflussen kann. Die Bestimmung der Hyperparameter von ${\\mathcal{A}}$ (wie z.B. Optimierungsfunktion, Lernrate, Kostenfunktion, Regularisierungsparameter usw.) sind daher auch Teil der Modellauswahl. Der Validierungsfehler $E_{val}$ kann nun als Entscheidungsgrundlage an verschiedenen Stellen des Lernrpozesses eingesetzt werden, wie zum Beispiel:\nBei der Auswahl geeigneter Hyperparameter wie z.B. Anzahl Schichten, Anzahl Zellen/Schicht, Aktivierungsfunktion, Regularisierungsparameter (siehe Abbildung 1). Abbildung 1 - Einsatz der Validierung für das Hyperparameter-Tuning\nBei der Auswahl der endgültigen Hypothese ($\\rightarrow$ Parameterauswahl!): unter allen Hypothesen, die während des Trainings durchlafen werden, wähle jene mit kleinstem $E_{val}$ (siehe Abbildung 2). Abbildung 2 - Einsatz der Validierung bei der Auswahl der entgültigen Hypothese\nBei der graphischen Darstellung von Lernkurven für die Diagnose von Über- und Unteranpassung (siehe Abbildung 3). Abbildung 3 - Lernkurven\nÜbliche train/val/test Aufteilung der Daten (in Prozent):\nbei $|D| \\approx 100.000 \\rightarrow$ ca. 60/20/20 bei $|D| \\approx 10.000.000 \\rightarrow$ ca. 98/1/1 Bemerkung:\nDas Modell ist trainiert für gute Ergebnisse auf Trainingsdaten und \"fine-tuned\" für gute Ergebnisse auf den Validierungsdaten. Ergebnisse auf Testdaten werden mit hoher wahrscheinlichkeit schlechter ausfallen, als auf Validierungsdaten ($E_{val}$ ist eine zu optimistische Näherung).\nSind Validierungs- und/oder Trainingsset zu klein, führt das zu schlechten Näherungen $E_{val}$ und folglich zu schlechten Entscheidungen.\nBei der Aufteilung muss ein gutes Trade-off gefunden werden. Wenn kein Gütesiegel notwendig ist, kann man auf das Testset verzichten und die Daten in Trainings- und Validierungsset aufteilen. Für eine bessere Näherung mit weniger Validierungsdaten kann k-fache Kreuzvalidierung eingesetzt werden (wenn genug Rechenkapazität vorhanden ist). K-fache Kreuzvalidierung (engl. k-fold cross-validation): Das Modell $(\\mathcal{H_m},\\mathcal{A_m})$ wird $k$ mal trainiert und validiert, jedes mal mit unterschiedlichen Trainings- und Validierungsmengen:\nDie Trainingsdaten werden in $k$ disjunkte Teilmengen $D_1, D_2, ..., D_k$ aufgeteilt.\nBei dem $i$-ten Training werden die Teilmenge $D_i$ für die Berechnung des Validierungsfehlers $e_i := E_{val}(h_m^{*(i)})$ und die restlichen $k-1$ Teilmengen für das Training verwendet.\nDer Kreuzvalidierungsfehler des Modells $(\\mathcal{H_m},\\mathcal{A_m})$ ist der Durchschnitt der $k$ Validierungsfehler $e_1, e_2, ..., e_k$ (siehe Abbildung 4). $$E_{CV}(m) := \\frac{1}{k} \\sum_{i=1}^{k} e_i = \\frac{1}{k} \\sum_{i=1}^{k} E_{val}(h_m^{*(i)})$$\nAbbildung 4 - Kreuzvalidierung\nBemerkung: Die Kreuzvalidierung wird nur bei der Modellauswahl eingesetzt: es liefert verlässlichere Näherungen für $E_{out}$ und führt daher zu besseren Entscheidungen. Das zuletzt ausgewählte Modell wird danach wie gewohnt auf den gesamten Trainigsdaten (ausgenommen Testdaten) trainiert und zum Schluss mit den Testdaten evaluiert.",
    "description": "Kurze Übersicht Training und Testing Der tatsächliche Erfolg eines Modells wird nicht durch niedrige Trainingskosten gemessen, sondern durch geringe Kosten auf ungesehenen Daten, d.h. hohe Vorhersagekraft, gute Generalisierung!\nDie Menge aller gelabelten Daten in Trainingsset und Testset aufteilen, Testset nicht während des Trainings einsetzen!.\n$E_{in}$ bezeichnet den Fehler auf dem Trainingsset, auch in-sample error. $E_{out}$ bezeichnet den Fehler auf dem gesamten Eingaberaum $X$, auch out-of-sample error. $E_{out}$ ist der eigentliche Indikator für den zukünftigen Erfolg des Modells, ist uns aber nicht zugänglich. $E_{test}$ bezeichnet den Fehler auf dem Testset und ist eine Näherung für $E_{out}$. Analogie:\n$E_{in}$ : Erfolg in Übungsaufgaben und Probeprüfungen.\n$E_{test}$ : Erfolg in Endprüfung.",
    "tags": [],
    "title": "NN07 - Training \u0026 Testing",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn07-training-testing.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht Performanzmetriken für Klassifizierungsprobleme Wahrheitsmatrix (engl. Confusion Matrix) Gibt eine Übersicht über die Anzahl von richtig und falsch klassifizierten Datenpunkten (bei binärer Klassifizierung) $TP =$ # True Positives $=$ Anzahl richtiger 1-Vorhersagen $FP =$ # False Positives $=$ Anzahl falscher 1-Vorhersagen $FN =$ # False Negatives $=$ Anzahl falscher 0-Vorhersagen $TN =$ # True Negatives $=$ Anzahl richtiger 0-Vorhersagen Bei Klassifizierungsproblemen mit $N$ Klassen hat man eine $N \\times N$ Matrix, die in Position $(i,j)$ die Anzahl der Klasse-$j$-Beispiele enthält, die als Klasse-$i$ vorhergesagt wurden. Abbildung 1 - Wahrheitsmatrix bei binärer Klassifizierung\nTreffergenauigkeit (engl. Accuracy) Anzahl richtig klassifizierter Datenpunkte, Erfolgsrate (engl. correct rate) $$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$\nAccuracy vermittelt ein falsches Bild des Erfolges bei unausgewogenen Datensätzen\nBeispiel:\nKlasse 1 hat 10, Klasse 0 hat 990 Beispiele. Ein Modell, das immer 0 ausgibt, hat $990/1000 = 0.99$ Treffergenauigkeit, ist aber offensichtlich kein gutes Modell! Precision Positive Predictive Value (PPV) Antwort auf: Von allen positiven Vorhersagen, wie viele sind richtig? $$Precision = \\frac{TP}{TP + FP}$$ Wahrscheinlichkeit, dass ein positiv klassifiziertes Beispiel auch tatsächlich positiv ist. Je näher an 1, desto besser. Accuracy of positive predictions. Recall True Positive Rate, auch Sensitivität (engl. Sensitivity) Antwort auf: Von allen positiven Beispielen, wie viele wurden richtig klassifiziert? $$Recall = \\frac{TP}{TP + FN}$$ Wahrscheinlichkeit, dass ein positives Beispiel tatsächlich als solches erkannt wird. Je näher an 1, desto besser. Accuracy of positive examples. Precision-Recall Trade-off Ein gutes Modell sollte hohe Precision und zugleich hohes Recall haben. Man kann die Precision eines Modells beliebig erhöhen (durch das Vergrößern des Schwellenwertes bei der Klassifizierung), jedoch wird dabei der Recall abnehmen. Genau so kann man den Recall eines Modells beliebig erhöhen (durch das Verkleinern des Schwellenwertes bei der Klassifizierung), jedoch wird dabei die Precision abnehmen. Es gilt ein gutes Trade-off zu finden. Eine Zwei-Zahlen-Metrik erschwert den Entscheidungsprozess bei Evaluierung und Modellauswahl. $F_1$-Score (Harmonisches Mittel) Fasst Precision (P) und Recall (R) in einer Metrik zusammen (Harmonisches Mittel von P und R): $$F_1-Score = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}} = 2 \\cdot \\frac{PR}{P + R}$$ Der $F_1$-Score wird nur dann hoch sein, wenn P und R beide hoch sind. Je näher an 1, desto besser. Sehr kleine P und R Werte ziehen den $F_1$-Score sehr stark herunter. In dieser Hinsicht gibt diese Metrik ein akkurates Bild über den Erfolg eines Modells.",
    "description": "Kurze Übersicht Performanzmetriken für Klassifizierungsprobleme Wahrheitsmatrix (engl. Confusion Matrix) Gibt eine Übersicht über die Anzahl von richtig und falsch klassifizierten Datenpunkten (bei binärer Klassifizierung) $TP =$ # True Positives $=$ Anzahl richtiger 1-Vorhersagen $FP =$ # False Positives $=$ Anzahl falscher 1-Vorhersagen $FN =$ # False Negatives $=$ Anzahl falscher 0-Vorhersagen $TN =$ # True Negatives $=$ Anzahl richtiger 0-Vorhersagen Bei Klassifizierungsproblemen mit $N$ Klassen hat man eine $N \\times N$ Matrix, die in Position $(i,j)$ die Anzahl der Klasse-$j$-Beispiele enthält, die als Klasse-$i$ vorhergesagt wurden. Abbildung 1 - Wahrheitsmatrix bei binärer Klassifizierung",
    "tags": [],
    "title": "NN08 - Performanzanalyse",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn08-testing.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "NN.Backprop.01: Gewichtsupdates für versteckte Schichten (2P) In der Vorlesung wurde(n) die Gewichtsupdates bei der Backpropagation für die Ausgabeschicht und die davor liegende letzte versteckte Schicht hergeleitet, wobei in der Ausgabeschicht die Sigmoid und in der versteckten Schicht die ReLU Aktivierungsfunktionen eingesetzt wurden. Leiten Sie die Gewichtsupdates für die erste versteckte Schicht (für ein Netz mit zwei echten versteckten Schichten) her. Verwenden Sie dabei die Sigmoid Funktion als Aktivierung in allen Schichten.\nThema: Verständnis Backpropagation\nNN.Backprop.02: Forward- und Backpropagation (2P) Betrachten Sie das folgende MLP mit zwei Schichten mit insgesamt zwei Zellen. Die Gewichte sind an den Kanten angegeben. Das Netz erhält den skalaren Input $x$ und berechnet daraus die Ausgabe $y$. Beide Zellen verwenden die Aktivierungsfunktion $\\sigma(z) = \\frac{1}{ 1 + e^{−z} }$.\nAbbildung 1\n(1P) Berechnen Sie die Ausgabe $y$ für die Eingabe $(x,y_T)=(0, 0.5)$. Wie groß ist der Fehler?\n(1P) Berechnen Sie die partiellen Ableitungen für die Gewichte. Wie lauten die Gewichtsupdates für das obige Trainingsbeispiel? Setzen Sie $\\alpha = 0.01$.\nNN.Backprop.03: MLP und Backpropagation (6P) Implementieren Sie ein Feedforward MLP mit mindestens einer versteckten Schicht. Nutzen Sie die Cross-Entropy Verlustfunktion.\n(2P) Implementieren Sie die Forwärtspropagation. Nutzen Sie als Aktivierungsfunktion in der Ausgangsschicht $g(z) = \\frac{1}{ 1 + e^{−z} }$ und in der versteckten Schicht $g(z) = ReLU(z)$.\n(2P) Implementieren Sie das Backpropagations-Verfahren zum Aktualisieren der Gewichte. Achten Sie insbesondere darauf, die bereits berechneten partiellen Ableitungen der jeweils hinteren Schicht wieder zu verwenden (und nicht jeweils erneut zu berechnen!), d.h. propagieren Sie die Fehler von hinten nach vorn durch das Netz.\n(2P) Trainieren Sie das Netz für den Iris-Datensatz (iris.csv) aus dem AIMA-Repository und nutzen Sie dabei die Variante des stochastischen Gradientenabstiegs. Messen Sie pro Epoche (also nach jedem Durchlauf durch den kompletten Datensatz) den Trainingsfehler. Zeichnen Sie den Trainingsfehler als Diagramm über den Epochen auf.\nFalls der Trainingsfehler nach einigen tausend Epochen nicht gegen einen Wert nahe Null strebt, erweitern Sie Ihr Netz (beispielsweise eine versteckte Schicht mehr oder mehr Zellen in der schon existierenden versteckten Schicht, ...) und trainieren Sie es erneut. Nach wievielen Epochen ist der Trainingsfehler fast Null?\nThema: Verständnis MLP und Backpropagation, Gefühl für nötige Größe des Netzes",
    "description": "NN.Backprop.01: Gewichtsupdates für versteckte Schichten (2P) In der Vorlesung wurde(n) die Gewichtsupdates bei der Backpropagation für die Ausgabeschicht und die davor liegende letzte versteckte Schicht hergeleitet, wobei in der Ausgabeschicht die Sigmoid und in der versteckten Schicht die ReLU Aktivierungsfunktionen eingesetzt wurden. Leiten Sie die Gewichtsupdates für die erste versteckte Schicht (für ein Netz mit zwei echten versteckten Schichten) her. Verwenden Sie dabei die Sigmoid Funktion als Aktivierung in allen Schichten.",
    "tags": [],
    "title": "Übungsblatt: Backpropagation",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nn-backprop.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Entscheidungsbäume (DTL)",
    "content": "Entscheidungsbäume: Klassifikation Attribute als Knoten im Baum Ausprägungen als Test (Ausgang, Verzweigung) Klasse (Funktionswert) als Blatt Erinnern Sie sich an das Beispiel mit der Auto-Reparatur aus der letzten Sitzung.\nDie relevanten Eigenschaften (Merkmale) eines Autos würden als Knoten im Baum repräsentiert. Beispiel: \"Motor startet\" oder \"Farbe\".\nJedes Merkmal hat eine Anzahl von möglichen Ausprägungen, diese entsprechen den Verzweigungen am Knoten. Beispiel: \"startet\", \"startet nicht\" oder \"rot\", \"weiß\", \"silber\", ... .\nEntsprechend kann man durch Abarbeiten des Entscheidungsbaumes am Ende zu einer Diagnose gelangen (Klasse).\nEine andere Sichtweise ist die Nutzung als Checkliste für eine Reparatur ...\nDefinition Entscheidungsbaum Erinnerung: Merkmalsvektor für Objekt $v$: $$ \\mathbf{x}(v) = (x_1, x_2, \\ldots, x_n) $$\n$n$ Merkmale (Attribute) Attribut $x_t$ hat $m_t$ mögliche Ausprägungen Ausprägung von $v$ bzgl. $x_t$: $\\quad x_t(v) = i \\quad$ (mit $i = 1 \\ldots m_t$) Alphabet für Baum: $$ \\lbrace x_t | t=1,\\ldots,n \\rbrace \\cup \\lbrace \\kappa | \\kappa = \\ast,A,B,C,\\ldots \\rbrace \\cup \\lbrace (,) \\rbrace $$\nEntscheidungsbaum $\\alpha$: $$ \\alpha = \\left\\lbrace \\begin{array}{ll} \\kappa \u0026 \\text{Terminalsymbole: } \\kappa = \\ast,A,B, \\ldots\\\\ x_t(\\alpha_1, \\alpha_2, \\ldots, \\alpha_{m_t}) \u0026 x_t \\text{ Testattribut mit } m_t \\text{ Ausprägungen} \\end{array}\\right. $$\nAnmerkung: Stellen Sie sich die linearisierte Schreibweise wieder wie den (verschachtelten) Aufruf von Konstruktoren vor. Es gibt die Oberklasse Baum, von der für jedes Attribut eine Klasse abgeleitet wird. D.h. der Konstruktor für eine Attributklasse erzeugt letztlich ein Objekt vom Obertyp Baum. Außerdem sind die Terminalsymbole A, B, ... Objekte vom Typ Blatt, welches eine Unterklasse von Baum ist ...\nDabei wird die Anzahl der möglichen Ausprägungen für ein Attribut berücksichtigt: Jede Ausprägung hat einen Parameter im Konstruktor. Damit werden die Unterbäume beim Erzeugen des Knotens übergeben.\nInduktion von Entscheidungsbäumen: CAL2 Anfangsschritt: $\\alpha^{(0)} = \\ast$ (totales Unwissen)\n$n$-ter Lernschritt: Objekt $v$ mit Klasse $k$, Baum $\\alpha^{(n-1)}$ gibt $\\kappa$ aus\n$\\kappa = \\ast$: ersetze $\\ast$ durch $k$ $\\kappa = k$: keine Aktion nötig $\\kappa \\neq k$: Fehler Ersetze $\\kappa$ mit neuem Test: $\\kappa \\gets x_{t+1}(\\ast, \\ldots, \\ast, k, \\ast, \\ldots, \\ast)$ $x_{t+1}$: nächstes Attribut, auf dem aktuellen Pfad noch nicht verwendet Symbol $k$ an Position $i$ wenn $x_{t+1}(v) = i$ $\\alpha^{(n)}$ bezeichnet den Baum im $n$-ten Lernschritt.\nCAL2 ist ein Meta-Algorithmus: Es ist ein Algorithmus, um einen Algorithmus zu lernen :-)\nBeispiel mit CAL2 $x_1$ $x_2$ $x_3$ $k$ 0 0 1 A 1 0 0 A 0 1 4 B 1 1 2 B 0 0 3 A Ergebnis: $x_1(x_2(A, B), x_2(A, B))$\nAnmerkung: Denken Sie an die Analogie von oben. $x_1$ kann als Konstruktor einer Klasse x1 betrachtet werden, die eine Unterklasse von Baum ist. Durch den Aufruf des Konstruktors wird als ein Baum erzeugt.\nEs gibt in $x_1$ zwei mögliche Ausprägungen, d.h. der Baum hat in diesem Knoten zwei alternative Ausgänge. Diese Unterbäume werden dem Konstruktor von x1 direkt beim Aufruf übergeben (müssen also Referenzen vom Typ Baum sein).\nCAL2: Bemerkungen Nur für diskrete Merkmale und disjunkte Klassen\nZyklischer Durchlauf durch Trainingsmenge\nAbbruch:\nAlle Trainingsobjekte richtig klassifiziert =\u003e Kein Fehler in einem kompletten Durchlauf (Differenzierung nötig, aber alle Merkmale verbraucht) (Lernschrittzahl überschritten) Wrap-Up Darstellung der Hypothese als Entscheidungsbaum CAL2: diskrete Attribute, disjunkte Klassen",
    "description": "Entscheidungsbäume: Klassifikation Attribute als Knoten im Baum Ausprägungen als Test (Ausgang, Verzweigung) Klasse (Funktionswert) als Blatt Erinnern Sie sich an das Beispiel mit der Auto-Reparatur aus der letzten Sitzung.\nDie relevanten Eigenschaften (Merkmale) eines Autos würden als Knoten im Baum repräsentiert. Beispiel: \"Motor startet\" oder \"Farbe\".\nJedes Merkmal hat eine Anzahl von möglichen Ausprägungen, diese entsprechen den Verzweigungen am Knoten. Beispiel: \"startet\", \"startet nicht\" oder \"rot\", \"weiß\", \"silber\", ... .",
    "tags": [],
    "title": "CAL2",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl2-cal2.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Entscheidungsbäume (DTL)",
    "content": "Pruning: Bedingt irrelevante Attribute Baum: $\\alpha = x_1(x_2(A, B), x_2(A, B), x_2(A, B))$\n$x_1$ ist bedingt irrelevant =\u003e Vereinfachung: $\\alpha = x_2(A, B)$\nAllgemein:\nSei $\\tilde{x}$ Weg zu Nichtendknoten $x_t$ Baum dort $\\alpha/\\tilde{x} = x_t(\\alpha_1, \\ldots, \\alpha_{m_t})$ $x_t$ ist bedingt irrelevant unter der Bedingung $\\tilde{x}$, wenn $\\alpha_1 = \\alpha_2 = \\ldots = \\alpha_{m_t}$ Vereinfachung: Ersetze in $\\alpha/\\tilde{x}$ den Test $x_t$ durch $\\alpha_1$ Anmerkung: Der durch das Entfernen von bedingt irrelevanten Attributen entstandene Baum hat exakt die selbe Aussage (Klassifikation) wie der Baum vor dem Pruning.\nAnmerkung: $x_1$ im obigen Beispiel ist sogar global irrelevant, da es sich hier um die Wurzel des Baumes handelt. Der Weg $\\tilde{x}$ ist in diesem Fall der leere Weg ...\nPruning: Bedingt redundante Attribute Baum: $\\alpha = x_1(\\ast, \\ast, x_2(A, B))$\n$x_1$ ist bedingt redundant =\u003e Vereinfachung: $\\alpha = x_2(A, B)$\nAllgemein:\nSei $\\tilde{x}$ Weg zu Nichtendknoten $x_t$ Baum dort $\\alpha/\\tilde{x} = x_t(\\ast, \\ldots, \\ast, \\alpha_i, \\ast, \\ldots, \\ast)$ (mit $\\alpha_i \\neq \\ast$) $x_t$ ist bedingt redundant unter der Bedingung $\\tilde{x}$ Vereinfachung: Ersetze in $\\alpha/\\tilde{x}$ den Test $x_t$ durch $\\alpha_i$ Anmerkung: Der durch das Entfernen von bedingt redundanten Attributen entstandene Baum hat eine etwas andere Klassifikation als der Baum vor dem Pruning. Wo vorher ein * ausgegeben wurde, wird nach dem Pruning u.U. ein Klassensymbol ausgegeben. Der Klassifikationsfehler erhöht sich aber nicht, da hier ein * wie ein falsches Klassensymbol zu werten ist.\nAnmerkung: $x_1$ im obigen Beispiel ist sogar global redundant, da es sich hier um die Wurzel des Baumes handelt. Der Weg $\\tilde{x}$ ist in diesem Fall der leere Weg ...\nAllgemeine Transformationsregel $$ x_1(x_2(a, b), x_2(c, d)) \\Leftrightarrow x_2(x_1(a, c), x_1(b, d)) $$ Wrap-Up Pruning: Entfernen bedingt redundanter und irrelevanter Tests Transformationsregel zum Umbauen von Entscheidungsbäumen",
    "description": "Pruning: Bedingt irrelevante Attribute Baum: $\\alpha = x_1(x_2(A, B), x_2(A, B), x_2(A, B))$\n$x_1$ ist bedingt irrelevant =\u003e Vereinfachung: $\\alpha = x_2(A, B)$\nAllgemein:\nSei $\\tilde{x}$ Weg zu Nichtendknoten $x_t$ Baum dort $\\alpha/\\tilde{x} = x_t(\\alpha_1, \\ldots, \\alpha_{m_t})$ $x_t$ ist bedingt irrelevant unter der Bedingung $\\tilde{x}$, wenn $\\alpha_1 = \\alpha_2 = \\ldots = \\alpha_{m_t}$ Vereinfachung: Ersetze in $\\alpha/\\tilde{x}$ den Test $x_t$ durch $\\alpha_1$ Anmerkung: Der durch das Entfernen von bedingt irrelevanten Attributen entstandene Baum hat exakt die selbe Aussage (Klassifikation) wie der Baum vor dem Pruning.",
    "tags": [],
    "title": "Pruning",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl3-pruning.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Entscheidungsbäume (DTL)",
    "content": "CAL3: Erweiterung von CAL2 für nicht-disjunkte Klassen Anfangsschritt: $\\alpha^{(0)} = \\ast$ (totales Unwissen)\n$n$-ter Lernschritt: Objekt $v$ mit Klasse $k$\nRückweisung (Endknoten mit $\\ast$): Ersetze $\\ast$ durch Vereinigungsklasse $/k1/$\nEndknoten mit Vereinigungsklasse:\nZähler für $k$ erhöhen, bzw. $k$ mit Anzahl $1$ in Vereinigungsklasse einfügen Falls nun die Summe aller Klassen am Endknoten größer/gleich $S_1$ (Statistikschwelle):\nFür genau eine Klasse gilt: $P(k | \\tilde{x}) \\ge S_2$: =\u003e Abschluss: Ersetze Vereinigungsklasse durch $k$ (für immer!)\nFür alle Klassen gilt: $P(k | \\tilde{x}) \u003c S_2$: =\u003e Differenzierung: Ersetze Vereinigungsklasse durch neuen Test: $\\kappa \\gets x_{t+1}(\\ast, \\ldots, \\ast, /k1/, \\ast, \\ldots, \\ast)$\n$x_{t+1}$: nächstes Attribut, auf dem aktuellen Pfad $\\tilde{x}$ noch nicht verwendet Symbol $k$ mit Anzahl 1 an Position $i$ wenn $x_{t+1}(v) = i$\nBeispiel mit CAL3 $x_1$ $x_2$ $k$ 0 0 A 0 1 B 0 1 A 1 0 B 1 1 A $S_1 = 4, S_2 = 0.7$ Ergebnis: $x_1(A, x_2(B, A))$\nTrainingsfehler: $1/5 = 0.2 \u003c 1-S_2 = 1-0.7 = 0.3$\nHinweis: Bei nicht überlappenden Klassen erzeugt CAL3 u.U. andere Bäume als CAL2 ...\nCAL3: Abbruchbedingungen und Parameter Parameter:\n$S_1$: Statistikschwelle, problemabhängig wählen $S_2$: $0.5 \u003c S_2 \\le 1.0$ Klassifikationsfehler kleiner als $1-S_2$ kleiner Fehler =\u003e großer Baum großer Fehler =\u003e kleiner Baum Abbruch:\nAlle Trainingsobjekte richtig klassifiziert =\u003e Kein Fehler in einem kompletten Durchlauf Alle Endknoten mit eindeutigen Klassensymbolen belegt Differenzierung nötig, aber alle Merkmale verbraucht Lernschrittzahl überschritten Wrap-Up CAL3: Erweiterung von CAL2 für überlappende Klassen Parameter $S_1$ (Anzahl Objekte bis Entscheidung), $S_2$ (Dominanz?) Trainingsfehler wg. überlappender Klassen!",
    "description": "CAL3: Erweiterung von CAL2 für nicht-disjunkte Klassen Anfangsschritt: $\\alpha^{(0)} = \\ast$ (totales Unwissen)\n$n$-ter Lernschritt: Objekt $v$ mit Klasse $k$\nRückweisung (Endknoten mit $\\ast$): Ersetze $\\ast$ durch Vereinigungsklasse $/k1/$\nEndknoten mit Vereinigungsklasse:\nZähler für $k$ erhöhen, bzw. $k$ mit Anzahl $1$ in Vereinigungsklasse einfügen Falls nun die Summe aller Klassen am Endknoten größer/gleich $S_1$ (Statistikschwelle):\nFür genau eine Klasse gilt: $P(k | \\tilde{x}) \\ge S_2$: =\u003e Abschluss: Ersetze Vereinigungsklasse durch $k$ (für immer!)",
    "tags": [],
    "title": "CAL3",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl4-cal3.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Entscheidungsbäume (DTL)",
    "content": "Wie Attribute wählen? Erinnerung: CAL2/CAL3 Zyklische Iteration durch die Trainingsmenge Ausschließlich aktuelles Objekt betrachtet Reihenfolge der \"richtigen\" Attributwahl bei Verzweigung unklar =\u003e Betrachte stattdessen die komplette Trainingsmenge!\nRelevanz =\u003e Informationsgehalt Shannon/Weaver (1949): Entropie Maß für die Unsicherheit einer Zufallsvariablen Anzahl der Bits zur Darstellung der Ergebnisse eines Zufallsexperiments Beispiele Münze, die immer auf dem Rand landet: keine Unsicherheit, 0 Bit Faire Münze: Kopf oder Zahl: Entropie 1 Bit Fairer 4-seitiger Würfel: 4 mögliche Ausgänge: Entropie 2 Bit Münze, die zu 99% auf einer Seite landet: Entropie nahe Null =\u003e Anzahl der Ja/Nein-Fragen, um zur gleichen Information zu kommen\nDefinition der Entropie $H(V)$ für Zufallsvariable $V$ Zufallsvariable $V$ =\u003e mögliche Werte $v_k$ Wahrscheinlichkeit für $v_k$ sei $p_k = P(v_k)$ $H(V) = -\\sum_k p_k \\log_2 p_k$\nHinweis: $\\log_2 x = \\frac{\\log_{10} x}{\\log_{10} 2} = \\frac{\\log x}{\\log 2}$\nNur eine Klasse: $\\log_2 1 = 0$ =\u003e $H(V) = 0$ Bit Zwei Klassen, gleichwahrscheinlich: $\\log_2 0.5 = -1$ =\u003e $H(V) = 1$ Bit Beispiele Entropie: faire Münze Entropie: $H(V) = -\\sum_k p_k \\log_2 p_k$\n$v_1 = \\operatorname{Kopf}, v_2 = \\operatorname{Zahl}$ $p_1 = 0.5, p_2 = 0.5$ $H(\\operatorname{Fair}) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1$ Bit $\\log_2 0.5 = -1$ Beispiele Entropie: unfaire Münze Entropie: $H(V) = -\\sum_k p_k \\log_2 p_k$\n$v_1 = \\operatorname{Kopf}, v_2 = \\operatorname{Zahl}$ $p_1 = 0.99, p_2 = 0.01$ $H(\\operatorname{UnFair}) = -(0.99 \\log_2 0.99 + 0.01 \\log_2 0.01)$ $H(\\operatorname{UnFair}) \\approx 0.08$ Bit\n$\\log_2 0.01 \\approx -6.64$ $\\log_2 0.99 \\approx -0,014$ Beispiele Entropie: 4-seitiger Würfel Entropie: $H(V) = -\\sum_k p_k \\log_2 p_k$\n$v_1 = 1, v_2 = 2, v_3 = 3, v_4 = 4$ $p_1 = p_2 = p_3 = p_4 = 0.25$ $H(\\operatorname{Wuerfel}) = -4\\cdot(0.25 \\log_2 0.25) = 2$ Bit $\\log_2 0.25 = -2$ Entropie der Trainingsmenge: Häufigkeit der Klassen zählen Nr. $x_1$ $x_2$ $x_3$ $k$ 1 0 0 0 A 2 1 0 2 A 3 0 1 1 A 4 1 1 0 B 5 0 1 1 B 6 0 1 0 A Anzahl Klasse $A$: 4 Anzahl Klasse $B$: 2 Gesamtzahl Beispiele: 6 Wahrscheinlichkeit für $A$: $p_A = 4/6 = 0.667$\nWahrscheinlichkeit für $B$: $p_B = 2/6 = 0.333$\n$$ \\begin{array}{rcl} H(S) \u0026=\u0026 -\\sum_k p_k \\log_2 p_k\\\\ \u0026=\u0026 -(4/6 \\cdot \\log_2 4/6 + 2/6 \\cdot \\log_2 2/6)\\\\ \u0026=\u0026 -(-0.39 -0.53) = 0.92 \\operatorname{Bit} \\end{array} $$ Mittlere Entropie nach Betrachtung von Attribut $A$ $$ R(S, A) = \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) $$ Auswahl von Attribut $A$ partitioniert die Trainingsmenge: Je Ausprägung $v$ von $A$ erhält man eine Submenge $S_v$\n$R(S, A)$ berechnet die mittlere Entropie der Trainingsmenge, nachdem Attribut $A$ ausgewählt wurde: Unsicherheit/nötige Bits nach Auswahl von Attribut $A$\nEntropie der Trainingsmenge nach Attributwahl Nr. $x_1$ $x_2$ $x_3$ $k$ 1 0 0 0 A 2 1 0 2 A 3 0 1 1 A 4 1 1 0 B 5 0 1 1 B 6 0 1 0 A Sei Attribut $x_1$ ausgewählt $x_1$ partitioniert die Trainingsmenge $x_1=0$ liefert $S_0 = \\lbrace 1,3,5,6 \\rbrace$ $x_1=1$ liefert $S_1 = \\lbrace 2,4 \\rbrace$ Häufigkeit für $x_1=0$: $4/6$ Häufigkeit für $x_1=1$: $2/6$ Gesamtzahl Beispiele: 6 $$ \\begin{array}{rcl} R(S, A) \u0026=\u0026 \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} H(S_v)\\\\ \u0026=\u0026 4/6 \\cdot H(\\lbrace 1,3,5,6 \\rbrace) + 2/6 \\cdot H(\\lbrace 2,4 \\rbrace)\\\\ \u0026=\u0026 4/6\\cdot(-3/4 \\cdot \\log_2 3/4 - 1/4 \\cdot \\log_2 1/4) +\\\\ \u0026\u0026 2/6\\cdot(-1/2 \\cdot \\log_2 1/2 - 1/2 \\cdot \\log_2 1/2)\\\\ \u0026=\u0026 0.54 + 0.33 = 0.87 \\operatorname{Bit} \\end{array} $$ Ausblick: Gini Impurity Wir haben hier die Entropie als Maß für den Informationsgehalt einer Trainingsmenge genutzt. $R(S,A)$ als die mittlere Entropie nach Betrachtung von Attribut $A$ wird von typischen Entscheidungsbaumverfahren wie ID3 und C4.5 genutzt, um bei einer Verzweigung das nächste möglichst aussagekräftige Merkmal auszuwählen.\nIn anderen Entscheidungsbaumlernern wird stattdessen die Gini Impurity zur Bestimmung des Informationsgehalts eingesetzt (u.a. CART). Dieses Maß sagt aus, wie oft man ein zufällig gezogenes Element des Datensatzes falsch klassifizieren würde, wenn man es mit einer zufälligen Klasse basierend auf der Verteilung der Klassen im Datensatz labeln würde.\nHierzu drei lesenswerte Blog-Einträge:\nDeep dive into the basics of Gini Impurity in Decision Trees with math Intuition Decision Trees, Explained Decision Tree Algorithm With Hands-On Example Wrap-Up Begriff und Berechnung der Entropie: Maß für die Unsicherheit Begriff und Berechnung des Informationsgewinns Entropie für eine Trainingsmenge Mittlere Entropie nach Wahl eines Attributs",
    "description": "Wie Attribute wählen? Erinnerung: CAL2/CAL3 Zyklische Iteration durch die Trainingsmenge Ausschließlich aktuelles Objekt betrachtet Reihenfolge der \"richtigen\" Attributwahl bei Verzweigung unklar =\u003e Betrachte stattdessen die komplette Trainingsmenge!\nRelevanz =\u003e Informationsgehalt Shannon/Weaver (1949): Entropie Maß für die Unsicherheit einer Zufallsvariablen Anzahl der Bits zur Darstellung der Ergebnisse eines Zufallsexperiments Beispiele Münze, die immer auf dem Rand landet: keine Unsicherheit, 0 Bit Faire Münze: Kopf oder Zahl: Entropie 1 Bit Fairer 4-seitiger Würfel: 4 mögliche Ausgänge: Entropie 2 Bit Münze, die zu 99% auf einer Seite landet: Entropie nahe Null =\u003e Anzahl der Ja/Nein-Fragen, um zur gleichen Information zu kommen",
    "tags": [],
    "title": "Entropie",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl5-entropy.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Entscheidungsbäume (DTL)",
    "content": "Wie Attribute wählen? Erinnerung: CAL2/CAL3\nZyklische Iteration durch die Trainingsmenge Ausschließlich aktuelles Objekt betrachtet Reihenfolge der \"richtigen\" Attributwahl bei Verzweigung unklar =\u003e Betrachte stattdessen die komplette Trainingsmenge!\nErinnerung Entropie: Maß für die Unsicherheit Entropie $H(S)$ der Trainingsmenge $S$: relative Häufigkeit der Klassen zählen\nMittlere Entropie nach Betrachtung von Attribut $A$\n$$ R(S, A) = \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) $$ Informationsgewinn durch Betrachtung von Attribut $A$\n$$ \\begin{array}{rcl} \\operatorname{Gain}(S, A) \u0026=\u0026 H(S) - R(S, A)\\\\[5pt] \u0026=\u0026 H(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) \\end{array} $$ $R(S,A)$ ist die Unsicherheit/nötige Bits nach Auswahl von Attribut A. Je kleiner $R(S,A)$, um so kleiner die verbleibende Unsicherheit bzw. um so kleiner die Anzahl der nötigen Bits zur Darstellung der partitionierten Trainingsmenge nach Betrachtung von Attribut $A$ ...\n=\u003e Je kleiner $R(S,A)$, um so größer der Informationsgewinn\nInformationsgewinn: Kriterium zur Auswahl von Attributen Informationsgewinn für alle Attribute berechnen Nehme Attribut mit größtem Informationsgewinn als nächsten Test Nr. $x_1$ $x_2$ $x_3$ $k$ 1 0 0 0 A 2 1 0 2 A 3 0 1 1 A 4 1 1 0 B 5 0 1 1 B 6 0 1 0 A $H(S) = 0.92 \\operatorname{Bit}$ $$ \\begin{array}{rcl} \\operatorname{Gain}(S, x_1) \u0026=\u0026 0.92 - 0.87 = 0.05 \\operatorname{Bit}\\\\ \\operatorname{Gain}(S, x_2) \u0026=\u0026 0.92 - 2/6 \\cdot 0 - 4/6 \\cdot 1\\\\ \u0026=\u0026 0.25 \\operatorname{Bit}\\\\ \\operatorname{Gain}(S, x_3) \u0026=\u0026 0.92 - 3/6 \\cdot 0.92 - 2/6 \\cdot 1 - 1/6 \\cdot 0\\\\ \u0026=\u0026 0.13 \\operatorname{Bit} \\end{array} $$ Informationsgewinn für $x_2$ am höchsten =\u003e wähle $x_2$ als nächsten Test\nEntscheidungsbaumlerner ID3 (Quinlan, 1986) def ID3(examples, attr, default): # Abbruchbedingungen if examples.isEmpty(): return default if examples.each(class == A): return A # all examples have same class if attr.isEmpty(): return examples.MajorityValue() # Baum mit neuem Test erweitern test = MaxInformationGain(examples, attr) tree = new DecisionTree(test) m = examples.MajorityValue() for v_i in test: ex_i = examples.select(test == v_i) st = ID3(ex_i, attr - test, m) tree.addBranch(label=v_i, subtree=st) return tree [Russell2020]: Man erhält aus dem \"Learn-Decision-Tree\"-Algorithmus [Russell2020, S. 678, Fig. 19.5] den hier vorgestellten ID3-Algorithmus, wenn man die Funktion $\\operatorname{Importance}(a, examples)$ als $\\operatorname{InformationGain}(examples, attr)$ implementiert/nutzt.\nHinweis: Mit der Zeile if examples.each(class == A): return A soll ausgedrückt werden, dass alle ankommenden Trainingsbeispiele die selbe Klasse haben und dass diese dann als Ergebnis zurückgeliefert wird. Das \"A\" steht im obigen Algorithmus nur symbolisch für die selbe Klasse! Es kann also auch ein anderes Klassensymbol als \"A\" sein ...\nBeispiel ID3 Nr. $x_1$ $x_2$ $x_3$ $k$ 1 0 0 0 A 2 1 0 2 A 3 0 1 1 A 4 1 1 0 B 5 0 1 1 B 6 0 1 0 A $x2$ höchsten Information Gain $x2=0$ =\u003e Beispiele 1,2 =\u003e A $x2=1$ =\u003e Beispiele 3,4,5,6 =\u003e Information Gain berechnen, weiter teilen und verzweigen Beobachtung: $\\operatorname{Gain}$ ist bei mehrwertigen Attributen höher Faire Münze:\nEntropie = $H(\\operatorname{Fair}) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1 \\operatorname{Bit}$ 4-seitiger Würfel:\nEntropie = $H(\\operatorname{Dice}) = -4\\cdot(0.25 \\log_2 0.25) = 2 \\operatorname{Bit}$ =\u003e $\\operatorname{Gain}$ ist bei mehrwertigen Attributen höher\nDamit würden Attribute bei der Wahl bevorzugt, nur weil sie mehr Ausprägungen haben als andere.\nAnmerkung: Im obigen Beispiel wurde einfach die Entropie für zwei \"Attribute\" mit unterschiedlich vielen Ausprägungen betrachtet, das ist natürlich kein $\\operatorname{Gain}(S, A)$. Aber es sollte deutlich machen, dass Merkmale mit mehr Ausprägungen bei der Berechnung des Gain für eine Trainingsmenge einfach wegen der größeren Anzahl an Ausprägungen rechnerisch bevorzugt würden.\nC4.5 als Verbesserung zu ID3 Normierter Informationsgewinn: $\\operatorname{Gain}(S, A) \\cdot \\operatorname{Normalisation}(A)$\n$$ \\operatorname{Normalisation}(A) = \\frac{1}{ \\sum_{v \\in \\operatorname{Values}(A)} p_v \\log_2 \\frac{1}{p_v} } $$ C4.5 kann zusätzlich u.a. auch noch mit kontinuierlichen Attributen umgehen, vgl. en.wikipedia.org/wiki/C4.5_algorithm.\nIn einem Paper (DOI 10.1007/s10115-007-0114-2) wurde der Algorithmus zu den \"Top 10 algorithms in data mining\" ausgewählt.\nIm Wikipedia-Artikel Information Gain finden Sie weitere Informationen zum \"Informationsgewinn\" (Information Gain).\nEin anderer, relativ ähnlich arbeitender Entscheidungsbaumlerner ist der CART (Classification And Regression Tree)-Algorithmus, wobei der Begriff \"CART\" allerdings oft auch einfach allgemein für \"Entscheidungsbaumlerner\" genutzt wird.\nHierzu drei lesenswerte Blog-Einträge:\nDeep dive into the basics of Gini Impurity in Decision Trees with math Intuition Decision Trees, Explained Decision Tree Algorithm With Hands-On Example Beispiele zur Normierung bei C4.5 Faire Münze:\nEntropie = $H(\\operatorname{Fair}) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1 \\operatorname{Bit}$ Normierung: $1/(0.5 \\log_2 (1/0.5) + 0.5 \\log_2 (1/0.5)) = 1/(0.5 \\cdot 1 + 0.5 \\cdot 1) = 1$ Normierter Informationsgewinn: $\\operatorname{Gain}(S, A) \\cdot \\operatorname{Normalisation}(A) = 1 \\operatorname{Bit} \\cdot 1 = 1 \\operatorname{Bit}$ 4-seitiger Würfel:\nEntropie = $H(\\operatorname{Dice}) = -4\\cdot(0.25 \\log_2 0.25) = 2 \\operatorname{Bit}$ Normierung: $1/(4\\cdot 0.25 \\log_2 (1/0.25)) = 1/(4\\cdot 0.25 \\cdot 2) = 0.5$ Normierter Informationsgewinn: $\\operatorname{Gain}(S, A) \\cdot \\operatorname{Normalisation}(A) = 2 \\operatorname{Bit} \\cdot 0.5 = 1 \\operatorname{Bit}$ =\u003e Normierung sorgt für fairen Vergleich der Attribute\nAnmerkung: Auch hier ist die Entropie natürlich kein $\\operatorname{Gain}(S, A)$. Das Beispiel soll nur übersichtlich deutlich machen, dass der \"Vorteil\" von Attributen mit mehr Ausprägungen durch die Normierung in C4.5 aufgehoben wird.\nWrap-Up Entscheidungsbaumlerner ID3 Nutze Information Gain zur Auswahl des nächsten Attributs Teile die Trainingsmenge entsprechend auf (\"nach unten hin\") Verbesserung durch Normierung des Information Gain: C4.5",
    "description": "Wie Attribute wählen? Erinnerung: CAL2/CAL3\nZyklische Iteration durch die Trainingsmenge Ausschließlich aktuelles Objekt betrachtet Reihenfolge der \"richtigen\" Attributwahl bei Verzweigung unklar =\u003e Betrachte stattdessen die komplette Trainingsmenge!\nErinnerung Entropie: Maß für die Unsicherheit Entropie $H(S)$ der Trainingsmenge $S$: relative Häufigkeit der Klassen zählen\nMittlere Entropie nach Betrachtung von Attribut $A$\n$$ R(S, A) = \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) $$ Informationsgewinn durch Betrachtung von Attribut $A$\n$$ \\begin{array}{rcl} \\operatorname{Gain}(S, A) \u0026=\u0026 H(S) - R(S, A)\\\\[5pt] \u0026=\u0026 H(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) \\end{array} $$ $R(S,A)$ ist die Unsicherheit/nötige Bits nach Auswahl von Attribut A. Je kleiner $R(S,A)$, um so kleiner die verbleibende Unsicherheit bzw. um so kleiner die Anzahl der nötigen Bits zur Darstellung der partitionierten Trainingsmenge nach Betrachtung von Attribut $A$ ...",
    "tags": [],
    "title": "ID3 und C4.5",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/dtl6-id3.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Problemlösen durch Suche im Problemgraphen. Aus den Basisalgorithmen Tree-Search und Graph-Search entstehen je nach verwendeter Datenstruktur und nach betrachteten Kosten unterschiedliche Suchalgorithmen.\nUninformierte Suche: ... jeder Schritt \"kostet\" gleich viel: nur die Anzahl der Schritte zählt ... Informierte Suche: ... Einsatz einer Kostenfunktion ... Lokale Suche: ... das Ziel ist im Weg ... Suche mit Tiefensuche Suche mit Breitensuche Suche mit Branch-and-Bound Suche mit Best First Suche mit A* Lokale Suche: Gradientensuche Lokale Suche: Simulated Annealing",
    "description": "Problemlösen durch Suche im Problemgraphen. Aus den Basisalgorithmen Tree-Search und Graph-Search entstehen je nach verwendeter Datenstruktur und nach betrachteten Kosten unterschiedliche Suchalgorithmen.\nUninformierte Suche: ... jeder Schritt \"kostet\" gleich viel: nur die Anzahl der Schritte zählt ... Informierte Suche: ... Einsatz einer Kostenfunktion ... Lokale Suche: ... das Ziel ist im Weg ... Suche mit Tiefensuche Suche mit Breitensuche Suche mit Branch-and-Bound Suche mit Best First Suche mit A* Lokale Suche: Gradientensuche Lokale Suche: Simulated Annealing",
    "tags": [],
    "title": "Suche",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Hole das Buch Das Beispiel ist ein Büroflur in der Uni. Neben den Büros gibt es eine Bibliothek und einen Kopiererraum, wo auch der Roboter sich gerade aufhält. Die Aufgabe für den Roboter lautet: Hole das Buch aus der Bibliothek (und bringe es zum Kopier). (Damit das Beispiel und der sich daraus ergebende Problemgraph nicht zu groß und zu unübersichtlich werden, soll das Ziel hier darin liegen, dass der Roboter das Buch in der Bibliothek aufnimmt.)\nEs stehen zwei Aktionen zur Verfügung:\nDer Roboter kann von einem in den nächsten Raum wechseln (Kosten siehe Pfeile) Der Roboter kann das Buch aufnehmen (Kosten: 3) Dabei sind die Durchgänge teilweise nur in einer Richtung zu benutzen (Pfeilrichtung).\nProblemgraph zum Kopiererbeispiel =\u003e Problemlösen == Suche im Graphen\nUninformierte (\"blinde\") Suche:\nKeine Informationen über die Kosten eines Pfades: Nur die Pfadlänge (Anzahl der Schritte) zählt.\nVarianten:\nTiefensuche Breitensuche Anmerkungen Wegesuche (Landkarte) Bei der Wegesuche hat man den Problemgraphen bereits durch die Orte und die Verbindungen (Straßen) zwischen ihnen gegeben. Es gibt nur eine ausführbare Aktion: \"fahre nach\".\nDabei können nur die Anzahl der Zwischenstationen auf dem Weg gezählt werden (\"uninformierte Suche\"), oder man ordnet den Kanten Kosten zu (bei der Wegesuche wären dies die Entfernungen zwischen den Orten oder die Zeit, die man von A nach B braucht) und landet damit bei der \"informierten Suche\".\nNormalerweise hat man eine Ordnung auf den Aktionen, d.h. für einen Knoten ergibt sich daraus eine Reihenfolge, in der die Aktionen angewendet werden und die Nachfolger expandiert werden. Bei der Wegesuche hat man dies nicht, insofern muss man willkürlich eine Ordnung festlegen. In dieser Veranstaltung ist dies die alphabetische Reihenfolge der Knoten (Orte).\nTiefensuche (TS, DFS) Erinnerung Tree-Search\nFüge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein Entnehme Knoten aus der Datenstruktur: Knoten ist gesuchtes Element: Abbruch, melde \"gefunden\" Expandiere alle Nachfolger des Knotens und füge diese in die Datenstruktur ein Falls die Datenstruktur leer ist: Abbruch, melde \"nicht gefunden\" Gehe zu Schritt 2 =\u003e Was passiert, wenn wir einen Stack einsetzen?\nBemerkungen Nachfolger eines Knotens: Alle von diesem Zustand durch Aktionen erreichbare Zustände\nSuchalgorithmus mit Stack als Datenstruktur =\u003e Tiefensuche\nZu betrachtender Knoten in Schritt 2 wird oben vom Stack genommen Expandierte Knoten werden in Schritt 2.a oben auf den Stack gelegt Dabei i.A. die vorgegebene Reihenfolge der Nachfolgeknoten beachten! Auswirkung: Weg wird in die Tiefe verfolgt (deshalb \"Tiefensuche\")\nIm [Russell2020] wird die Datenstruktur zum Halten der zu expandierenden Knoten (also hier im Fall der Tiefensuche der Stack) auch \"Frontier\" genannt.\nBacktracking: Wenn der Weg in eine Sackgasse führt, d.h. ein Knoten keine Nachfolger hat, werden bei der Expansion des Knotens keine Nachfolger auf den Stack gelegt. Die Evaluation des nächsten Knotens auf dem Stack bewirkt deshalb ein Zurückspringen im Suchbaum zum letzten Knoten auf dem aktuellen Weg mit noch offenen Alternativen ...\nKonventionen für diese Lehrveranstaltung In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im [Russell2020]). Dies erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt. Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die partiellen Wege aufzuschreiben.\nNicht Bestandteil der Algorithmen, dient aber der Nachvollziehbarkeit: Expandierte Knoten sollen alphabetisch sortiert an der korrekten Stelle in der Datenstruktur auftauchen, dabei soll aber natürlich die Reihenfolge der ursprünglich in der Datenstruktur enthaltenen Knoten nicht modifiziert werden. (Bei \"echten\" Problemen wird die Reihenfolge der expandierten Nachfolger in der Regel durch eine Reihenfolge der anwendbaren Operationen bestimmt.)\nWeitere Hinweise Die Tiefensuche wurde zufällig am Beispiel Tree-Search eingeführt. Man kann auch Graph-Search einsetzen. Wichtig ist nur, dass als Datenstruktur ein Stack genutzt wird.\nBei Tree-Search werden bereits besuchte Knoten u.U. immer wieder besucht. Zyklen im aktuell entwickelten Pfad sind also möglich! Außerdem sind mehrere Wege zum selben (Zwischen-/End-) Knoten in der Datenstruktur möglich!\nIm [Russell2020] wird der Begriff \"Backtracking\" für den rekursiven Tiefensuche-Algorithmus verwendet. Dies steht im Gegensatz zum üblichen Sprachgebrauch in der KI!\nTiefensuche (rekursive Variante) Startknoten ist gesuchtes Element: Abbruch, melde \"gefunden\" Für jeden Nachfolger des Startknotens: Rufe Tiefensuche für aktuellen (Nachfolger-) Knoten auf Ergebnis \"gefunden\": Abbruch, melde \"gefunden\" Abbruch, melde \"nicht gefunden\" Bemerkungen Eigenschaften wie \"normale\" Tiefensuche Einfacher zu implementieren: Nutzung des Stacks wird auf den Compiler verlagert (Funktionsaufruf, Stack des Prozesses ...) Speicherbedarf: Für jeden Knoten wird nur der nächste Knoten expandiert, plus Speicher für die Funktion Eigenschaften der Tiefensuche Siehe Breitensuche\nWrap-Up Uninformierte Suchverfahren Keine weiteren Pfadkosten (nur Anzahl der Schritte) Tiefensuche: Verfolge einen Pfad zuerst in die Tiefe Backtracking bei Sackgassen (automatisch durch den Stack)",
    "description": "Hole das Buch Das Beispiel ist ein Büroflur in der Uni. Neben den Büros gibt es eine Bibliothek und einen Kopiererraum, wo auch der Roboter sich gerade aufhält. Die Aufgabe für den Roboter lautet: Hole das Buch aus der Bibliothek (und bringe es zum Kopier). (Damit das Beispiel und der sich daraus ergebende Problemgraph nicht zu groß und zu unübersichtlich werden, soll das Ziel hier darin liegen, dass der Roboter das Buch in der Bibliothek aufnimmt.)",
    "tags": [],
    "title": "Suche mit Tiefensuche",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search1-dfs.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nUninformierte (\"blinde\") Suche:\nKeine Informationen über die Kosten eines Pfades: Nur die Pfadlänge (Anzahl der Schritte) zählt.\nVarianten:\nTiefensuche Breitensuche Breitensuche (BS, BFS) Erinnerung Graph-Search\nFüge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein Entnehme Knoten aus der Datenstruktur: Knoten ist gesuchtes Element: Abbruch, melde \"gefunden\" Markiere aktuellen Knoten, und Expandiere alle Nachfolger des Knotens und füge alle unmarkierten Nachfolger, die noch nicht in der Datenstruktur sind, in die Datenstruktur ein Falls die Datenstruktur leer ist: Abbruch, melde \"nicht gefunden\" Gehe zu Schritt 2 =\u003e Was passiert, wenn wir eine Queue einsetzen?\nBemerkungen Nachfolger eines Knotens: Alle von diesem Zustand durch Aktionen erreichbare Zustände\nSuchalgorithmus mit Queue als Datenstruktur =\u003e Breitensuche\nZu betrachtender Knoten in Schritt 2 wird vorn aus der Queue genommen Expandierte Knoten werden in Schritt 2.a hinten in die Queue eingefügt Dabei i.A. die vorgegebene Reihenfolge der Nachfolgeknoten beachten! Auswirkung: Suchbaum wird ebenenweise aufgebaut (deshalb \"Breitensuche\")\nGraph-Search: Markierte Knoten müssen geeignet gespeichert werden: separate Datenstruktur =\u003e Aufwand!\nKonventionen für diese Lehrveranstaltung In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im [Russell2020]). Dies erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt. Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die partiellen Wege aufzuschreiben.\nNicht Bestandteil der Algorithmen, dient aber der Nachvollziehbarkeit: Expandierte Knoten sollen alphabetisch sortiert an der korrekten Stelle in der Datenstruktur auftauchen, dabei soll aber natürlich die Reihenfolge der ursprünglich in der Datenstruktur enthaltenen Knoten nicht modifiziert werden. (Bei \"echten\" Problemen wird die Reihenfolge der expandierten Nachfolger in der Regel durch eine Reihenfolge der anwendbaren Operationen bestimmt.)\nWeitere Hinweise Die Breitensuche wurde zufällig am Beispiel Graph-Search eingeführt. Man kann auch die Tree-Search-Variante einsetzen. Wichtig ist nur, dass als Datenstruktur eine Queue genutzt wird.\nIm [Russell2020] wird die Breitensuche ebenfalls auf der Basis des Graph-Search-Algorithmus eingeführt. Allerdings wird die Abbruchbedingung modifiziert: Die Zielbedingung wird nicht erst (wie bei Graph-Search eigentlich definiert) geprüft, wenn ein Knoten aus der Queue entnommen wird, sondern bereits bei der Erzeugung der Nachfolgerknoten (vor dem Einfügen in die Queue). Dadurch spart man sich die Expansion einer zusätzlichen Ebene: Die Komplexität wäre in diesem Fall \"nur\" $O(b^{d})$.\nEigenschaften Breitensuche vs. Tiefensuche Tiefensuche Breitensuche Vollständigkeit nein1 ja2 Optimalität nein ja Zeitkomplexität $O(b^m)$ $O(b^{d+1})$ Speicherkomplexität $O(bm)$ $O(b^{d+1})$ Zeitkomplexität: maximal zu expandierende Knotenzahl Speicher: TS: in jeder Tiefe weitere $b$ Knoten speichern BS: alle Knoten einer Ebene im Speicher halten3 b: Verzweigungsfaktor, d: Ebene d. höchsten Lösungsknotens, m: Länge d. längsten Pfades\nPraxisvergleich Breitensuche vs. Tiefensuche Breitensuche: Annahme: $b=10$, 10.000 Knoten/s, 1.000 Byte/Knoten\nTiefe exp. Knoten Zeit Speicher 2 $10^3$ 0.1 s 1 MB 4 $10^5$ 10 s 100 MB 6 $10^7$ 20 min 10 GB 8 $10^9$ 30 h 1 TB 10 $10^{11}$ 130 d 100 TB Tiefensuche: Annahme: längster Pfad (Tiefe) $m=1000$\n=\u003e Speicherbedarf ca. 10 MB\nWrap-Up Uninformierte Suchverfahren Keine weiteren Pfadkosten (nur Anzahl der Schritte) Breitensuche: Verfolge alle Pfade (baue den Suchbaum ebenenweise auf) gilt für Tree-Search-Variante; vollständig in Graph-Search-Variante bei endlichem Suchraum ↩︎\nfalls b endlich ↩︎\n$O(b^{d})$ mit vorgezogener Zielprüfung (vgl. [Russell2020]) ↩︎",
    "description": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nUninformierte (\"blinde\") Suche:\nKeine Informationen über die Kosten eines Pfades: Nur die Pfadlänge (Anzahl der Schritte) zählt.\nVarianten:\nTiefensuche Breitensuche Breitensuche (BS, BFS) Erinnerung Graph-Search\nFüge Startknoten in leere Datenstruktur (Stack, Queue, ...) ein Entnehme Knoten aus der Datenstruktur: Knoten ist gesuchtes Element: Abbruch, melde \"gefunden\" Markiere aktuellen Knoten, und Expandiere alle Nachfolger des Knotens und füge alle unmarkierten Nachfolger, die noch nicht in der Datenstruktur sind, in die Datenstruktur ein Falls die Datenstruktur leer ist: Abbruch, melde \"nicht gefunden\" Gehe zu Schritt 2 =\u003e Was passiert, wenn wir eine Queue einsetzen?",
    "tags": [],
    "title": "Suche mit Breitensuche",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search2-bfs.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nInformierte Suche: Nutzung der Kostenfunktion:\nGesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf aktuellem Weg erreichter Knoten $g(n)$ tatsächliche Kosten für Weg vom Start bis Knoten $n$ $h(n)$ geschätzte Restkosten für Weg von Knoten $n$ zum Ziel =\u003e $h(n)$ wird auch \"heuristische Funktion\" oder \"Heuristik\" genannt Varianten:\nBranch-and-Bound Best First A* Branch-and-Bound (BnB) Variante der Breitensuche mit Kosten\nIdee: Expandiere den bisher günstigsten partiellen Weg\nKostenfunktion: $f(n) = g(n)$\nDatenstruktur: sortierte Queue (Prioritätsqueue)\nVoraussetzung: alle Aktionen haben positive Kosten (jeden Knoten $n$ gilt: $g(n) \u003e 0$)\nHinweis: Die Branch-and-Bound-Suche taucht im [Russell2020] als Erweiterung der \"Uniformen Suche\" auf ...\nBnB: Finde einen Weg von A nach H Bemerkungen zu BnB mit Graph-Search Graph-Search fordert: Expandierte Nachfolgerknoten, die schon in der Queue sind, sollen nicht (erneut) in die Queue aufgenommen werden.\nProblem dabei: Was ist mit den Kosten?! Der neu expandierte Weg könnte günstiger sein als der schon in der Queue enthaltene.\nLösung (vgl. Optimierungsmöglichkeiten für A*):\nFüge zunächst alle neu expandierten partiellen Pfade (mit unmarkierten Endknoten) in die Queue ein, sortiere diese und behalte von mehreren Pfaden zum gleichen Knoten nur den jeweils günstigsten in der Queue\nPfade, deren Endknoten bereits früher im Pfad vorkommt (Schleifen), werden bei Graph-Search in Schritt 2 nicht in die Queue aufgenommen (der Endknoten wäre bei einer Schleife ja bereits markiert und der neue Pfad würde bei Graph-Search nicht weiter beachtet).\nDas Einfärben ist kein Problem, da nur der jeweils günstigste Knoten aus der Queue genommen, gefärbt und expandiert wird. D.h. alle anderen Wege sind zu diesem Zeitpunkt bereits teurer. Wenn man nun (später) über einen anderen Weg zu einem bereits gefärbten Knoten kommt, kann der neue Weg nicht günstiger sein (positive Kosten vorausgesetzt).\nKonventionen für diese Lehrveranstaltung In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im [Russell2020]). Dies erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt. Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die partiellen Wege aufzuschreiben.\nAuf dem Papier sortiert sich die Queue schlecht, deshalb können Sie darauf verzichten, wenn Sie den im nächsten Schritt zu expandierenden Weg unterstreichen. Wer nicht mit Unterstreichen arbeiten will, muss eben dann manuell sortieren ...\nWenn bei der Graph-Search-Variante ein Weg nicht in die Queue aufgenommen wird, weil bereits ein anderer (günstigerer) Weg zum selben (Zwischen-/End-) Knoten bereits in der Queue enthalten ist, schreiben Sie dies geeignet auf. Dies gilt auch für den analogen Fall, wenn ein Weg aus der Queue entfernt wird, weil ein günstigerer Weg zum selben (Zwischen-/End-) Knoten eingefügt werden soll.\nEigenschaften von BnB Siehe A*\nWrap-Up Informierte Suchverfahren Nutzen reale Pfadkosten und/oder Schätzungen der Restkosten Branch-and-Bound: nur reale Pfadkosten $g(n)$",
    "description": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nInformierte Suche: Nutzung der Kostenfunktion:\nGesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf aktuellem Weg erreichter Knoten $g(n)$ tatsächliche Kosten für Weg vom Start bis Knoten $n$ $h(n)$ geschätzte Restkosten für Weg von Knoten $n$ zum Ziel =\u003e $h(n)$ wird auch \"heuristische Funktion\" oder \"Heuristik\" genannt Varianten:",
    "tags": [],
    "title": "Suche mit Branch-and-Bound",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search3-branchandbound.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nInformierte Suche: Nutzung der Kostenfunktion:\nGesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf aktuellem Weg erreichter Knoten $g(n)$ tatsächliche Kosten für Weg vom Start bis Knoten $n$ $h(n)$ geschätzte Restkosten für Weg von Knoten $n$ zum Ziel =\u003e $h(n)$ wird auch \"heuristische Funktion\" oder \"Heuristik\" genannt Varianten:\nBranch-and-Bound Best First A* Best-First (BF, BFS) Idee: Expandiere den partiellen Weg, der verspricht, dem Ziel am nächsten zu sein (Heuristik)\nKostenfunktion: $f(n) = h(n)$\nDatenstruktur: sortierte Queue (Prioritätsqueue)\nVoraussetzungen: $h(n)$ positiv, $h(n) = 0$ für den Zielknoten\nKonventionen BF In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im [Russell2020]). Dies erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt. Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die partiellen Wege aufzuschreiben.\nAuf dem Papier sortiert sich die Queue schlecht, deshalb können Sie darauf verzichten, wenn Sie den im nächsten Schritt zu expandierenden Weg unterstreichen. Wer nicht mit Unterstreichen arbeiten will, muss eben dann manuell sortieren ...\nWenn bei der Graph-Search-Variante ein Weg nicht in die Queue aufgenommen wird, weil bereits ein anderer (günstigerer) Weg zum selben (Zwischen-/End-) Knoten bereits in der Queue enthalten ist, schreiben Sie dies geeignet auf. Dies gilt auch für den analogen Fall, wenn ein Weg aus der Queue entfernt wird, weil ein günstigerer Weg zum selben (Zwischen-/End-) Knoten eingefügt werden soll.\nEigenschaften von BF Siehe A*\nWrap-Up Informierte Suchverfahren Nutzen reale Pfadkosten und/oder Schätzungen der Restkosten Best-First: nur Schätzungen $h(n)$",
    "description": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nInformierte Suche: Nutzung der Kostenfunktion:\nGesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf aktuellem Weg erreichter Knoten $g(n)$ tatsächliche Kosten für Weg vom Start bis Knoten $n$ $h(n)$ geschätzte Restkosten für Weg von Knoten $n$ zum Ziel =\u003e $h(n)$ wird auch \"heuristische Funktion\" oder \"Heuristik\" genannt Varianten:",
    "tags": [],
    "title": "Suche mit Best First",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search4-bestfirst.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nInformierte Suche: Nutzung der Kostenfunktion:\nGesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf aktuellem Weg erreichter Knoten $g(n)$ tatsächliche Kosten für Weg vom Start bis Knoten $n$ $h(n)$ geschätzte Restkosten für Weg von Knoten $n$ zum Ziel =\u003e $h(n)$ wird auch \"heuristische Funktion\" oder \"Heuristik\" genannt Varianten:\nBranch-and-Bound Best First A* A*-Suche Kombination aus Branch-and-Bound und Best-First-Suche\nKostenfunktion: $f(n) = g(n) + h(n)$\nDatenstruktur: sortierte Queue (Prioritätsqueue)\nVoraussetzung:\nAlle Aktionen haben positive Kosten ($g(n) \\ge \\epsilon$) Heuristik $h(n)$ muss zulässig/konsistent sein Konventionen für diese Lehrveranstaltung In der Beschreibung der Algorithmen werden häufig nur die letzten Knoten der partiellen Wege in den Datenstrukturen mitgeführt (das gilt auch für die Beschreibung im [Russell2020]). Dies erschwert die Nachvollziehbarkeit, wenn man die Queue oder den Stack schrittweise aufschreibt. Deshalb wird für diese Veranstaltung die Konvention eingeführt, immer die partiellen Wege aufzuschreiben.\nNotieren Sie die Bestandteile der Kosten für jeden partiellen Weg in der Queue einzeln: \"$g(n) + h(n) = f(n)$\". Das erleichtert Ihnen die weiteren Schritte, da Sie dort ja nur mit $g(n)$ weiter rechnen dürfen. Gleichzeitig erleichtert es die Nachvollziehbarkeit.\nAuf dem Papier sortiert sich die Queue schlecht, deshalb können Sie darauf verzichten, wenn Sie den im nächsten Schritt zu expandierenden Weg unterstreichen. Wer nicht mit Unterstreichen arbeiten will, muss eben dann manuell sortieren ...\nWenn bei der Graph-Search-Variante ein Weg nicht in die Queue aufgenommen wird, weil bereits ein anderer (günstigerer) Weg zum selben (Zwischen-/End-) Knoten bereits in der Queue enthalten ist, schreiben Sie dies geeignet auf. Dies gilt auch für den analogen Fall, wenn ein Weg aus der Queue entfernt wird, weil ein günstigerer Weg zum selben (Zwischen-/End-) Knoten eingefügt werden soll.\nA*-Suche -- Anforderungen an Heuristik (Tree-Search) Tree-Search-Variante: Die Heuristik muss zulässig sein:\nSeien $h^\\star(n)$ die tatsächlichen optimalen Restkosten von einem Knoten $n$ zum nächsten Ziel. Dann muss für jeden beliebigen Knoten $n$ gelten: $$h(n) \\le h^\\star(n)$$ Außerdem muss gelten: $h(n) \\ge 0$ für jeden Knoten $n$ $h(n) = 0$ für jeden Zielknoten $n$ =\u003e Beispiel: Luftlinie als Abschätzung\nHinweis: Im der englischen Ausgabe des [Russell2020] wird die zulässige Heuristik auch \"admissible heuristic\" genannt.\nA* ist optimal A* (Tree-Search-Variante) mit zulässiger Heuristik ist optimal.\nBeweis siehe Übung :-)\nEinfache Verbesserungen A* (Tree-Search) Dynamische Programmierung: Behalte von mehreren Pfaden zum gleichen Knoten nur den günstigsten in der Queue\nPfade, deren Endknoten bereits früher im Pfad vorkommt (Schleifen), werden in Schritt 2 nicht in die Queue aufgenommen\nÜbergang zur Graph-Search-Variante und Markierung von Knoten\n=\u003e Achtung: Dann schärfere Anforderungen an Heuristik (Konsistenz)\nA*-Suche -- Anforderungen an Heuristik (Graph-Search) Graph-Search-Variante: Die Heuristik muss konsistent sein:\nFür jeden Knoten $n$ und jeden durch eine Aktion $a$ erreichten Nachfolger $m$ gilt: $$h(n) \\le c(n,a,m) + h(m)$$ mit $c(n,a,m)$ Schrittkosten für den Weg von $n$ nach $m$ mit Aktion $a$.\nAußerdem muss gelten:\n$h(n) \\ge 0$ für jeden Knoten $n$ $h(n) = 0$ für jeden Zielknoten $n$ =\u003e Eine konsistente Heuristik ist gleichzeitig zulässig.\nHinweis: Im der englischen Ausgabe des [Russell2020] wird die konsistente Heuristik auch \"consistent heuristic\" genannt.\nEigenschaften Branch-and-Bound, Best-First, A* Branch-and-Bound Best-First A* Kosten $f(n) = g(n)$ $f(n) = h(n)$ $f(n) = g(n) + h(n)$ Vollständigkeit ja1 nein2 ja Optimalität ja nein ja Aufwand exponentiell exponentiell exponentiell Bemerkung Probiert erst alle \"kleinen\" Pfade Suchverlauf stark abh. v. Heuristik Heuristik: zulässig bzw. konsistent Wrap-Up Informierte Suchverfahren\nNutzen reale Pfadkosten und/oder Schätzungen der Restkosten A*: komplette Kostenfunktion $f(n) = g(n)+h(n)$ =\u003e besondere Anforderungen an die Heuristik! (Tree-Search: zulässige Heuristik; Graph-Search: konsistente Heuristik) Ausblick auf Verbesserungen der vorgestellten Suchverfahren:\nBeschränkung der Suchtiefe (\"Depth-Limited-Search\") Iterative Vertiefung der Suchtiefe (\"Iterative-Deepening-Search\"), beispielsweise IDA* (\"Iterative-Deepening A*\") Beschränkung des verwendeten Speichers, beispielsweise SMA* (\"Simplified Memory-Bounded A*\") BnB vollständig: Kosten größer Epsilon (positiv) ↩︎\ngilt für Tree-Search-Variante; vollständig bei Graph-Search und endlichen Problemräumen ↩︎",
    "description": "Hole das Buch =\u003e Problemlösen == Suche im Graphen\nInformierte Suche: Nutzung der Kostenfunktion:\nGesamtkosten: $f(n) = g(n) + h(n)$\n$n \\in S$ auf aktuellem Weg erreichter Knoten $g(n)$ tatsächliche Kosten für Weg vom Start bis Knoten $n$ $h(n)$ geschätzte Restkosten für Weg von Knoten $n$ zum Ziel =\u003e $h(n)$ wird auch \"heuristische Funktion\" oder \"Heuristik\" genannt Varianten:",
    "tags": [],
    "title": "Suche mit A*",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search5-astar.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "DTL.01: Entscheidungsbäume mit CAL3 und ID3 (6P) Es ist wieder Wahlkampf: Zwei Kandidaten O und M bewerben sich um die Kanzlerschaft. Die folgende Tabelle zeigt die Präferenzen von sieben Wählern.\nNr. Alter Einkommen Bildung Kandidat 1 $\\ge 35$ hoch Abitur O 2 $\u003c 35$ niedrig Master O 3 $\\ge 35$ hoch Bachelor M 4 $\\ge 35$ niedrig Abitur M 5 $\\ge 35$ hoch Master O 6 $\u003c 35$ hoch Bachelor O 7 $\u003c 35$ niedrig Abitur M Trainieren Sie nacheinander mit den Verfahren CAL3 (3P) und ID3 (3P) auf der obigen Trainingsmenge je einen Entscheidungsbaum. Nutzen Sie für CAL3 dabei die Schwellen $S_1=4$ und $S_2=0.7$.\nSie können dafür eine Handsimulation anwenden oder die Algorithmen implementieren. Sie können gern auch die Java-Klassen im Paket aima.core.learning bzw. die Python-Klassen in learning.py als Ausgangspunkt nutzen.1\nDTL.02: Pruning (1P) Vereinfachen Sie schrittweise den Baum\n$$x_3(x_2(x_1(C,A), x_1(B,A)), x_1(x_2(C,B), A))$$ so weit wie möglich.\nNutzen Sie die linearisierte Schreibweise. Geben Sie die jeweils verwendete Regel an.\nThema: Anwendung der Transformations- und Pruning-Regeln\nDTL.03: Machine Learning mit Weka (3P) Weka (waikato.github.io/weka-wiki/) ist eine beliebte Sammlung von (in Java implementierten) Algorithmen aus dem Bereich des Maschinellen Lernens. Laden Sie sich das Tool in der aktuellen stabilen Version herunter und machen Sie sich mit der beiliegenden Dokumentation vertraut.\nLaden Sie sich die Beispieldatensätze \"Zoo\" (zoo.csv) und \"Restaurant\" (restaurant.csv) aus dem AIMA-Repository (github.com/aimacode/aima-data) herunter.2 Zum Laden der Beispieldatensätze in Weka müssen die .csv-Dateien eine Kopfzeile mit den Namen der Attribute haben. Passen Sie die Dateien entsprechend an und laden Sie diese im Reiter \"Pre-Process\" mit \"Open file ...\".\nHinweis: Wenn Sie Weka 3.6 einsetzen, sind alle für dieses Blatt erforderlichen Algorithmen bereits vorhanden. In neueren Versionen müssen Sie in der Weka-Haupt-GUI den Paketmanager unter \"Tools\" starten und dort nach einem Paket suchen, welches ID3 enthält, und dieses Paket nachinstallieren.\nTraining mit J48 (1P)\nWechseln Sie auf den Reiter \"Classify\" und wählen Sie mit dem Button \"Choose\" den Entscheidungsbaum-Lerner J48 aus. (Dies ist eine Java-Implementierung von C4.5. Die ID3-Implementierung funktioniert für den zoo.csv-Datensatz leider nicht ...)\nLernen Sie für die beiden Datensätze je einen Entscheidungsbaum. Wie sehen die Bäume aus? Wie hoch ist jeweils die Fehlerrate für den Trainingssatz? (Stellen Sie unter \"Test options\" den Haken auf \"Use training set\".) Interpretieren Sie die Confusion Matrix.\nARFF-Format (1P)\nLesen Sie in der beiliegenden Doku zum Thema \"ARFF\" nach. Dabei handelt es sich um ein spezielles Datenformat, womit man Weka mitteilen kann, welche Attribute es gibt und welchen Typ diese haben und welche Werte auftreten dürfen. (Link)\nErklären Sie die Unterschiede zwischen \"nominal\", \"ordinal\" (bzw. \"numeric\") und \"string\".\nKonvertieren Sie den Zoo- und Restaurantdatensatz in das ARFF-Format. Beachten Sie, dass die ID3-Implementierung von Weka nicht mit bestimmten Attributtypen umgehen kann.\nTraining mit ID3 und J48 (1P)\nTrainieren Sie für die im letzten Schritt erstellten Datensätze (Zoo und Restaurant) im ARFF-Format erneut Entscheidungsbäume. Nutzen Sie diesmal sowohl ID3 als auch J48.\nVergleichen Sie wieder die Ergebnisse (Entscheidungsbäume, Fehlerraten, Confusion Matrix) untereinander und mit den Ergebnissen aus dem J48-Lauf mit den .csv-Dateien.\nThema: Kennenlernen von Weka\nIm Python-Code tauchen immer wieder \"TODO\"-Marker auf - bitte mit Vorsicht genießen! ↩︎\nZum Zoo-Datensatz gibt es die Erklärung direkt im Repo, für den Restaurant-Datensatz finden Sie die Erklärung im AIMA (Buch). ↩︎",
    "description": "DTL.01: Entscheidungsbäume mit CAL3 und ID3 (6P) Es ist wieder Wahlkampf: Zwei Kandidaten O und M bewerben sich um die Kanzlerschaft. Die folgende Tabelle zeigt die Präferenzen von sieben Wählern.\nNr. Alter Einkommen Bildung Kandidat 1 $\\ge 35$ hoch Abitur O 2 $\u003c 35$ niedrig Master O 3 $\\ge 35$ hoch Bachelor M 4 $\\ge 35$ niedrig Abitur M 5 $\\ge 35$ hoch Master O 6 $\u003c 35$ hoch Bachelor O 7 $\u003c 35$ niedrig Abitur M Trainieren Sie nacheinander mit den Verfahren CAL3 (3P) und ID3 (3P) auf der obigen Trainingsmenge je einen Entscheidungsbaum. Nutzen Sie für CAL3 dabei die Schwellen $S_1=4$ und $S_2=0.7$.",
    "tags": [],
    "title": "Übungsblatt: Entscheidungsbäume (Decision Tree Learner DTL)",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-dtl.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Unterschiede in den Suchproblemen? Bisher betrachtete Suchverfahren:\nSystematische Erkundung des Suchraums Weg zur Lösung wichtig =\u003e Oft aber nur das Ziel an sich interessant! (Und nicht, wie man dort hin gelangt.)\nBeispiel: Stundenplan\nAnalogie: Bergsteigen ohne Karte und Pfade Gradienten-Suche: \"Gehe in Richtung des steilsten Anstiegs der Zielfunktion.\"\n=\u003e Schrittweise Verbesserung des aktuellen Zustands (Lokale Suche)\nVerschiedene Namen: \"Hill-climbing\", \"Greedy local search\" Kann auch als Minimierung angewendet werden Pseudoalgorithmus Gradientensuche \"Wie Bergsteigen am Mount Everest in dickem Nebel mit Gedächtnisverlust\"\nSetze currNode auf den Startknoten currNode ist gesuchtes Element: Abbruch, melde \"gefunden\" Expandiere alle Nachfolger von currNode Setze nextNode auf Nachfolger mit höchster Bewertung Falls Bewertung von nextNode $\\leq$ Bewertung von currNode: Abbruch, melde \"nicht gefunden\" Setze currNode auf nextNode Gehe zu Schritt 2 Beispiel Gradientensuche: $n$-Damen Ziel: Setze $n$ Damen auf ein $n \\times n$-Spielfeld ohne Konflikte Start: Setze $n$ Damen auf ein $n \\times n$-Spielfeld (mit Konflikten) Suche: Bewege jeweils eine Dame so, daß die Anzahl der Konflikte reduziert wird Schauen Sie sich auch Abb. 4.3 auf Seite 130 im [Russell2020] an!\nHinweis: Alle Damen stehen von Anfang an auf dem Brett und werden nur verschoben =\u003e \"vollständige Zustandsformulierung\"\nEigenschaften 8-Damen-Problem ($n=8$) Zustandsraum: $8^8 \\approx 17$ Millionen Zustände! Beginnend mit zufällig erzeugtem Startzustand: bleibt in 86% der Fälle stecken, d.h. findet Lösung nur in 14% der Fälle. Beobachtung: Lösung nach durchschnittlich 4 Schritten, oder Verfahren bleibt nach durchschnittlich 3 Schritten stecken. Quelle: nach [Russell2020, p. 131]\nEigenschaften Gradientensuche Vollständigkeit: nein Optimalität: nein Komplexität: linear in der Anzahl der zu expandierenden Knoten Zielfunktion (Bewertung) nötig!\nProblem: lokale Maxima und Plateaus\nLokale Maxima/Minima: Algorithmus findet nur eine suboptimale Lösung Plateaus: Hier muss der Algorithmus mit zufälligen Zügen explorieren Wrap-Up Lokale Suchverfahren: Nur das Ergebnis zählt!\nGradientenverfahren: Gehe in Richtung des stärksten Anstiegs der Kostenfunktion",
    "description": "Unterschiede in den Suchproblemen? Bisher betrachtete Suchverfahren:\nSystematische Erkundung des Suchraums Weg zur Lösung wichtig =\u003e Oft aber nur das Ziel an sich interessant! (Und nicht, wie man dort hin gelangt.)\nBeispiel: Stundenplan\nAnalogie: Bergsteigen ohne Karte und Pfade",
    "tags": [],
    "title": "Lokale Suche: Gradientensuche",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search6-gradient.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Suche",
    "content": "Motivation Problem: lokale Maxima und Plateaus\nLokale Maxima/Minima: Algorithmus findet nur eine suboptimale Lösung Plateaus: Hier muss der Algorithmus mit zufälligen Zügen explorieren Mögliche Lösungen:\nNeustart des Algorithmus, wenn kein Fortschritt erzielt wird Rauschen \"injizieren\" Gedankenexperiment: Ausweg aus lokalen Minima \"Drehen der Landschaft\": Minimieren statt Maximieren Ball wird in Zustandsraum-Landschaft gesetzt. Folge: rollt steilsten Abstieg hinunter rollt evtl. in Tal auf halber Höhe (lokales Minimum) =\u003e bleibt dort gefangen =\u003e \"Schütteln der Landschaft\" -- Ball springt aus dem Tal und rollt in anderes Tal\nNicht zu stark schütteln -- sonst wird u.U. globales Minimum verlassen!\nAnalogie Härten von Metall Metall erhitzen bis Atome frei beweglich Langsam abkühlen =\u003e stabiles Atomgitter mit minimalem Energiezustand\nÜbertragen der Idee Starkes \"Schütteln\" (hohe \"Temperatur\") am Anfang Schrittweises \"Abkühlen\" =\u003e \"Schütteln\" im Laufe der Zeit verringern =\u003e Simulated Annealing\nPseudocode Simulated Annealing (Minimierungsproblem) def simulated_annealing(problem): current = problem.startNode t = 0; temp = schedule(t) while temp\u003e0: temp = schedule(++t) neighbors = current.expandSuccessors() if not neighbors: return current working = random.choice(neighbors) dE = problem.value(current) - problem.value(working) if dE \u003e 0 or probability(math.exp(dE/temp)): current = working return current Wenn dE positiv ist, dann ist der Nachfolger \"besser\" (hier: kleiner bewertet) als der aktuelle Knoten und wird immer als nächster Knoten übernommen.\nWenn dE negativ ist, dann ist der betrachtete Nachfolger \"schlechter\" (hier: größer bewertet) als der aktuelle Knoten. Dann wird er mit einer Wahrscheinlichkeit math.exp(dE/temp) als nächster Knoten übernommen. Diese Wahrscheinlichkeit ist bei hohen Temperaturen temp eher hoch, und sinkt, je niedriger die Temperatur temp wird.\nDie Temperatur temp bewegt sich dabei von hohen positiven Werten auf den Wert Null (wird also nicht negativ).\nDetail: Akzeptieren von Verschlechterungen Quelle: \"Exp e.svg\" by Marcel Marnitz, reworked by Georg-Johann on Wikimedia Commons (Public Domain)\nWahrscheinlichkeit zum Akzeptieren einer Verschlechterung: math.exp(dE/temp) $dE$ negativ =\u003e $\\exp\\left(\\text{dE}/\\text{temp}\\right) = \\exp\\left(-\\frac{|\\text{dE}|}{\\text{temp}}\\right) = \\frac{1}{\\exp\\left(\\frac{|\\text{dE}|}{\\text{temp}}\\right)}$ $\\exp(a)$ bzw. $e^a$:\n$a\u003c0$: geht gegen 0 $a=0$: 1 $a\u003e0$: steil (exponentiell) gegen Unendlich ... Wenn $dE$ negativ ist, wird math.exp(dE/temp) ausgewertet. Damit ergibt sich wegen $dE$ negativ: $\\exp\\left(\\text{dE}/\\text{temp}\\right) = \\exp\\left(-\\frac{|\\text{dE}|}{\\text{temp}}\\right) = \\frac{1}{\\exp\\left(\\frac{|\\text{dE}|}{\\text{temp}}\\right)}$. Betrachtung für $dE$ (nur negativer Fall!) und $\\text{temp}$:\nTemperatur $\\text{temp}$ hoch: $a = \\frac{|\\text{dE}|}{\\text{temp}}$ ist positiv und klein (nahe Null), d.h. $\\exp(a)$ nahe 1 (oder größer), d.h. die Wahrscheinlichkeit $1/\\exp(a)$ ist nahe 1 (oder kleiner) Temperatur $\\text{temp}$ wird kleiner und geht gegen Null: $a = \\frac{|\\text{dE}|}{\\text{temp}}$ ist positiv und wird größer, d.h. $\\exp(a)$ geht schnell gegen Unendlich, d.h. die Wahrscheinlichkeit $1/\\exp(a)$ geht gegen 0 Abkühlungsplan problemabhängig wählen Initiale Temperatur: So hoch, daß praktisch jede Änderung akzeptiert wird\nAbkühlen: $T_k = \\alpha T_{k-1}$ mit $0.8 \\le \\alpha \\le 0.99$\nAusreichend langsam abkühlen! Typisch: jede Stufe so lange halten, daß etwa 10 Änderungen akzeptiert wurden Stop: Keine Verbesserungen in 3 aufeinander folgenden Temperaturen\nDer Abkühlungsplan muss problemabhängig gewählt werden. Das Beispiel zeigt typische Elementes eines solchen Abkühlungsplans.\nEigenschaften Simulated Annealing Vollständigkeit: ja (mit gewisser Wahrscheinlichkeit) Optimalität: ja (mit gewisser Wahrscheinlichkeit) Voraussetzung: geeigneter Abkühlungsplan\nAnwendungen von Simulated Annealing Flugplan-Scheduling Layout-Probleme (Chipentwurf, Leiterplatten) Produktionsplanung Wrap-Up Lokale Suchverfahren: Nur das Ergebnis zählt!\nGradientenverfahren Analogie Bergsteigen: Gehe in Richtung des stärksten Anstiegs der Kostenfunktion =\u003e Hill-Climbing Achtung: Probleme mit lokalen Minima =\u003e Simulated Annealing",
    "description": "Motivation Problem: lokale Maxima und Plateaus\nLokale Maxima/Minima: Algorithmus findet nur eine suboptimale Lösung Plateaus: Hier muss der Algorithmus mit zufälligen Zügen explorieren Mögliche Lösungen:\nNeustart des Algorithmus, wenn kein Fortschritt erzielt wird Rauschen \"injizieren\" Gedankenexperiment: Ausweg aus lokalen Minima \"Drehen der Landschaft\": Minimieren statt Maximieren Ball wird in Zustandsraum-Landschaft gesetzt. Folge: rollt steilsten Abstieg hinunter rollt evtl. in Tal auf halber Höhe (lokales Minimum) =\u003e bleibt dort gefangen =\u003e \"Schütteln der Landschaft\" -- Ball springt aus dem Tal und rollt in anderes Tal",
    "tags": [],
    "title": "Lokale Suche: Simulated Annealing",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/searching/search7-annealing.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Lokale Suche mit Methoden, die der biologischen Evolution abgeschaut bzw. nachempfunden sind.\nEinführung Evolutionäre Algorithmen Modellierung mit Genetischen Algorithmen",
    "description": "Lokale Suche mit Methoden, die der biologischen Evolution abgeschaut bzw. nachempfunden sind.\nEinführung Evolutionäre Algorithmen Modellierung mit Genetischen Algorithmen",
    "tags": [],
    "title": "Genetische Algorithmen",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Genetische Algorithmen",
    "content": "Evolution sehr erfolgreich bei Anpassung Quelle: Photo by Johannes Plenio on Unsplash.com (Unsplash License)\nWie funktioniert's?\nEA -- Zutaten und Mechanismen Zutaten:\nIndividuen: Kodierung möglicher Lösungen Population von Individuen Fitnessfunktion: Bewertung der Angepasstheit Mechanismen (\"Operatoren\"):\nSelektion Rekombination (Crossover) Mutation EA -- Allgemeiner Ablauf EA -- Beispiel Jedes Individuum kodiert ein Spielfeld mit einer konkreten Anordnung aller Königinnen =\u003e Vollständige Zustandsbeschreibung.\nDabei korrespondiert der Index in das Array des Individuums mit der jeweiligen Spalte des Spielfelds. Die Zahl an einer Arrayposition gibt dann an, in welcher Zeile in dieser Spalte eine Königin ist.\nCrossover: Die ausgewählten Individuen werden an der selben Stelle aufgetrennt und die Hälften verkreuzt zu zwei neuen Individuen zusammengesetzt. Es entstehen zwei neue Anordnungen der Königinnen (zwei neue Spielfelder).\nEA -- Strömungen Genetische Algorithmen (GA)\nHolland und Goldberg (ab 1960) Binäre Lösungsrepräsentation (Bitstring): $\\mathbf{g} = (g_1, \\dots, g_m)\\in \\{ 0,1\\}^m$ Fitnessbasierte stochastische Selektion $\\mu$ Eltern erzeugen $\\mu$ Kinder Evolutionsstrategien (ES)\nRechenberg und Schwefel (ab 1960) Kodierung reellwertiger Parameter: $\\mathbf{g} = (\\mathbf{x}, \\mathbf{\\sigma})$ mit $\\mathbf{x} = (x_1, \\dots, x_n) \\in \\mathbb{R}^n$ $\\mu$ Eltern erzeugen $\\lambda$ Kinder mit $\\mu \\le \\lambda$ Evolutionäre Programmierung (EP)\nHinweis: Häufig finden sich Mischformen, beispielsweise GA mit reellwertigen Parametern\nHinweis: Im Folgenden werden Genetische Algorithmen (GA) betrachtet. Sie finden jeweils Hinweise auf die Gestaltung der Operatoren bei ES.\nAnwendungsbeispiele für Evolutionäre Algorithmen Berechnung und Konstruktion komplexer Bauteile: beispielsweise Tragflächenprofile (Flugzeuge), Brücken oder Fahrzeugteile unter Berücksichtigung bestimmter Nebenbedingungen Scheduling-Probleme: Erstellung von Stunden- und Raumplänen oder Fahrplänen Berechnung verteilter Netzwerktopologien: Wasserversorgung, Stromversorgung, Mobilfunk Layout elektronischer Schaltkreise Wrap-Up Lokale Suchverfahren: Nur das Ergebnis zählt!\nEvolutionäre Algorithmen: Unterschied GA und ES (grober Überblick)",
    "description": "Evolution sehr erfolgreich bei Anpassung Quelle: Photo by Johannes Plenio on Unsplash.com (Unsplash License)\nWie funktioniert's?\nEA -- Zutaten und Mechanismen Zutaten:\nIndividuen: Kodierung möglicher Lösungen Population von Individuen Fitnessfunktion: Bewertung der Angepasstheit Mechanismen (\"Operatoren\"):\nSelektion Rekombination (Crossover) Mutation EA -- Allgemeiner Ablauf EA -- Beispiel",
    "tags": [],
    "title": "Einführung Evolutionäre Algorithmen",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea1-intro.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Genetische Algorithmen",
    "content": "EA -- Allgemeiner Ablauf Kodierung Individuen Binäre Lösungsrepräsentation (Bitstring): $\\mathbf{g} = (g_1, \\dots, g_m)\\in \\{ 0,1\\}^m$\nString gliedert sich in $n$ Elemente (mit $n \\le m$) =\u003e jedes Segment entspricht einer Problemvariablen Dekodierungsfunktion $\\Gamma : \\{0,1\\}^m \\to \\mathbb{R}^n$ Alle relevanten Aspekte des Problems müssen in die Codierung einfließen!\nBei ES hat man einen Vektor mit reellen Zahlen, wobei jeder Eintrag einen Parameter des Problems darstellt. Eine Dekodierungsfunktion benötigt man entsprechend nicht.\nBei der Erzeugung der Startpopulation werden die Individuen zufällig (mit zufälligen Werten) initialisiert.\nFitnessfunktion $\\Phi$ ordnet jedem Individuum $\\mathbf{g}_i$ eine reelle Zahl zu: $$\\Phi(\\mathbf{g}_i) = F(\\Gamma(\\mathbf{g}_i)) - w\\cdot\\sum_j(Z_j(\\Gamma(\\mathbf{g}_i)))^2$$\nZielfunktion $F$: wie sehr genügt ein Individuum bereits dem Optimierungproblem Strafterme $Z_j$: Anreicherung der Optimierung mit weiteren Informationen Gewichte $w$: statisch oder dynamisch (Abkühlen) Die Wahl einer guten Fitnessfunktion ist oft eine Herausforderung, aber dennoch wichtig, da damit die Suche gesteuert wird!\nSelektion: Erstelle Matingpool mit $\\mu$ Individuen Fitnessproportionale Selektion (Roulette Wheel Selection): Auswahlwahrscheinlichkeit für Individuum $\\mathbf{g}_k$: $$p_{sel}(\\mathbf{g}_k) = \\frac{\\Phi(\\mathbf{g}_k)}{\\sum_j \\Phi(\\mathbf{g}_j)}$$ =\u003e Voraussetzung: positive Fitnesswerte\nTurnier-Selektion (Tournament Selection):\nTurniergröße $\\xi$ Turnier: ziehe $\\xi$ Individuen gleichverteilt (mit Zurücklegen!) und kopiere fittestes Individuum in den Matingpool Führe $\\mu$ Turniere durch Hinweis: Es gibt noch viele weitere Selektionsmechanismen. Die vorgestellten sind in der Praxis am gebräuchlichsten.\nÜber die Selektion wird der sogenannte \"Selektionsdruck\" aufgebaut: Wie gut muss ein Individuum sein (im Vergleich zu den restlichen Individuen in der Population), damit es eine Chance zur Reproduktion erhält? Dürfen sich nur die \"Guten\" fortpflanzen, oder erhalten auch die \"Schlechten\" eine gewisse Chance?\nDa jedes Individuum einen Punkt im Suchraum darstellt, beeinflusst die Wahl der Selektion die Geschwindigkeit der Suche, begünstigt u.U. aber auch ein eventuelles Festfahren in lokalen Minima. Dies kann beispielsweise geschehen, wenn immer nur die \"Guten\" selektiert werden, aber die \"Guten\" der Population sich in der Nähe eines lokalen Minimums befinden. Dann werden auch die Nachfolger sich wieder dort aufhalten.\nCrossover: Erzeuge zwei Nachkommen aus zwei Eltern Festlegung der Crossover-Wahrscheinlichkeit $p_{cross}$ (typisch: $p_{cross} \\ge 0.6$)\nSelektiere Eltern $\\mathbf{g}_a$ und $\\mathbf{g}_b$ gleichverteilt aus Matingpool\nZufallsexperiment:\nmit $1-p_{cross}$: Kinder identisch zu Eltern (kein Crossover)\nmit $p_{cross}$: Crossover mit $\\mathbf{g}_a$ und $\\mathbf{g}_b$\nZiehe $i$ gleichverteilt mit $1 \u003c i \u003c m$ Kinder aus $\\mathbf{g}_a$ und $\\mathbf{g}_b$ zusammenbauen: $$\\mathbf{g}_c = (g_{a,1}, \\dots, g_{a,i}, \\; g_{b,{i+1}}, \\dots, g_{b,m})$$ und $$\\mathbf{g}_d = (g_{b,1}, \\dots, g_{b,i}, \\; g_{a,{i+1}}, \\dots, g_{a,m})$$ =\u003e Trenne Eltern an gleicher Stelle auf, vertausche Bestandteile\nGehe zu Schritt 1, bis insg. $\\mu$ Nachkommen\nAnmerkung: Die Eltern werden jeweils in die Ausgangsmenge zurückgelegt.\nMit einer kleinen Wahrscheinlichkeit sind die Kinder also identisch zu den Eltern. Dies ist im Sinne der lokalen Suche wichtig, um bereits erreichte gute Positionen im Suchraum nicht zu verlieren: Es könnte sein, dass die Nachfolger alle schlechter sind ...\nVarianten: $N$-Punkt-Crossover, Shuffle-Crossover\nBei ES wird parameterweise gekreuzt. Dabei gibt es verschiedene Möglichkeiten: Übernahme eines Parameters von einem Elternteil, Verrechnen (beispielsweise Mitteln) der Werte beider Eltern, ... Bei ES heißt \"Crossover\" deshalb oft \"Rekombination\".\nMutation Mutationswahrscheinlichkeit $p_{mut}$ (typische Werte: $p_{mut} = 0.01$ oder $p_{mut} = 0.001$)\nFür alle Individuen:\nMutiere jedes Gen eines Individuums mit $p_{mut}$:\n$$ g_i^{(t+1)} = \\left\\{ \\begin{array}{ll} \\neg g_i^{(t)} \u0026 \\mbox{ falls } \\chi_i \\le p_{mut}\\\\[5pt] \\phantom{\\neg} g_i^{(t)} \u0026 \\mbox{ sonst } \\end{array} \\right. $$ =\u003e$\\chi_i$ gleichverteilte Zufallsvariable (Intervall $[0,1]$), für jedes Bit $g_i$ neu bestimmen\nAnmerkung: Die optimale Mutationsrate $p_{mut}^*$ ist von Länge $m$ des Bitstrings abhängig; annäherbar durch $p_{mut}^* \\approx 1/m$.\nDie beim Crossover erstellten Nachfolger liegen im Suchraum in der Nähe der Eltern. Durch die Mutationsrate bestimmt man, ob und wie weit sich ein Kind entfernen kann. Dies entspricht dem Bild des \"Schüttelns\" der Zustandslandschaft.\nBei ES unterscheidet man Mutationswahrscheinlichkeit und Mutationsrate. Es wird parameterweise mutiert.\nBewertungskriterien Vorsicht: Es handelt sich um Zufallsexperimente. Wenn man nicht nur direkt nach einer Lösung sucht, sondern beispielsweise Parametereinstellungen oder die Wahl der Fitnessfunktion für ein Problem vergleichen will, muss man jeweils mehrere Experimente mit der selben Einstellung machen und Kenngrößen berechnen.\nGeschwindigkeit: AES Average Evaluations to a Solution $$ \\mbox{AES } = \\frac{\\sum\\limits_{i \\in \\mbox{erfolgreiche Läufe}} \\mbox{Generationen von Lauf } i}{\\mbox{Anzahl der erfolgreichen Läufe}} $$\nDie AES liegt im Intervall $[0, maxGen]$.\nLösungswahrscheinlichkeit: SR Success Rate $$ \\mbox{SR } = \\frac{\\mbox{Anzahl der erfolgreichen Läufe}}{\\mbox{Anzahl aller Läufe}} $$\nDie SR liegt im Intervall $[0, 1]$.\nTypische Läufe Populationsgröße $\\mu=15$ Anzahl Nachfahren $\\lambda=100$ Abbruch nach $maxGen=200$ Generationen Stochastischer Algorithmus! Ausreichend Wiederholungen durchführen und mitteln!\nHinweis: Die Parameter müssen problemabhängig gewählt werden. Zu hohe Werte für $\\mu$ und $\\lambda$ führen dazu, dass man bei kleinen Problemen mit hoher Wahrscheinlichkeit bereits am Anfang eine Lösung \"würfelt\", also gar kein GA nutzt. Wenn dies allerdings nicht passiert, sorgt eine hohe Populationsgröße dafür, dass jeder Schritt sehr lange dauert. Die Abbruchgrenze ist ebenfalls mit Augenmaß zu wählen: Ein zu kleiner Wert sorgt für zu frühen Abbruch (keine Lösung!), ein zu hoher Wert sorgt beim Festfressen des Algorithmus für eine unnötige weitere \"Suche\" ...\nWrap-Up Lokale Suchverfahren: Nur das Ergebnis zählt!\nEvolutionäre Algorithmen: Begriffe: Individuum, Population, Kodierung Operationen: Selektion, Rekombination, Mutation Bewertung mit Fitnessfunktion",
    "description": "EA -- Allgemeiner Ablauf Kodierung Individuen Binäre Lösungsrepräsentation (Bitstring): $\\mathbf{g} = (g_1, \\dots, g_m)\\in \\{ 0,1\\}^m$\nString gliedert sich in $n$ Elemente (mit $n \\le m$) =\u003e jedes Segment entspricht einer Problemvariablen Dekodierungsfunktion $\\Gamma : \\{0,1\\}^m \\to \\mathbb{R}^n$ Alle relevanten Aspekte des Problems müssen in die Codierung einfließen!\nBei ES hat man einen Vektor mit reellen Zahlen, wobei jeder Eintrag einen Parameter des Problems darstellt. Eine Dekodierungsfunktion benötigt man entsprechend nicht.",
    "tags": [],
    "title": "Modellierung mit Genetischen Algorithmen",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ea2-ga.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "Search.01: Problemformalisierung, Zustandsraum (3P) Drei Elben und drei Orks befinden sich an einem Ufer eines Flusses und wollen diesen überqueren. Es steht dazu ein Pferd zur Verfügung, welches maximal zwei Wesen tragen kann. Das Pferd kann den Fluss nicht allein überqueren.\nGesucht ist eine Möglichkeit, alle Elben und Orks über den Fluss zu bringen. Dabei darf zu keiner Zeit an keinem Ufer die Anzahl der sich dort befindlichen Orks größer sein als die der dort wartenden Elben, da es sonst zu Konflikten zwischen beiden Gruppen kommt.\nFormalisieren Sie das Problem (Zustände, Aktionen, Start- und Endzustand). Skizzieren Sie den Problemgraph. Thema: Formalisierung von Problemen, Problemgraph\nSearch.02: Suchverfahren (5P) Betrachten Sie folgende Landkarte und Restwegschätzungen:\nQuelle: MapGermanyGraph.svg by Regnaron and Jahobr on Wikimedia Commons (Public Domain)\nFinden Sie nacheinander mit Tiefensuche (1P) und Breitensuche (1P) (jeweils in der Graph-Search-Variante) sowie A* (2P) (in der Tree-Search-Variante mit der Verbesserung \"keine Zyklen\", siehe Vorlesung) jeweils einen Weg von Würzburg nach München.\nVergleichen Sie die drei Algorithmen: Wie viele Einträge gibt es in der Datenstruktur maximal, wie oft wird die Hauptschleife durchlaufen (also ein Element aus der Datenstruktur entnommen, untersucht und weiterentwickelt)?\nSie können dafür eine Handsimulation anwenden oder die Algorithmen implementieren. Sie können gern auch die Java-Klassen im Paket aima.core.search bzw. die Python-Klassen in search.py als Ausgangspunkt nutzen.1\nDürfen die oben gegebenen Restkostenabschätzungen in A* verwendet werden? (1P)\nFalls ja, warum? Falls nein, warum? Wie müssten die Abschätzungen ggf. korrigiert werden? Falls Sie der Meinung waren, die Abschätzungen sind nicht korrekt, korrigieren Sie die Abschätzungen nun und führen Sie erneut eine Suche mit A* durch.\nHinweis: Reihenfolge bei gleichen $f(n)$-Kosten: alphabetische Reihenfolge, d.h. Mannheim käme vor München, Karlsruhe vor Kassel etc.\nSearch.03: Dominanz (1P) Was bedeutet \"Eine Heuristik $h_1(n)$ dominiert eine Heuristik $h_2(n)$\"?\nWie wirkt sich die Nutzung einer dominierenden Heuristik $h_1(n)$ in A* aus (im Vergleich zur Nutzung einer Heuristik $h_2$, die von $h_1$ dominiert wird)?\nGeben Sie selbstgewählte Beispiele an.\nThema: Begriff der dominierenden Heuristik (Selbststudium)\nSearch.04: Beweis der Optimalität von A* (1P) Beweisen Sie, dass A* in der Tree-Search-Variante bei Nutzung einer zulässigen Heuristik optimal ist.\nThema: Bedeutung einer zulässigen Heuristik (Selbststudium)\nIm Python-Code tauchen immer wieder \"TODO\"-Marker auf - bitte mit Vorsicht genießen! ↩︎",
    "description": "Search.01: Problemformalisierung, Zustandsraum (3P) Drei Elben und drei Orks befinden sich an einem Ufer eines Flusses und wollen diesen überqueren. Es steht dazu ein Pferd zur Verfügung, welches maximal zwei Wesen tragen kann. Das Pferd kann den Fluss nicht allein überqueren.\nGesucht ist eine Möglichkeit, alle Elben und Orks über den Fluss zu bringen. Dabei darf zu keiner Zeit an keinem Ufer die Anzahl der sich dort befindlichen Orks größer sein als die der dort wartenden Elben, da es sonst zu Konflikten zwischen beiden Gruppen kommt.",
    "tags": [],
    "title": "Übungsblatt: Problemlösen, Suche",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-search.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Man kann Spiele auch als Suchproblem betrachten und als Ziel die Suche nach dem optimalen Zug definieren.\nEinführung Optimale Spiele Minimax Heuristiken Alpha-Beta-Pruning",
    "description": "Man kann Spiele auch als Suchproblem betrachten und als Ziel die Suche nach dem optimalen Zug definieren.\nEinführung Optimale Spiele Minimax Heuristiken Alpha-Beta-Pruning",
    "tags": [],
    "title": "Spiele",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Spiele",
    "content": "Backgammon: Zwei Spieler, was ist der beste Zug? Quelle: \"position-backgammon-decembre\" by serialgamer_fr on Flickr.com (CC BY 2.0)\nZwei Spieler, ein Spielstand und ein Würfelergebnis: Was ist jetzt der beste Zug?!\nMotivation: Unterschied zu Suche?! =\u003e Mehrere konkurrierende Agenten an Suche beteiligt!\n=\u003e (Re-) Aktion des Gegners unbekannt/nicht vorhersehbar.\nSpiele und Umgebungen Deterministisch Zufallskomponente Voll beobachtbar Schach, Go, ... Backgammon, Monopoly Partiell beobachtbar Schiffe-versenken Bridge, Poker, Skat, ... =\u003e Bis auf Roboterfußball in KI traditionell keine physischen Spiele!\nBrettspiele sind interessant für KI Brettspiele gut abstrakt darstellbar:\nZustände einfach repräsentierbar Aktionen wohldefiniert (und i.d.R. sehr einfach) Realisierung als Suchproblem möglich Problem: Suchbäume werden in Praxis riesig\nBeispiel Schach:\nIm Mittel 35 Aktionen (branching factor) von jeder Position Oft mehr als 40 Züge pro Spieler =\u003e Suchbäume mit mehr als 80 Ebenen $35^{80} \\approx 10^{123}$ mögliche Knoten! (Aber \"nur\" rund $10^{40}$ verschiedene Zustände) Quelle: [Russell2020, pp. 193-196]\nEigenschaften guter Spielalgorithmen Zeit begrenzt\nIrgendeine gute Entscheidung treffen! =\u003e Bewertungsfunktion (auch für Zwischenzustände) Speicher begrenzt\nEvaluierungsfunktion für Zwischenzustände Löschen von irrelevanten Zweigen Strategien nötig\nVorausschauend spielen (Züge \"vorhersehen\") Wrap-Up Spiele kann man als Suchproblem betrachten Merkmale: Mehrere Agenten beteiligt Beobachtbarkeit der Umgebung Zufallskomponente Spielstrategie Problem: Riesige Spielbäume Umgang mit begrenzten Ressourcen (Zeit, Speicher)",
    "description": "Backgammon: Zwei Spieler, was ist der beste Zug? Quelle: \"position-backgammon-decembre\" by serialgamer_fr on Flickr.com (CC BY 2.0)\nZwei Spieler, ein Spielstand und ein Würfelergebnis: Was ist jetzt der beste Zug?!\nMotivation: Unterschied zu Suche?! =\u003e Mehrere konkurrierende Agenten an Suche beteiligt!\n=\u003e (Re-) Aktion des Gegners unbekannt/nicht vorhersehbar.",
    "tags": [],
    "title": "Einführung Optimale Spiele",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games1-intro.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Spiele",
    "content": "Spiele als Suchproblem: Minimax Terminologie Zwei abwechselnd spielende Spieler: MAX und MIN, wobei MAX beginnt\nBeide Spieler spielen in jedem Zug optimal Spielergebnis wird aus Sicht von MAX bewertet: $+1$, wenn Spieler MAX gewinnt $-1$, wenn Spieler MIN gewinnt $0$, wenn unentschieden Spieler MAX versucht, das Spielergebnis zu maximieren Spieler MIN versucht, das Spielergebnis zu minimieren Startzustand: Initialer Zustand des Spielbrettes\nAktionen: Legale Züge, abhängig vom Spielzustand\nZieltest: Ist das Spiel vorbei?\n=\u003e Startzustand und anwendbare Aktionen definieren den Zustandsraum.\nNutzenfunktion: $\\operatorname{UTILITY}(s,p)$: Wert des Spiels für Spieler $p$ im Spielzustand $s$\nStrategie: Spieler benötigen Strategie, um zu gewünschtem Endzustand zu kommen (unabhängig von den Entscheidungen des Gegenspielers) =\u003e einfacher Pfad von Start zu Ziel reicht nicht\nHinweis: Nullsummenspiel! (Der Gewinn des einen Spielers ist der Verlust des anderen Spielers.)\nEine mit dem Minimax-Algorithmus berechnete Strategie wird auch Minimax-Strategie genannt. Sie sichert dem betreffenden Spieler den höchstmöglichen Gewinn, der unabhängig von der Spielweise des Gegners erreichbar ist.\nBei Nicht-Nullsummenspielen, bei denen die Niederlage des Gegners nicht zwangsläufig mit dem eigenen Gewinn zusammenfällt (d.h. Gewinn des einen Spielers $\\ne$ Verlust des anderen Spielers), liefert der Minimax-Algorithmus nicht unbedingt eine optimale Strategie.\nSpielbaum TTT Minimax (Idee) Erzeuge kompletten Suchbaum mit Tiefensuche Wende Nutzenfunktion (Utility) auf jeden Endzustand an Ausgehend von Endzuständen =\u003e Bewerte Vorgängerknoten: Knoten ist Min-Knoten: Nutzen ist das Minimum der Kindknoten Knoten ist Max-Knoten: Nutzen ist das Maximum der Kindknoten Startknoten: Max wählt den Zug, der zum Nachfolger mit der höchsten Bewertung führt Annahme: Beide spielen perfekt. Fehler verbessern das Resultat für den Gegner.\nMinimax-Algorithmus: Funktionen für MAX- und MIN-Knoten def Max-Value(state): if Terminal-Test(state): return Utility(state) v = -INF for (a, s) in Successors(state): v = MAX(v, Min-Value(s)) return v def Min-Value(state): if Terminal-Test(state): return Utility(state) v = +INF for (a, s) in Successors(state): v = MIN(v, Max-Value(s)) return v Hinweis I: Auf wikipedia.org/wiki/Minimax finden Sie eine Variante mit einem zusätzlichen Tiefenparameter, um bei einer bestimmten Suchtiefe abbrechen zu können. Dies ist bereits eine erweiterte Version, wo man beim Abbruch durch das Erreichen der Suchtiefe statt Utility() eine Eval()-Funktion braucht (vgl. Minimax: Heuristiken).\nWenn man ohne Suchtiefenbeschränkung arbeiten will, braucht man diesen Parameter nicht! Der Algorithmus terminiert auch ohne Suchtiefenbeschränkung!\nHinweis II: Im [Russell2020, S. 196, Abb. 6.3] findet sich eine Variante, die die auf der nächsten Folien gezeigte Startfunktion mit den hier gezeigten Min-Value()- und Max-Value()-Funktionen verschmilzt. Dabei wird in den beiden Hilfsfunktionen nicht nur der min- bzw. max-Wert optimiert, sondern auch der jeweils beste Zug gespeichert und als Rückgabe zurückgeliefert.\nMinimax-Algorithmus: Sonderbehandlung Startknoten def Minimax(state): (val, action) = (-INF, null) for (a, s) in Successors(state): v = Min-Value(s) if (val \u003c= v): (val, action) = (v, a) return action Startknoten ist ein MAX-Knoten Nicht nur Kosten, sondern auch die zugehörige Aktion speichern Minimax Beispiel Aufwand Minimax maximale Tiefe des Spielbaums: $m$ in jedem Zustand $b$ gültige Züge =\u003e Zeitkomplexität $O(b^m)$ Gedankenexperiment:\nerster Zug: $b$ Möglichkeiten, zweiter Zug: jeweils wieder $b$ Möglichkeiten $\\rightarrow$ $b \\star b = b^2$, dritter Zug: jeweils wieder $b$ Möglichkeiten $\\rightarrow$ $b \\star (b \\star b) = b^3$, ..., $m$. Zug: jeweils wieder $b$ Möglichkeiten $\\rightarrow$ $b^m$ Wrap-Up Minimax: Entwickelt Spielbaum, bewertet Zustände entsprechend Max und Min Gewinn von Max: +1, Gewinn von Min: -1 Max wählt das Maximum der möglichen Züge von Min Min wählt das Minimum der möglichen Züge von Max Spielbaum wird bis zu den Blättern entwickelt, Bewertung mit Utility",
    "description": "Spiele als Suchproblem: Minimax Terminologie Zwei abwechselnd spielende Spieler: MAX und MIN, wobei MAX beginnt\nBeide Spieler spielen in jedem Zug optimal Spielergebnis wird aus Sicht von MAX bewertet: $+1$, wenn Spieler MAX gewinnt $-1$, wenn Spieler MIN gewinnt $0$, wenn unentschieden Spieler MAX versucht, das Spielergebnis zu maximieren Spieler MIN versucht, das Spielergebnis zu minimieren Startzustand: Initialer Zustand des Spielbrettes\nAktionen: Legale Züge, abhängig vom Spielzustand\nZieltest: Ist das Spiel vorbei?",
    "tags": [],
    "title": "Minimax",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games2-minimax.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Spiele",
    "content": "Wenn die Zeit nicht reicht: Suchtiefe begrenzen Einführung neuer Funktionen:\nCutoff-Test statt Terminal-Test\nBeispielsweise bei erreichter Tiefe oder Zeitüberschreitung\nEval statt Utility\nBewertung der erreichten Position (statt nur Bewertung des Endzustandes)\nBedingungen an Eval:\nEndknoten in selber Reihenfolge wie bei Utility Schnell zu berechnen (!) Beispiel Schach Mögliche Evaluierungskriterien:\nMaterialwert: Bauer 1, Läufer/Springer 3, Turm 5, Dame 9 Stellungsbewertung: Sicherheit des Königs, Stellung der Bauern Daumenregeln: 3 Punkte Vorteil =\u003e sicherer Sieg Nutzung gewichteter Features $f_i$: $\\operatorname{Eval}(s) = w_1f_1(s) + w_2f_2(s) + \\ldots$\nBeispiel: $w_1 = 9$ und $f_1(s)$ = (# weiße Königinnen) - (# schwarze Königinnen) Alternativ: Speicherung von Positionen plus Bewertung in Datenbanken =\u003e Lookup mit $\\operatorname{Eval}(s)$ (statt Berechnung zur Laufzeit)\nMinimax mit mehreren Spielern Hier maximiert jeder Spieler sein eigenes Ergebnis. Im Grunde müsste diese Variante dann besser \"Maximax\" heissen ...\nWenn es an einer Stelle im Suchbaum mehrere gleich gute (beste) Züge geben sollte, kann der Spieler Allianzen bilden: Er könnte dann einen Zug auswählen, der für einen der Mitspieler günstiger ist.\nZufallsspiele Quelle: \"position-backgammon-decembre\" by serialgamer_fr on Flickr.com (CC BY 2.0)\nBackgammon: Was ist in dieser Situation der optimale Zug?\nMinimax mit Zufallsspielen: ZUFALLS-Knoten Zusätzlich zu den MIN- und MAX-Knoten führt man noch Zufalls-Knoten ein, um das Würfelergebnis repräsentieren zu können. Je möglichem Würfelergebnis $i$ gibt es einen Ausgang, an dem die Wahrscheinlichkeit $P(i)$ dieses Ausgangs annotiert wird.\n=\u003e Für Zufallsknoten erwarteten Minimax-Wert (Expectimax) nutzen\nMinimax mit Zufall: Expectimax Expectimax-Wert für Zufallsknoten $C$:\n$$ \\operatorname{Expectimax}(C) = \\sum_i P(i) \\operatorname{Expectimax}(s_i) $$ $i$ mögliches Würfelergebnis $P(i)$ Wahrscheinlichkeit für Würfelergebnis $s_i$ Nachfolgezustand von $C$ gegeben Würfelergebnis $i$ Für die normalen Min- und Max-Knoten liefert Expectimax() die üblichen Aufrufe von Min-Value() bwz. Max-Value().\nAuf wikipedia.org/wiki/Expectiminimax finden Sie eine Variante mit einem zusätzlichen Tiefenparameter, um bei einer bestimmten Suchtiefe abbrechen zu können. Dies ist bereits eine erweiterte Version, wo man beim Abbruch durch das Erreichen der Suchtiefe statt Utility() eine Eval()-Funktion braucht. Zusätzlich kombiniert der dort gezeigte Algorithmus die Funktionen Expectimax(), Min-Value() und Max-Value() in eine einzige Funktion.\nEine ähnliche geschlossene Darstellung finden Sie im [Russell2020, S. 212].\nHinweis: Üblicherweise sind die Nachfolger der Zufallsknoten gleich wahrscheinlich. Dann kann man einfach mit dem Mittelwert der Bewertung der Nachfolger arbeiten.\nWrap-Up Minimax: Kriterien zur Begrenzung der Suchtiefe, Bewertung Eval statt Utility Erweiterung auf $\u003e2$ Spieler Erweiterung auf Spiele mit Zufall: Expectimax",
    "description": "Wenn die Zeit nicht reicht: Suchtiefe begrenzen Einführung neuer Funktionen:\nCutoff-Test statt Terminal-Test\nBeispielsweise bei erreichter Tiefe oder Zeitüberschreitung\nEval statt Utility\nBewertung der erreichten Position (statt nur Bewertung des Endzustandes)\nBedingungen an Eval:\nEndknoten in selber Reihenfolge wie bei Utility Schnell zu berechnen (!) Beispiel Schach Mögliche Evaluierungskriterien:\nMaterialwert: Bauer 1, Läufer/Springer 3, Turm 5, Dame 9 Stellungsbewertung: Sicherheit des Königs, Stellung der Bauern Daumenregeln: 3 Punkte Vorteil =\u003e sicherer Sieg Nutzung gewichteter Features $f_i$: $\\operatorname{Eval}(s) = w_1f_1(s) + w_2f_2(s) + \\ldots$",
    "tags": [],
    "title": "Heuristiken",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games3-heuristics.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Spiele",
    "content": "Verbesserung Minimax-Algorithmus =\u003e Minimax-Baum: Verbesserungen möglich?\nAlpha-beta-Pruning Minimax-Algorithmus mit zusätzlichen Informationen:\n$\\alpha$: bisher bester Wert für MAX (höchster Wert) $\\beta$: bisher bester Wert für MIN (kleinster Wert) =\u003e Beobachtungen:\n$\\alpha$ für MAX-Knoten wird nie kleiner $\\beta$ für MIN-Knoten wird nie größer Pruning-Regeln Schneide (unter) MIN-Knoten ab, deren $\\beta$ $\\le$ dem $\\alpha$ des MAX-Vorgängers ist.\nSchneide (unter) MAX-Knoten ab, deren $\\alpha$ $\\ge$ dem $\\beta$ des MIN-Vorgängers ist.\nAbbruch, wenn kein Platz mehr zwischen Alpha und Beta\nAlpha-beta-Pruning -- Der Algorithmus (Skizze) def Max-Value(state, alpha, beta): if Terminal-Test(state): return Utility(state) v = -INF for (a, s) in Successors(state): v = MAX(v, Min-Value(s, alpha, beta)) if (v \u003e= beta): return v alpha = MAX(alpha, v) return v def Min-Value(state, alpha, beta): if Terminal-Test(state): return Utility(state) v = +INF for (a, s) in Successors(state): v = MIN(v, Max-Value(s, alpha, beta)) if (v \u003c= alpha): return v beta = MIN(beta, v) return v Initialer Aufruf von Max-Value() mit $\\alpha = -\\infty$ und $\\beta = +\\infty$\nAchtung: Es kursieren Varianten von diesem Algorithmus, bei denen auf die Hilfsvariable v verzichtet wird und stattdessen alpha bzw. beta direkt modifiziert werden und als Rückgabewert dienen. Das kann zu anderen oder falschen Ergebnissen führen! Sie können das in der Aufgabe auf Blatt 03 gut beobachten.\nAlpha-beta-Pruning -- Eigenschaften Pruning beeinflusst nicht das Endergebnis!\nSortierung der Nachfolger spielt große Rolle\nPerfekte Sortierung: $O(b^{d/2})$ =\u003e Verdopplung der Suchtiefe möglich\nFür Schach immer noch zu aufwändig ...\nVerbesserungen für Alpha-beta-Pruning \"Killer-Move\": Maximale Effizienz nur wenn optimaler Zug immer zuerst untersucht =\u003e Zu untersuchende Züge sortieren/priorisieren, zb. Schach: a) Figuren schlagen b) Drohen c) Vorwärts ziehen d) Rückwärts ziehen\nVerändern der Suchtiefe nach Spielsituation\nBewertungsfunktion Eval:\nDatenbanken mit Spielsituationen und Expertenbewertung: Eröffnungsspiele (besonders viele Verzweigungen) Endspiele Lernen der optimalen Gewichte für Eval-Funktion Berücksichtigung von Symmetrien Beispiel DeepBlue (IBM, 1997) Alpha-beta-Pruning mit Tiefenbeschränkung: ca. 14 Halbzüge Dynamische Tiefenbeschränkung (stellungsabhängig, max. ca. 40 Züge) Heuristische Stellungsbewertung Eval: mehr als 8.000 Features ca. 4.000 Eröffnungsstellungen ca. 700.000 Spielsituationen (von Experten bewertet) Endspiel-Datenbank: alle Spiele mit 5 Steinen, viele mit 6 Steinen Quelle: [Russell2014, p. 185]\nBeispiel AlphaGo (Google, 2016) Beschränkung der Suchtiefe: Bewertung der Stellung durch \"Value Network\"\nBeschränkung der Verzweigungsbreite: Bestimmung von Zugkandidaten durch \"Policy Network\"\nTraining dieser \"Deep Neural Networks\":\nÜberwachtes Lernen: \"Analyse\" von Spiel-Datenbanken Reinforcement-Lernen: Self-Play, Bewertung am Ende Züge mit Monte-Carlo-Baumsuche ausgewählt Quelle: [Silver2016], siehe auch deepmind.com/research/alphago/\nWrap-Up Alpha-beta-Pruning:\nMitführen der bisher besten Werte für MAX und MIN: $\\alpha$ und $\\beta$ Abschneiden von Pfaden, die Verschlechterung bewirken würden Endergebnis bleibt erhalten Effizienzsteigerung abhängig von Sortierung der Nachfolger Viele Verbesserungen denkbar:\nZu untersuchende Züge \"richtig\" sortieren (Heuristik) Suchtiefe begrenzen und Bewertungsfunktion (statt Nutzenfunktion) Positionen mit Datenbank abgleichen",
    "description": "Verbesserung Minimax-Algorithmus =\u003e Minimax-Baum: Verbesserungen möglich?\nAlpha-beta-Pruning Minimax-Algorithmus mit zusätzlichen Informationen:\n$\\alpha$: bisher bester Wert für MAX (höchster Wert) $\\beta$: bisher bester Wert für MIN (kleinster Wert) =\u003e Beobachtungen:\n$\\alpha$ für MAX-Knoten wird nie kleiner $\\beta$ für MIN-Knoten wird nie größer Pruning-Regeln Schneide (unter) MIN-Knoten ab, deren $\\beta$ $\\le$ dem $\\alpha$ des MAX-Vorgängers ist.",
    "tags": [],
    "title": "Alpha-Beta-Pruning",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/games4-alphabeta.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "EA.01: Modellierung von GA (2P) Betrachten Sie das 8-Queens-Problem sowie das Landkarten-Färbeproblem (aus Vorlesung CSP: Intro). Starten Sie beim Färbeproblem mit fünf verschiedenen Farben, Ziel sollte eine konfliktfreie Einfärbung mit einer minimalen Anzahl an Farben sein.\nGeben Sie für beide Probleme je eine geeignete Kodierung der Individuen, passende Operatoren (Crossover, Mutation) und eine geeignete Fitnessfunktion an, damit die Probleme mit einem GA gelöst werden können. Begründen Sie Ihre Wahl!\nWas würden Sie noch benötigen, um die obigen Probleme jeweils mit Simulated Annealing lösen zu können?\nThema: Modellierung für GA und Gradientensuche\nEA.02: Implementierung (5P) Implementieren Sie den in der Vorlesung besprochenen GA und wenden Sie den Algorithmus nacheinander auf beide Probleme an. Sie können gern auch die Java-Klassen im Paket aima.core.search.local bzw. die Python-Klassen in search.py als Ausgangspunkt nutzen.1\nUntersuchen Sie systematisch unterschiedliche Varianten/Einstellungen der in der VL vorgestellten Operatoren. Führen Sie pro Einstellung jeweils mind. 100 Läufe durch und messen Sie die besprochenen Kennzahlen.\nErstellen Sie eine geeignete (systematische!) Auswertung Ihrer Experimente.\nEA.03: Anwendungen (3P) Analysieren Sie die Implementierung von Randal Olson \"Here's Waldo: Computing the optimal search strategy for finding Waldo\" (Direktlink). Schauen Sie sich nun den \"Evolution Simulator\" an. Wie ist dort die Modellierung erfolgt (Kodierung, Operatoren, Fitnessfunktion)? Wie werden EA/GA konkret im \"american fuzzy lop\" eingesetzt? Welche Fitnessfunktion wurden in den drei Beispielen jeweils genutzt, wie die Individuen und die Operatoren codiert?\nRecherchieren Sie, in welchen anderen Anwendungen Evolutionäre Algorithmen eingesetzt werden. Erklären Sie kurz, wie und wofür die EA/GA jeweils genutzt werden.\nThema: Analyse von GA-Implementierungen\nIm Python-Code tauchen immer wieder \"TODO\"-Marker auf - bitte mit Vorsicht genießen! ↩︎",
    "description": "EA.01: Modellierung von GA (2P) Betrachten Sie das 8-Queens-Problem sowie das Landkarten-Färbeproblem (aus Vorlesung CSP: Intro). Starten Sie beim Färbeproblem mit fünf verschiedenen Farben, Ziel sollte eine konfliktfreie Einfärbung mit einer minimalen Anzahl an Farben sein.\nGeben Sie für beide Probleme je eine geeignete Kodierung der Individuen, passende Operatoren (Crossover, Mutation) und eine geeignete Fitnessfunktion an, damit die Probleme mit einem GA gelöst werden können. Begründen Sie Ihre Wahl!\nWas würden Sie noch benötigen, um die obigen Probleme jeweils mit Simulated Annealing lösen zu können?",
    "tags": [],
    "title": "Übungsblatt: Lokale Suche, GA",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-ea.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Was haben Typ-Inferenz, Sudoku, das 8-Queens-Problem, das Einfärben von Landkarten und das Erstellen von Stundenplänen gemeinsam?\nEs handelt sich um eine bestimmte Art von Suchproblemen, wobei den Parametern (Variablen) Werte so zugewiesen werden müssen, dass Einschränkungen bzw. Relationen zwischen den Variablen gelten.\nEinführung Constraints Lösen von diskreten CSP Heuristiken Kantenkonsistenz und AC-3",
    "description": "Was haben Typ-Inferenz, Sudoku, das 8-Queens-Problem, das Einfärben von Landkarten und das Erstellen von Stundenplänen gemeinsam?\nEs handelt sich um eine bestimmte Art von Suchproblemen, wobei den Parametern (Variablen) Werte so zugewiesen werden müssen, dass Einschränkungen bzw. Relationen zwischen den Variablen gelten.\nEinführung Constraints Lösen von diskreten CSP Heuristiken Kantenkonsistenz und AC-3",
    "tags": [],
    "title": "Constraintsolving",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e CSP",
    "content": "Motivation: Einfärben von Landkarten Die Skizze soll eine Landkarte mit verschiedenen Ländern darstellen. Die Aufgabe lautet: Färbe jedes Land mit einer Farbe ein, um die Übersichtlichkeit zu erhöhen. Verwende dabei so wenig wie möglich unterschiedliche Farben. Aneinander grenzende Länder müssen unterschiedliche Farben bekommen (=\u003e Constraint).\nEinfärben von Landkarten: Formalisierung Variablen: A, B, C, D, E, F\nWerte: $\\lbrace red, green, blue \\rbrace$\nConstraints: Benachbarte Regionen müssen unterschiedliche Farben haben\nMögliche Lösung: Zuweisung an Variablen (\"Belegung\") $\\lbrace \\operatorname{A} = red, \\operatorname{B} = blue, \\operatorname{C} = green, \\operatorname{D} = red, \\operatorname{E} = blue, \\operatorname{F} = blue \\rbrace$\nDefinition: Constraint Satisfaction Problem (CSP) Ein CSP $\\langle V, D, C \\rangle$ besteht aus:\nMenge von Variablen $V = \\lbrace V_1, V_2, \\ldots, V_n \\rbrace$ Je $V_i$ nicht leere Domäne $D_i = \\lbrace d_{i,1}, d_{i,2}, \\ldots, d_{i,m_i} \\rbrace$ Menge von Constraints $C = \\lbrace C_1, C_2, \\ldots, C_p \\rbrace$ (Randbedingungen, Abhängigkeiten zwischen Variablen) Zuweisung/Belegung (Assignment) $\\alpha$:\nZuweisung von Werten an (einige/alle) Variablen: $\\alpha = \\lbrace X=a, Y=b, \\ldots \\rbrace$ (aus den jeweiligen Wertebereichen) Konsistente Belegung: Randbedingungen sind nicht verletzt Vollständige Belegung: Alle Variablen sind belegt Lösung eines CSP: Vollständige und konsistente Belegung\nConstraint-Graph Ein CSP kann man auch als Constraint-Graph darstellen. Die Variablen werden zu Knoten im Graph, die Constraints zu Kanten zwischen den Knoten. Dadurch kann man die aus dem Problemlösen bekannten Algorithmen anwenden ...\nConstraints -- Arität Die Arität betrifft hier die \"Stelligkeit\": Wie viele Variablen stehen in einem Constraint miteinander in Beziehung? (Also wie viele Parameter hat ein Constraint?)\nunär: betrifft einzelne Variablen Beispiel: $\\operatorname{A} \\neq red$\nbinär: betrifft Paare von Variablen Beispiel: $\\operatorname{A} \\neq \\operatorname{B}$\nhöhere Ordnung: betrifft 3 oder mehr Variablen\nPräferenzen: \"soft constraints\" Beispiel: \"rot ist besser als grün\"\nAbbildung über Gewichtung =\u003e Constraint-Optimierungsproblem (COP)\nConstraints -- Wertebereiche Endliche Domänen: $d$ Werte =\u003e $O(d^n)$ mögliche Zuweisungen (exponentiell in der Zahl der Variablen)\nUnendliche Domänen: reelle Zahlen, natürliche Zahlen =\u003e Keine Auflistung der erlaubten Wertekombinationen mehr möglich =\u003e Übergang zu Gleichungen/Ungleichungen: $job_1+5",
    "description": "Motivation: Einfärben von Landkarten Die Skizze soll eine Landkarte mit verschiedenen Ländern darstellen. Die Aufgabe lautet: Färbe jedes Land mit einer Farbe ein, um die Übersichtlichkeit zu erhöhen. Verwende dabei so wenig wie möglich unterschiedliche Farben. Aneinander grenzende Länder müssen unterschiedliche Farben bekommen (=\u003e Constraint).\nEinfärben von Landkarten: Formalisierung Variablen: A, B, C, D, E, F\nWerte: $\\lbrace red, green, blue \\rbrace$",
    "tags": [],
    "title": "Einführung Constraints",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp1-intro.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e CSP",
    "content": "Einfärben von Landkarten als CSP Endliche Domänen: Formulierung als Suchproblem def BT_Search(assignment, csp): if complete(assignment): return assignment var = VARIABLES(csp, assignment) for value in VALUES(csp, var): if consistent(value, var, assignment, csp): assignment += {var = value} if INFERENCE(csp, assignment, var) != failure: result = BT_Search(assignment, csp) if result != failure: return result assignment -= {var = value} return failure Quelle: Eigener Code basierend auf einer Idee nach [Russell2020, p. 176, fig. 5.5]\nHierbei handelt es sich um eine etwas angepasste Tiefensuche: Starte mit leerem Assignment und weise schrittweise Variablen passende Werte zu und mache notfalls Backtracking.\nBT-Suche für CSP am Beispiel Landkartenfärbeproblem Wrap-Up Lösung von CSP mit endlichen Domänen mit Hilfe der Backtracking-Suche",
    "description": "Einfärben von Landkarten als CSP Endliche Domänen: Formulierung als Suchproblem def BT_Search(assignment, csp): if complete(assignment): return assignment var = VARIABLES(csp, assignment) for value in VALUES(csp, var): if consistent(value, var, assignment, csp): assignment += {var = value} if INFERENCE(csp, assignment, var) != failure: result = BT_Search(assignment, csp) if result != failure: return result assignment -= {var = value} return failure Quelle: Eigener Code basierend auf einer Idee nach [Russell2020, p. 176, fig. 5.5]",
    "tags": [],
    "title": "Lösen von diskreten CSP",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp2-backtrackingsearch.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e CSP",
    "content": "VARIABLES: Variablen-Sortierung, Welche Variable soll betrachtet werden? VARIABLES: Welche Variable zuerst ausprobieren?\nMinimum Remaining Values (MRV): (vgl. [Russell2020, S. 177])\nWähle Variable mit wenigsten freien Werten (die am meisten eingeschränkte Variable)\n=\u003e reduziert den Verzweigungsgrad\nBeispiel:\nFreie Auswahl, alle haben gleich viele freie Werte (jeweils 3) =\u003e wähle A B und C haben nur noch zwei freie Werte =\u003e wähle B (oder C) C hat nur noch einen Wert, D noch zwei, der Rest drei =\u003e wähle C VARIABLES: Gleichstand bei MRV VARIABLES: Welche Variable zuerst ausprobieren?\nGradheuristik: Erweiterung von MRV bei Gleichstand (vgl. [Russell2020, S. 177])\nWähle Variable mit meisten Constraints auf offene (noch nicht zugewiesene) Variablen\n=\u003e reduziert den Verzweigungsgrad in späteren Schritten\nBeispiel:\nMRV: Alle haben gleich viele freie Werte (jeweils 3) =\u003e Gradheuristik: B, C und D haben die meisten Verbindungen (Constraints) auf offene Variablen =\u003e wähle B (oder C oder D) MRV: A, C und D haben nur noch zwei freie Werte =\u003e Gradheuristik: C und D haben je zwei Constraints auf noch offene Variablen =\u003e wähle C (oder D) MRV: A und D haben beide nur noch einen Wert =\u003e Gradheuristik: D hat die meisten Verbindungen (Constraints) auf offene Variablen =\u003e wähle D VALUES: Werte-Sortierung, Welchen Wert soll ich ausprobieren? VALUES: Welchen Wert zuerst ausprobieren?\nLeast Constraining Value (LCV): (vgl. [Russell2020, S. 177])\nWähle Wert, der für verbleibende Variablen die wenigsten Werte ungültig macht\n=\u003e verringert die Wahrscheinlichkeit für Backtracking\nBeispiel:\nSei A gewählt: Alle Werte machen in den anderen Variablen einen Wert ungültig =\u003e freie Wahl des Wertes =\u003e wähle beispielsweise rot Sei B gewählt: Alle Werte machen in den anderen Variablen einen Wert ungültig =\u003e freie Wahl des Wertes =\u003e wähle beispielsweise grün Sei D gewählt: Verbleibende Werte rot und blau Wahl von rot würde für C einen Wert übrig lassen (blau) Wahl von blau würde für C keinen Wert übrig lassen =\u003e LCV: Wahl von rot! Hinweis: Diese Heuristik ist in der Praxis sehr aufwändig zu berechnen! Man müsste für jeden Wert die noch offenen Constraints anschauen und berechnen, wie viele Werte damit jeweils ungültig gemacht werden. Die Idee ist aber dennoch interessant, und möglicherweise kann man sie für ein reales Problem so adaptieren, dass bei der Umsetzung nur wenig zusätzlicher Aufwand entsteht.\nWrap-Up Verbesserung der BT-Suche mit Heuristiken: MRV, Gradheuristik, LCV",
    "description": "VARIABLES: Variablen-Sortierung, Welche Variable soll betrachtet werden? VARIABLES: Welche Variable zuerst ausprobieren?\nMinimum Remaining Values (MRV): (vgl. [Russell2020, S. 177])\nWähle Variable mit wenigsten freien Werten (die am meisten eingeschränkte Variable)\n=\u003e reduziert den Verzweigungsgrad\nBeispiel:\nFreie Auswahl, alle haben gleich viele freie Werte (jeweils 3) =\u003e wähle A B und C haben nur noch zwei freie Werte =\u003e wähle B (oder C) C hat nur noch einen Wert, D noch zwei, der Rest drei =\u003e wähle C VARIABLES: Gleichstand bei MRV",
    "tags": [],
    "title": "Heuristiken",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp3-heuristics.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e CSP",
    "content": "Problem bei BT-Suche Zuweisung eines Wertes an Variable $X$:\nPasst zu aktueller Belegung Berücksichtigt aber nicht restliche Constraints =\u003e macht weitere Suche u.U. unmöglich/schwerer Lösung: Nach Zuweisung alle nicht zugewiesenen Nachbarvariablen prüfen\nINFERENCE: Vorab-Prüfung (Forward Checking) Inference: Frühzeitiges Erkennen von Fehlschlägen! (vgl. [Russell2020, S. 178])\nNach Zuweisung eines Wertes an Variable $X$:\nBetrachte alle nicht zugewiesenen Variablen $Y$: Falls Constraints zw. $X$ und $Y$, dann ... ... entferne alle inkonsistenten Werte aus dem Wertebereich von $Y$. Beispiel:\nSei A auf rot gesetzt =\u003e entferne rot in B und C Sei D auf grün gesetzt =\u003e entferne grün in B und C und E Problem: Für B und C bleibt nur noch blau; sind aber benachbart!\nForward Checking findet nicht alle Inkonsistenzen! Nach $\\lbrace A=red, D=green \\rbrace$ bleibt für B und C nur noch blue B und C sind aber benachbart Übergang von Forward Checking zu Kantenkonsistenz Forward Checking erzeugt Konsistenz für alle Constraints der gerade betrachteten (belegten) Variablen.\nIdee: Ausdehnen auf alle Kanten ... =\u003e Einschränken der Wertemengen\nDefinition Kantenkonsistenz (Arc Consistency) Eine Kante von $X$ nach $Y$ ist \"konsistent\", wenn für jeden Wert $x \\in D_X$ und für alle Constraints zwischen $X$ und $Y$ jeweils ein Wert $y \\in D_Y$ existiert, so dass der betrachtete Constraint durch $(x,y)$ erfüllt ist.\nEin CSP ist kanten-konsistent, wenn für alle Kanten des CSP Konsistenz herrscht.\nBeispiel Kantenkonsistenz $V = \\lbrace a,b,c,d,e \\rbrace$ $\\mathrm{C} = \\lbrace ((a,b), \\ne), ((b,c), \\ne), ((a,c), \\ne), ((c,d), =), ((b,e), \u003c) \\rbrace$ $D_a=D_b=D_c=\\lbrace 1,2,3 \\rbrace$, $D_d=\\lbrace 1,2 \\rbrace$, $D_e=\\lbrace 1,2,3 \\rbrace$\nEinschränkung der Ausgangswertemengen (kanten-konsistent)\n$D_a=\\lbrace 1,2,3 \\rbrace$, $D_b=\\lbrace 1,2 \\rbrace$, $D_c=\\lbrace 1,2 \\rbrace$, $D_d=\\lbrace 1,2 \\rbrace$, $D_e=\\lbrace 2,3 \\rbrace$\n=\u003e Kantenkonsistenz ist nur lokale Konsistenz!\nAnmerkung: $((a,b), \\ne)$ ist Kurzform für $\\left((a,b), \\lbrace (x,y) \\in D_a \\times D_b | x \\ne y \\rbrace\\right)$\nAC-3 Algorithmus: Herstellen von Kantenkonsistenz def AC3(csp): queue = Queue(csp.arcs) while not queue.isEmpty(): (x,y) = queue.dequeue() if ARC_Reduce(csp,x,y): if D_x.isEmpty(): return false for z in x.neighbors(): queue.enqueue(z,x) return true def ARC_Reduce(csp, x, y): change = false for v in D_x: if not (any w in D_y and csp.C_xy(v,w)): D_x.remove(v); change = true return change Quelle: Eigener Code basierend auf einer Idee nach [Russell2020, p. 171, fig. 5.3]\nAnmerkung: Die Queue in AC-3 ist wie eine (mathematische) Menge zu betrachten: Jedes Element kann nur genau einmal in einer Menge enthalten sein. D.h. wenn man bei queue.enqueue(z,x) die Rückkanten von den Nachbarn in die Queue aufnimmt, sorgt die Queue eigenständig dafür, dass es keine doppelten Vorkommen einer Kante in der Queue gibt. (Falls die verwendete Queue in einer Programmiersprache das nicht unterstützt, müsste man bei queue.enqueue(z,x) stets abfragen, ob die Kante (z,x) bereits in der Queue ist und diese dann nicht erneut hinzufügen.) AC-3 hat eine Laufzeit von $O(d^3n^2)$ ($n$ Knoten, maximal $d$ Elemente pro Domäne). Leider findet auch AC-3 nicht alle Inkonsistenzen ... (NP-hartes Problem).\nHinweis: In gewisser Weise kann man Forward Checking als ersten Schritt bei der Herstellung von Kantenkonsistenz interpretieren.\nEinsatz des AC-3 Algorithmus Vorverarbeitung: Reduktion der Wertemengen vor BT-Suche\nNach AC-3 evtl. bereits Lösung gefunden (oder ausgeschlossen) Propagation: Einbetten von AC-3 als Inferenzschritt in BT-Suche (MAC -- Maintaining Arc Consistency)\nNach jeder Zuweisung an $X_i$ Aufruf von AC-3-Variante: Initial nur Kanten von $X_i$ zu allen noch nicht zugewiesenen Nachbarvariablen Anschließend rekursiver Aufruf von BT-Suche Wrap-Up Anwendung von Forward Checking und ... ... die Erweiterung auf alle Kanten: AC-3, Kantenkonsistenz",
    "description": "Problem bei BT-Suche Zuweisung eines Wertes an Variable $X$:\nPasst zu aktueller Belegung Berücksichtigt aber nicht restliche Constraints =\u003e macht weitere Suche u.U. unmöglich/schwerer Lösung: Nach Zuweisung alle nicht zugewiesenen Nachbarvariablen prüfen\nINFERENCE: Vorab-Prüfung (Forward Checking) Inference: Frühzeitiges Erkennen von Fehlschlägen! (vgl. [Russell2020, S. 178])\nNach Zuweisung eines Wertes an Variable $X$:\nBetrachte alle nicht zugewiesenen Variablen $Y$: Falls Constraints zw. $X$ und $Y$, dann ... ... entferne alle inkonsistenten Werte aus dem Wertebereich von $Y$. Beispiel:",
    "tags": [],
    "title": "Kantenkonsistenz und AC-3",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/csp4-ac3.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "Games.01: Handsimulation: Minimax und alpha-beta-Pruning (3P) (1P) Geben Sie für den Spielbaum die Minimax-Bewertungen an.\n(1P) Markieren Sie die Kanten, die bei alpha-beta-Pruning nicht mehr untersucht werden würden, d.h. wo Pruning stattfinden würde. Geben Sie für jeden Knoten die (sich ändernden) $\\alpha$- und $\\beta$-Werte an.\n(1P) Können die Knoten derart geordnet werden, dass alpha-beta-Pruning eine größere Anzahl von Zweigen abschneidet? Wenn ja, geben Sie eine solche Ordnung an. Wenn nein, begründen Sie Ihre Antwort.\nHinweis: Reihenfolge der Abarbeitung der Kindknoten: Wie in der VL von links nach rechts.\nThema: Minimax und alpha-beta-Pruning\nGames.02: Optimale Spiele: Minimax und alpha-beta-Pruning (4P) (2P) Implementieren Sie den Minimax-Algorithmus (wie in der VL besprochen) am Beispiel Tic Tac Toe in einer Sprache Ihrer Wahl.\n(1P) Ergänzen Sie Ihre Implementierung um alpha-beta-Pruning.\n(1P) Vergleichen Sie die Anzahl der jeweils berechneten Knoten. Überlegen Sie sich dazu ein sinnvolles Szenario.\nThema: Anwendung Minimax und alpha-beta-Pruning\nGames.03: Minimax vereinfachen (1P) Vereinfachen Sie den Minimax-Algorithmus aus der Vorlesung, indem Sie die Eigenschaft Nullsummenspiel berücksichtigen und die Funktionen Min-Value und Max-Value in eine einzige Funktion ohne explizite Unterscheidung der Spieler zusammenfassen.\nÜberlegen Sie sich einen Beispielbaum und zeigen Sie anhand dessen die Bewertung durch den Minimax-Algorithmus und durch Ihren vereinfachten Algorithmus.\nThema: Nullsummenspiel, Minimax\nGames.04: Suchtiefe begrenzen (1P) Die Verwendung der Suchtiefenbeschränkung erfordert den Einsatz einer Evaluierungsfunktion.\nBetrachten Sie die auf https://github.com/aimacode/aima-exercises/blob/master/markdown/5-Adversarial-Search/exercises/ex_9/question.md gegebene Evaluierungsfunktion für Tic-Tac-Toe.\nGeben Sie die Werte der Evaluierungsfunktion für sechs verschiedene Spielzustände an (3 Endzustände, 3 Zwischenzustände). Begründen Sie, warum diese Evaluierungsfunktion im Zusammenhang mit Tic-Tac-Toe sinnvoll sein kann.\nThema: Suchtiefenbegrenzung und Evaluierungsfunktion\nGames.05: Minimax generalisiert (1P) Betrachten Sie nun das Problem, den Spielbaum eines Drei-Personen-Spiels zu evaluieren, das nicht notwendigerweise die Nullsummenbedingung erfüllt.\nDie Spieler heißen 1, 2 und 3. Im Gegensatz zu Zwei-Personen-Nullsummenspielen liefert die Bewertungsfunktion nun Tripel $(x_1, x_2, x_3)$ zurück, wobei $x_i$ der Wert für Spieler $i$ ist. Allianzen zwischen Spielern sind nicht erlaubt.\nVervollständigen Sie den Spielbaum, indem Sie alle inneren Knoten und den Wurzelknoten mit den entsprechenden Wert-Tripeln annotieren.\nThema: Minimax generalisiert für mehrere Spieler",
    "description": "Games.01: Handsimulation: Minimax und alpha-beta-Pruning (3P) (1P) Geben Sie für den Spielbaum die Minimax-Bewertungen an.\n(1P) Markieren Sie die Kanten, die bei alpha-beta-Pruning nicht mehr untersucht werden würden, d.h. wo Pruning stattfinden würde. Geben Sie für jeden Knoten die (sich ändernden) $\\alpha$- und $\\beta$-Werte an.\n(1P) Können die Knoten derart geordnet werden, dass alpha-beta-Pruning eine größere Anzahl von Zweigen abschneidet? Wenn ja, geben Sie eine solche Ordnung an. Wenn nein, begründen Sie Ihre Antwort.",
    "tags": [],
    "title": "Übungsblatt: Spiele",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-games.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "Ich habe Symptome beobachtet. Kann ich die Ursache (also die Krankheit) bestimmen, wenn ich etwas Hintergrundwissen habe:\nWie häufig treten verschieden Krankheiten auf Welche Krankheit zeigt welche Symptome (und wie oft treten die dann auf) Kann ich aus diesen Daten einen Klassifikator lernen?\nWiederholung Wahrscheinlichkeitstheorie Klassifikation mit Naive Bayes",
    "description": "Ich habe Symptome beobachtet. Kann ich die Ursache (also die Krankheit) bestimmen, wenn ich etwas Hintergrundwissen habe:\nWie häufig treten verschieden Krankheiten auf Welche Krankheit zeigt welche Symptome (und wie oft treten die dann auf) Kann ich aus diesen Daten einen Klassifikator lernen?\nWiederholung Wahrscheinlichkeitstheorie Klassifikation mit Naive Bayes",
    "tags": [],
    "title": "Naive Bayes",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Naive Bayes",
    "content": "Ereignisse und Wahrscheinlichkeit Hinweis: Die folgende Darstellung zur Einführung in die Wahrscheinlichkeitstheorie dient dem Verständnis des Naive Bayes Klassifikationsalgorithmus und ist teilweise eher oberflächlich gehalten. Sie kann und soll keine entsprechende mathematische Einführung ersetzen!\nEreignisse Ereignisse $\\Omega = \\lbrace \\omega_1, \\omega_2, \\ldots, \\omega_n \\rbrace$: endliche Menge der Ausgänge eines Zufallsexperiments\nElementarereignis: Die $\\omega_i \\in \\Omega$\ndecken alle möglichen Versuchsergebnisse ab, und schließen sich gegenseitig aus Regeln Wenn $A$ und $B$ Ereignisse sind, dann auch $A \\cup B$ $\\Omega$ wird als sicheres Ereignis bezeichnet: Enthält definitionsgemäß alle Versuchsausgänge, d.h. ein in der Menge enthaltenes Ereignis muss auftreten Die leere Menge $\\emptyset$ wird als unmögliches Ereignis bezeichnet Die Variablen $A$ und $B$ heißen auch Zufallsvariablen Im Rahmen dieser Veranstaltung betrachten wir nur diskrete Zufallsvariablen mit endlichem Wertebereich!\nWahrscheinlichkeit Wahrscheinlichkeit:\nSei $\\Omega = \\lbrace \\omega_1, \\omega_2, \\ldots, \\omega_n \\rbrace$ endlich. Die Wahrscheinlichkeit $P(A)$ für ein Ereignis $A$ ist dann definiert als\n$$ P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\text{Anzahl der für A günstigen Fälle}}{\\text{Anzahl der möglichen Fälle}} $$ Man könnte auch schreiben: $P(A) = \\sum_{\\omega \\in A} P(\\omega)$\nHinweis: Diese Definition von Wahrscheinlichkeit geht von gleich wahrscheinlichen Elementarereignissen aus! Die allgemeine Definition geht über einen entsprechenden Grenzwert.\nVerteilung Den Vektor mit den Wahrscheinlichkeiten aller Elementarereignisse nennt man auch Verteilung.\nBeispiel: $\\mathbf{P}(A) = (P(A=1), P(A=2), \\ldots, P(A=6)) = (1/6, 1/6, \\ldots, 1/6)$\nHinweis: Wir betrachten hier nur diskrete Zufallsvariablen. Für kontinuierliche Variablen wird die Verteilung mit Hilfe einer Dichtefunktion dargestellt, beispielsweise der Gauss'schen Funktion.\nBeispiel Einmaliges Würfeln mit einem Spielwürfel: $\\Omega = \\lbrace 1,2,3,4,5,6 \\rbrace$ Elementarereignisse: $\\lbrace 1,2,3,4,5,6 \\rbrace$ Das Würfeln einer geraden Zahl ($A = \\lbrace 2,4,6 \\rbrace$) ist kein Elementarereignis, ebenso wie das Würfeln einer Zahl kleiner 5 ($B = \\lbrace 1,2,3,4 \\rbrace$), da $A \\cap B = \\lbrace 2,4 \\rbrace \\ne \\emptyset$ Wahrscheinlichkeit, eine 1 zu würfeln: $P(A \\in \\lbrace 1 \\rbrace) = P(A=1) = \\frac{1}{6}$. Anmerkung: Man schreibt statt $P(A \\in \\lbrace 1 \\rbrace)$ oft einfach $P(1)$. Wahrscheinlichkeit, eine gerade Zahl zu würfeln: $P(A \\in \\lbrace 2,4,6 \\rbrace) = P(A=2 \\vee A=4 \\vee A=6) = \\frac{|\\lbrace 2,4,6 \\rbrace|}{|\\lbrace 1,2,3,4,5,6 \\rbrace|} = \\frac{3}{6} = 0.5$ Rechenregeln: Kolmogorov Axiome Sei $A$ ein Ereignis, also $A \\subseteq \\Omega$:\n$0 \\le P(A) \\le 1$ $\\Omega = \\lbrace \\omega_1, \\omega_2, \\ldots, \\omega_n \\rbrace$: $\\sum_{i} P(\\omega_i) = 1$ (Normierungsbedingung: Summe über die Wahrscheinlichkeiten aller Elementarereignisse ist immer 1)\n$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ Daraus folgt (u.a.):\n$P(\\Omega) = 1$ $P(\\emptyset) = 0$ $P(A) = 1- P(\\neg A)$ $A$ und $B$ unabhängig: $P(A \\cup B) = P(A) + P(B)$\n$P(A \\cap B)$ ist leer, wenn $A$ und $B$ sich nicht überlappen\n$A \\subseteq B$: $P(A) \\le P(B)$\nVerbundwahrscheinlichkeiten $$P(A,B) = P(B,A) = \\text{ Wahrscheinlichkeit, dass A und B gleichzeitig auftreten }$$ Halsschmerzen $\\neg$ Halsschmerzen Schnupfen 0.04 0.06 $\\neg$ Schnupfen 0.01 0.89 $P(S,H) = 0.04$ Die Tabelle kann man so lesen: In 4 von 100 Fällen tritt das Ereignis \"Schnupfen\" gleichzeitig mit dem Ereignis \"Halsschmerzen\" auf, in 6 von 100 Fällen tritt \"Schupfen\" ohne Halsschmerzen auf. ... In Summe kommt man wieder auf 100 Fälle (100 Prozent).\nNach diesen Zahlen liegt also die Verbundwahrscheinlichkeit für die Ereignisse \"Schnupfen\" und \"Husten\", d.h. $P(S,H)$, bei 4 Prozent.\nHinweis: Die gezeigten Zahlen und Zusammenhänge sind fiktiv und dienen lediglich zur Verdeutlichung der Wahrscheinlichkeitsbegriffe!\nBedingte Wahrscheinlichkeit Definition: Bedingte Wahrscheinlichkeit für $A$ gegeben $B$:\n$$P(A|B) = \\frac{P(A,B)}{P(B)}$$ Halsschmerzen $\\neg$ Halsschmerzen Schnupfen 0.04 0.06 $\\neg$ Schnupfen 0.01 0.89 $P(\\text{Schnupfen } | \\text{ Halsschmerzen}) = \\frac{P(S,H)}{P(H)} = \\frac{0.04}{0.04+0.01} = 0.8$ $P(\\text{Halsschmerzen } | \\text{ Schnupfen}) = \\frac{P(H,S)}{P(S)} = \\frac{0.04}{0.04+0.06} = 0.4$ Wegen $P(A|B) = \\dfrac{P(A,B)}{P(B)}$ ist $P(A,B) = P(A|B)P(B) = P(B|A)P(A)$ (Produkt-Regel)!\nMarginalisierung Halsschmerzen $\\neg$ Halsschmerzen $\\sum$ Schnupfen 0.04 0.06 0.1 $\\neg$ Schnupfen 0.01 0.89 0.9 $\\sum$ 0.05 0.95 1 $P(S) = P(S,H) + P(S, \\neg H)$ Allgemein: Seien $B_1, \\ldots, B_n$ Elementarereignisse mit $\\bigcup_i B_i = \\Omega$. Dann ist $$P(A) = \\sum_i P(A,B_i) = \\sum_i P(A|B_i)P(B_i)$$\nDiesen Vorgang nennt man Marginalisierung. Die resultierende Verteilung $P(A)$ nennt man auch \"Randverteilung\", da sie mit einer Projektion eines Quaders auf eine Seitenfläche vergleichbar ist.\nKettenregel Produktregel: Wegen $P(A|B) = \\dfrac{P(A,B)}{P(B)}$ gilt $P(A,B) = P(A|B)P(B)$\nVerallgemeinerung (Kettenregel): $$ \\begin{array}{rcl} P(A_1,A_2,\\ldots,A_n) \u0026=\u0026 P(A_n,\\ldots,A_2,A_1)\\\\ \u0026 = \u0026 P(A_n|A_{n-1},\\ldots,A_1)P(A_{n-1},\\ldots,A_1)\\\\ \u0026 = \u0026 P(A_n|A_{n-1},\\ldots,A_1)P(A_{n-1}|A_{n-2},\\ldots,A_1)P(A_{n-2},\\ldots,A_1)\\\\ \u0026 = \u0026 \\ldots\\\\ \u0026 = \u0026 P(A_n|A_{n-1},\\ldots,A_1) \\ldots P(A_2|A_1)P(A_1)\\\\ \u0026 = \u0026 \\prod_i P(A_i|A_1,\\ldots,A_{i-1}) \\end{array} $$\nBayes-Regel Bedingte Wahrscheinlichkeit: $P(A,B) = P(A|B)P(B) = P(B|A)P(A)$\n$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$ $P(A)$ nennt man \"Prior\" oder \"A-priori-Wahrscheinlichkeit\" (Das ist die Wahrscheinlichkeit für $A$ ohne weiteres Wissen) $P(B|A)$ nennt man \"Likelihood\" (Wie wahrscheinlich ist das Auftreten von $B$, gegeben $A$?) $P(A|B)$ nennt man \"Posterior\" oder \"A-posteriori-Wahrscheinlichkeit\" (Wie wahrscheinlich ist $A$, wenn $B$ eingetreten ist?) $P(B)$ ist ein Normierungsfaktor Wenn man (siehe später: Naive Bayes Klassifikator) $A$ als Klasse und $B$ als Daten betrachtet:\n$P(A)$: Wie wahrscheinlich ist eine bestimmte Klasse an sich (A-priori-Wahrscheinlichkeit der Klassen)? $P(B|A)$: Wie wahrscheinlich sind bestimmte Daten, gegeben die Klasse $A$? (Likelihood der Daten) $P(A|B)$: Gegeben die Daten $B$, wie wahrscheinlich ist die Klasse $A$? (Posterior) In der Medizin hat sucht man i.d.R. die Ursache für beobachtete Symptome: $$ P(\\text{Ursache}|\\text{Symptome}) = \\frac{P(\\text{Symptome}|\\text{Ursache})P(\\text{Ursache})}{P(\\text{Symptome})} $$\nAus der A-priori-Wahrscheinlichkeit für bestimmte Krankheiten und der Likelihood der Symptome (wie wahrscheinlich sind Symptome, gegeben eine Krankheit) kann man die Wahrscheinlichkeit für das Vorliegen einer Erkrankung gegeben bestimmte Symptome berechnen.\nBeispiel Bayes Bei Arthrose wird in 80 Prozent der Fälle ein steifes Gelenk beobachtet Eine von 10.000 Personen hat Arthrose Eine von 10 Personen hat ein steifes Gelenk =\u003e Ich habe ein steifes Gelenk. Habe ich Arthrose?\nGegeben: $P(A) = 0.0001, P(S) = 0.1, P(S|A) = 0.8$ Gesucht: $P(A|S)$ $$ P(A|S) = \\frac{P(S|A)P(A)}{P(S)} = \\frac{0.8 \\times 0.0001}{0.1} = 0.0008 = 0.08\\% $$ Wenn ein steifes Gelenk vorliegt, ist die Wahrscheinlichkeit, dann an Arthrose erkrankt zu sein, bei nur 0.08%. Kein Grund zur Sorge in diesem Fall :-)\n=\u003e Wie wahrscheinlich ist ein steifes Gelenk ohne Arthrose, also $P(S|\\neg A$)?\nMit Marginalisierung: $P(S) = P(S|A)P(A) + P(S|\\neg A)P(\\neg A)$, d.h. $0.1 = 0.8 \\times 0.0001 + P(S|\\neg A) \\times (1-0.0001)$, d.h. $P(S|\\neg A) = 0.0999$\nIn knapp 10 Prozent der Fälle würde man im obigen Beispiel bei der Diagnose \"keine Arthrose\" ein steifes Gelenk beobachten.\nHinweis: Die genannten Zahlen und Zusammenhänge sind rein fiktional und sollen lediglich zur Veranschaulichung der Bayes-Regel dienen!\nSchauen Sie sich auch das Beispiel 7.9 in [Ertel2017, Ex. 7.9, S. 135] an!\nUnabhängige Ereignisse $P(\\text{Halsschmerzen},\\text{ Regen}) = P(\\text{Regen }|\\text{ Halsschmerzen})P(\\text{Halsschmerzen})$ $P(\\text{Regen }|\\text{ Halsschmerzen}) = \\text{ ?? }$ $= P(\\text{Regen})$\nZwei Ereignisse $A$ und $B$ sind unabhängig, wenn $$ P(A|B) = P(A) $$\n=\u003e $P(A,B) = P(A|B)P(B) = P(A)P(B)$\nDies kann man verallgemeinern (bedingte Unabhängigkeit):\n$X$ und $Y$ sind bedingt unabhängig (gegeben $Z$), wenn $P(X|Y,Z) = P(X|Z)$ bzw. $P(Y|X,Z) = P(Y|Z)$\nDaraus folgt:\n$$ P(X,Y|Z) = P(X|Y,Z)P(Y|Z) = P(X|Z)P(Y|Z) $$ Wrap-Up Grundlagen der Wahrscheinlichkeitstheorie Elementarereignisse und Wahrscheinlichkeit Rechenregeln Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit Marginalisierung (Bedingte) Unabhängigkeit Bayes'sche Regel",
    "description": "Ereignisse und Wahrscheinlichkeit Hinweis: Die folgende Darstellung zur Einführung in die Wahrscheinlichkeitstheorie dient dem Verständnis des Naive Bayes Klassifikationsalgorithmus und ist teilweise eher oberflächlich gehalten. Sie kann und soll keine entsprechende mathematische Einführung ersetzen!\nEreignisse Ereignisse $\\Omega = \\lbrace \\omega_1, \\omega_2, \\ldots, \\omega_n \\rbrace$: endliche Menge der Ausgänge eines Zufallsexperiments\nElementarereignis: Die $\\omega_i \\in \\Omega$\ndecken alle möglichen Versuchsergebnisse ab, und schließen sich gegenseitig aus Regeln Wenn $A$ und $B$ Ereignisse sind, dann auch $A \\cup B$ $\\Omega$ wird als sicheres Ereignis bezeichnet: Enthält definitionsgemäß alle Versuchsausgänge, d.h. ein in der Menge enthaltenes Ereignis muss auftreten Die leere Menge $\\emptyset$ wird als unmögliches Ereignis bezeichnet Die Variablen $A$ und $B$ heißen auch Zufallsvariablen Im Rahmen dieser Veranstaltung betrachten wir nur diskrete Zufallsvariablen mit endlichem Wertebereich!",
    "tags": [],
    "title": "Wiederholung Wahrscheinlichkeitstheorie",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb1-probability.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Naive Bayes",
    "content": "Medizinische Diagnostik mit NB Bei Arthrose wird in 80 Prozent der Fälle ein steifes Gelenk beobachtet: $P(S|A) = 0.8$ Eine von 10.000 Personen hat Arthrose: $P(A) = 0.0001$ Eine von 10 Personen hat ein steifes Gelenk: $P(S) = 0.1$ =\u003e Ich habe ein steifes Gelenk. Habe ich Arthrose?\nTextklassifikation mit NB Mails, manuell markiert:\nD1: (\"Sieben Zwerge fraßen sieben Ziegen\", OK) D2: (\"Sieben Ziegen traten sieben Wölfe\", SPAM) D3: (\"Sieben Wölfe fraßen sieben Böcke\", OK) D4: (\"Sieben Böcke traten sieben Zwerge\", SPAM) Neue Mails:\nT1: (\"Sieben Zwerge fraßen sieben Wölfe\") T2: (\"Sieben Zwerge traten sieben Ziegen\") Lernen Sie mit Hilfe der Trainingsmenge einen Naive-Bayes-Klassifikator und wenden Sie diesen auf die beiden Test-Dokumente an.\nNaive Bayes Verallgemeinerte Bayes Regel $$ P(H|D_1, \\ldots, D_n) = \\frac{P(D_1, \\ldots, D_n | H)P(H)}{P(D_1, \\ldots, D_n)} $$\nAnnahme: $D_i$ sind bedingt unabhängig $$ P(D_1, \\ldots, D_n | H) = P(D_1 | H) \\cdot \\ldots \\cdot P(D_n | H) = \\prod_i P(D_i | H) $$\nBeobachtung: $P(D_1, \\ldots, D_n)$ für alle Hypothesen $h \\in H$ gleich\nNaive Bayes Klassifikator bzw. MAP (\"Maximum a Posteriori\") $$ h_{MAP} = \\operatorname{argmax}_{h \\in H} P(h | D_1, \\ldots, D_n) = \\operatorname{argmax}_{h \\in H} P(h) \\prod_i P(D_i | h) $$\nNaive Bayes: Wähle die plausibelste Hypothese, die von den Daten unterstützt wird.\nBayes'sches Lernen $$ h_{MAP} = \\operatorname{argmax}_{h \\in H} P(h | D_1, \\ldots, D_n) = \\operatorname{argmax}_{h \\in H} P(h) \\prod_i P(D_i | h) $$ Training: Bestimme die Wahrscheinlichkeiten aus Trainingsdaten $\\mathbf{S}$\nFür jede Klasse $h$: Schätze $P(h) = \\dfrac{|S(h)|}{|S|}$ Für jedes Attribut $D_i$ und jede Ausprägung $x \\in D_i$: Schätze $P(D_i=x | h) = \\dfrac{|S_{D_i}(x) \\cap S(h)|}{|S(h)|}$ Klassifikation: Wähle wahrscheinlichste Klasse $h_{MAP}$ für Vektor $\\mathbf{x}$\n$h_{MAP} = \\operatorname{argmax}_{h \\in H} P(h) \\prod_{x \\in \\mathbf{x}} P(x | h)$ Beispiel Klassifikation mit NB Nase läuft Husten Gerötete Haut Fieber Klasse 1 1 1 0 krank 1 1 0 0 krank 0 0 1 1 krank 1 0 0 0 gesund 0 0 0 0 gesund Eingabe: Person mit Husten und Fieber Gesucht: $P(\\text{krank})$, $P(\\text{gesund})$, $P(\\text{Nase=0}|\\text{krank})$, $P(\\text{Nase=0}|\\text{gesund})$, ...\nWähle Klasse $$ \\begin{array}{rl} h_{MAP} = \\operatorname{argmax}_{h \\in \\lbrace \\text{gesund, krank} \\rbrace} \u0026 P(h) \\cdot P(\\text{Nase=0}|h) \\cdot P(\\text{Husten=1}|h) \\\\ \u0026 \\cdot P(\\text{Haut=0}|h) \\cdot P(\\text{Fieber=1}|h) \\end{array} $$\nErgebnis: (nur die für den zu klassifizierenden Beispiel-Vektor nötigen Werte, die restlichen müssten aber auch beim \"Training\" berechnet werden!)\nP(gesund) = 2/5 = 0.4 P(krank) = 3/5 = 0.6 P(Nase=0 | gesund) = 1/2 = 0.5 P(Nase=0 | krank) = 1/3 = 0.333 P(Husten=1 | gesund) = 0/2 = 0 P(Husten=1 | krank) = 2/3 = 0.667 P(Haut=0 | gesund) = 2/2 = 1 P(Haut=0 | krank) = 1/3 = 0.333 P(Fieber=1 | gesund) = 0/2 = 0 P(Fieber=1 | krank) = 1/3 = 0.333 h = gesund: P(gesund) * P(Nase=0 | gesund) * P(Husten=1 | gesund) * P(Haut=0 | gesund) * P(Fieber=1 | gesund) = 0.4*0.5*0*1*0 = 0 h = krank: P(krank) * P(Nase=0 | krank) * P(Husten=1 | krank) * P(Haut=0 | krank) * P(Fieber=1 | krank) = 0.6*0.333*0.667*0.33*0.333 = 0.015 =\u003e Klasse \"krank\" gewinnt ...\nTextklassifikation mit NB Texte als Trainingsmenge:\nText zerlegen in Terme (Wörter, sonstige relevante Token) ggf. Entfernen von Stoppwörtern (beispielsweise Artikel u.ä.) ggf. Stemming und Lemmatisierung für restliche Terme ggf. weitere Vorverarbeitungsschritte (Groß-Klein-Schreibung, ...) Terme zusammenfassen als Menge: \"Bag of Words\" (mit Häufigkeit) Naive Bayes \"trainieren\":\nA-priori-Wahrscheinlichkeit der Klassen: $P(c) = \\dfrac{N_c}{N} = \\dfrac{\\text{Anzahl Dokumente in Klasse c}}{\\text{Anzahl Dokumente}}$\nLikelihood der Daten (Terme):\n$P(t|c) = \\dfrac{\\operatorname{count}(t,c)}{\\sum_{v \\in V} \\operatorname{count}(v,c)}$ mit $\\operatorname{count}(t,c)$ Anzahl der Vorkommen von Term $t$ in allen Dokumenten der Klasse $c$ und $V$ die Vereinigung aller Terme aller Dokumente (als Menge)\nVariante mit Laplace-Glättung (s.u.): $P(t|c) = \\dfrac{\\operatorname{count}(t,c) + 1}{\\sum_{v \\in V} \\operatorname{count}(v,c) + |V|}$\nNaivität im Naive Bayes Unabhängigkeit der Attribute oft nicht gegeben\n=\u003e $P(D_1, \\ldots, D_n | H) \\ne \\prod_i P(D_i | H)$\nA-posteriori-Wahrscheinlichkeiten oft unrealistisch nah an 1 oder 0\nPraxis: Dennoch häufig sehr gute Ergebnisse\nWichtig: Solange die Maximierung über alle Hypothesen die selben Ergebnisse liefert, müssen die konkreten Schätzungen/Werte nicht exakt stimmen ...\nWenn Attribute nicht (bedingt) unabhängig sind, kann sich der NB verschätzen, d.h. es kommt dann u.U. zu einer höheren Fehlerrate, da bestimmte Eigenschaften in der Trainingsmenge zu hoch gewichtet werden.\nLaplace-Schätzer Problem: Attribut-Ausprägung für bestimmte Klasse nicht in Trainingsmenge:\n=\u003e Bedingte Wahrscheinlichkeit ist 0 =\u003e Produkt gleich 0 Lösung: \"Laplace-Schätzer\" (auch \"Laplace-Glättung\")\nStatt $P(D_i=x | h) = \\dfrac{|S_{D_i}(x) \\cap S(h)|}{|S(h)|}$\nnutze $P(D_i=x|h) = \\dfrac{|S_{D_i}(x) \\cap S(h)| + m \\cdot p_i}{|S(h)| + m}$\nmit $m$: frei wählbarer Faktor, und\n$p_i$: A-priori-Wahrscheinlichkeit für $P(D_i=x|h)$\nHintergrundwissen oder einfach uniforme Verteilung der Attributwerte: $p_i = 1/|D_i|$ (Wahrscheinlichkeit für eine Attributausprägung ist 1/(Anzahl der Ausprägungen des Attributs))\n=\u003e \"virtuelle\" Trainingsbeispiele ($m$ ist die Zahl der virtuellen Trainingsbeispiele)\nProbleme mit Floating Point Underflow MAP berechnet Produkt mit vielen Termen\nProblem: Bei kleinen Zahlen kann Floating Point Underflow auftreten!\nLösung: Logarithmus maximieren (Produkt geht in Summe über)\nErinnerung: $\\log(x \\cdot y) = \\log(x) + \\log(y)$ und Logarithmus streng monoton\n$$ \\begin{array}{rcl} h_{MAP} \u0026=\u0026 \\operatorname{argmax}_{h \\in H} P(h|D_1, \\ldots, D_n) \\\\[5pt] \u0026=\u0026 \\operatorname{argmax}_{h \\in H} P(h) \\prod_i P(D_i | h) \\\\[5pt] \u0026=\u0026 \\operatorname{argmax}_{h \\in H} [\\log(P(h)) + \\sum_i \\log(P(D_i | h))] \\end{array} $$ Maximum Likelihood Maximum a Posteriori $$ h_{MAP} = \\operatorname{argmax}_{h \\in H} P(h | D_1, \\ldots, D_n) = \\operatorname{argmax}_{h \\in H} P(h) \\prod_i P(D_i | h) $$\nAnnahme: Klassen uniform verteilt =\u003e $P(h_i) = P(h_j)$\nMaximum Likelihood $$ h_{ML} = \\operatorname{argmax}_{h \\in H} \\prod_i P(D_i | h) $$\n=\u003e Maximiere die Likelihood der Daten\nAusblick: Kontinuierliche Attribute Bisher sind wir von diskreten Attributen ausgegangen. Bei kontinuierlichen Attributen hat man zwei Möglichkeiten:\nDiskretisierung der Attribute: Aufteilung in Intervalle und Bezeichnung der Intervalle mit einem Namen Einsatz einer Verteilungsannahme und deren Dichtefunktion, beispielsweise Annahme von normalverteilten Daten mit der Dichtefunktion $$ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} e^{- \\frac{(x - \\mu)^2}{2 \\sigma^2}} $$ wobei $\\mu$ der Mittelwert und $\\sigma^2$ die Varianz der Daten sind. Hinweis zum Sprachgebrauch In Abhängigkeit von der Verteilung der $P(D_i | h)$ spricht man von\n\"multinominalem\" NB: Attribute umfassen mehrere Kategorien (verschiedene Ausprägungen, wie im \"Wahlkampf\"-Beispiel: Attribut \"Bildung\" hat die Ausprägungen \"Abitur\", \"Bachelor\" und \"Master\") Bernoulli NB: Attribute sind binär (Ausprägung 0 oder 1), typischerweise bei der Textklassifikation Gauss'sches NB: Annahme einer Normalverteilung der Attribut-Ausprägungen Wrap-Up Klassifikation mit Naive Bayes Annahme von Unabhängigkeit =\u003e \"Naive\" Bayes Klassifikation Schätzen der bedingten Wahrscheinlichkeiten aus den Trainingsdaten Klassifikation durch Nutzung der geschätzten Wahrscheinlichkeiten Hinweis auf Naivität der Annahme, dennoch sehr gute Erfolge in Praxis Hinweis auf Probleme mit niedrigen Wahrscheinlichkeiten",
    "description": "Medizinische Diagnostik mit NB Bei Arthrose wird in 80 Prozent der Fälle ein steifes Gelenk beobachtet: $P(S|A) = 0.8$ Eine von 10.000 Personen hat Arthrose: $P(A) = 0.0001$ Eine von 10 Personen hat ein steifes Gelenk: $P(S) = 0.1$ =\u003e Ich habe ein steifes Gelenk. Habe ich Arthrose?\nTextklassifikation mit NB Mails, manuell markiert:\nD1: (\"Sieben Zwerge fraßen sieben Ziegen\", OK) D2: (\"Sieben Ziegen traten sieben Wölfe\", SPAM) D3: (\"Sieben Wölfe fraßen sieben Böcke\", OK) D4: (\"Sieben Böcke traten sieben Zwerge\", SPAM) Neue Mails:",
    "tags": [],
    "title": "Klassifikation mit Naive Bayes",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb2-naivebayes.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "CSP.01: Logikrätsel (2P) Betrachten Sie die Variante des berühmten \"Einstein-Rätsels\" auf Wikipedia.\nFormulieren Sie das Problem als CSP (Variablen, Wertebereiche, Constraints) zunächst auf dem Papier. Machen Sie sich klar, was die Variablen und was deren Wertebereiche sind. Schreiben Sie die Constraints als (unäre bzw. binäre) Relationen zwischen den Variablen auf.\nHinweis: Machen Sie sich zunächst klar, was die Variablen und was deren Wertebereiche sind. Schreiben Sie die Constraints als (unäre bzw. binäre) Relationen auf.\nThema: Formulierung von Problemen als CSP\nCSP.02: Framework für Constraint Satisfaction (2P) Lösen Sie nun das obige Rätsel (aus CSP.01):\nLösen Sie das Rätsel zunächst mit dem Basis-Algorithmus BT_Search aus der Vorlesung. Erweitern Sie den Algorithmus um die Heuristiken MRV und Gradheuristik und lösen Sie das Problem erneut. Vergleichen Sie die Ergebnisse und die Laufzeit der beiden Experimente. Wenden Sie vor dem Start von BT_Search den AC-3 an. Erhalten Sie damit bereits eine Lösung (bzw. Unlösbarkeit)? Falls nicht, wenden Sie anschließend den ergänzten Algorithmus aus Schritt (2) an. Vergleichen Sie wieder die Ergebnisse und die Laufzeiten. Sie können dafür eine Handsimulation anwenden oder die Algorithmen implementieren. Sie können gern auch die Java-Klassen im Paket aima.core.search.csp bzw. die Python-Klassen in csp.py als Ausgangspunkt nutzen.1\nCSP.03: Kantenkonsistenz mit AC-3 (3P) Sei $D=\\lbrace 0, \\ldots, 5 \\rbrace$, und ein Constraintproblem definiert durch\n$$\\langle \\lbrace v_1, v_2, v_3, v_4 \\rbrace, \\lbrace D_{v_1} = D_{v_2} = D_{v_3} = D_{v_4} = D \\rbrace, \\lbrace c_1, c_2, c_3, c_4 \\rbrace \\rangle$$ mit\n$c_1=\\left((v_1,v_2), \\lbrace (x,y) \\in D^2 | x+y = 3 \\rbrace\\right)$, $c_2=\\left((v_2,v_3), \\lbrace (x,y) \\in D^2 | x+y \\le 3 \\rbrace\\right)$, $c_3=\\left((v_1,v_3), \\lbrace (x,y) \\in D^2 | x \\le y \\rbrace\\right)$ und $c_4=\\left((v_3,v_4), \\lbrace (x,y) \\in D^2 | x \\ne y \\rbrace\\right)$. (1P) Zeichen Sie den Constraint-Graph (2P) Wenden Sie den AC-3-Algorithmus auf das CSP an. Geben Sie den Zustand der Queue und das Ergebnis von ARC_Reduce, d.h. den Ergebniszustand des aktuellen $D_i$, für jede Iteration des Algorithmus an. Thema: Handsimulation des AC-3-Algorithmus\nCSP.04: Forward Checking und Kantenkonsistenz (2P) Betrachten Sie erneut das CSP aus der vorigen Aufgabe und die Zuweisung $\\alpha = \\lbrace v_1 \\to 2 \\rbrace$.\n(1P) Erzeugen Sie Kantenkonsistenz in $\\alpha$. Geben Sie hierzu die Wertebereiche der Variablen vor und nach dem Erzeugen der Kantenkonsistenz an.\nHinweis: Sie dürfen annehmen, dass der Wertebereich von Variablen mit bereits zugewiesenen Werten nur aus dem zugewiesenen Wert besteht, während unbelegte Variablen den vollen Wertebereich haben.\nHinweis: Sie müssen zur Lösung dieser Teilaufgabe nicht den AC-3 nutze.\n(1P) Führen Sie Forward-Checking in $\\alpha$ aus. Vergleichen Sie das Ergebnis mit (1).\nThema: Kantenkonsistenz und Forward Checking verstehen\nCSP.05: Anwendungen (1P) Recherchieren Sie, in welchen Anwendungen CSP vorkommen und mit der BT-Suche (plus Heuristiken) oder sogar AC-3 gelöst werden. Erklären Sie kurz, wie und wofür die Algorithmen jeweils genutzt werden.\nThema: Anwendungen von CSP, BT-Suche und AC-3\nIm Python-Code tauchen immer wieder \"TODO\"-Marker auf - bitte mit Vorsicht genießen! ↩︎",
    "description": "CSP.01: Logikrätsel (2P) Betrachten Sie die Variante des berühmten \"Einstein-Rätsels\" auf Wikipedia.\nFormulieren Sie das Problem als CSP (Variablen, Wertebereiche, Constraints) zunächst auf dem Papier. Machen Sie sich klar, was die Variablen und was deren Wertebereiche sind. Schreiben Sie die Constraints als (unäre bzw. binäre) Relationen zwischen den Variablen auf.\nHinweis: Machen Sie sich zunächst klar, was die Variablen und was deren Wertebereiche sind. Schreiben Sie die Constraints als (unäre bzw. binäre) Relationen auf.",
    "tags": [],
    "title": "Übungsblatt: Constraints",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-csp.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Organisatorisches",
    "content": "Elektronische Klausur: Termin, Materialien Termin Die schriftliche Prüfung erfolgt durch eine Klausur, die als digitale Prüfung auf einem Prüfungs-ILIAS durchgeführt wird.\nEs wird angestrebt, die Klausur in Präsenz in den Rechnerpools am Campus Minden durchzuführen. Falls dies wegen der Corona-Situation oder anderer Umstände nicht möglich sein sollte, wird die Klausur als \"Open-Book-Ausarbeitung\" im Home-Office durchgeführt.\nEs wird in beiden Prüfungszeiträumen ein Termin angeboten. Die Termine werden vom Prüfungsamt bekannt gegeben.\nDauer jeweils 90 Minuten.\nDie konkrete Durchführungsform (in Präsenz am Campus Minden oder im Home-Office) wird Ihnen spätestens zwei Wochen vor der Prüfung über das LSF bekanntgegeben Zugelassene Hilfsmittel ​ Präsenz (in Minden) Open-Book-Ausarbeitung (Homeoffice) Zugelassene Materialien: DIN-A4-Spickzettel (beidseitig)\nSie dürfen einen Spickzettel im DIN-A4-Format benutzen, der beidseitig beschrieben sein kann.\nIch möchte Sie hier noch einmal ermuntern, diesen Zettel tatsächlich manuell zu erstellen (also ganz traditionell zu schreiben), da bereits der Schreibvorgang einen gewissen Lerneffekt bewirkt!\nFalls die Prüfung als Open-Book-Ausarbeitung im Home-Office durchgeführt werden sollte, dürfen Sie alle Unterlagen benutzen.\nAusnahme: Keine Hilfe durch Dritte! (insbesondere keine Zusammenarbeit, keine Kommunikation)\nSie sollen die Prüfung eigenständig bearbeiten. Hilfe von Dritten sowie jegliche Kommunikation mit Dritten ist in keinem Fall zugelassen und wird als Täuschungsversuch gewertet.\nEinsicht Prüfungseinsicht: Zeitnah; Bekanntgabe per Mail Technische Vorbereitungen ​ Präsenz (in Minden) Open-Book-Ausarbeitung (Homeoffice) Diese Bemerkungen betreffen die Durchführung als Präsenzprüfung in den Räumen am Campus Minden.\nHSBI-Zugangsdaten: Username, Passwort\nBei der Durchführung der Prüfung am Campus Minden wird Ihnen ein Rechner zur Verfügung gestellt. Dort läuft voraussichtlich ein Browser im Kiosk-Mode, wo Sie sich am Prüfungs-ILIAS anmelden. Dazu benötigen Sie ihre HSBI-Zugangsdaten, mit denen Sie sich auch im \"normalen\" ILIAS anmelden.\nStudierendenausweis und Personalausweis\nAn der Prüfung dürfen nur Personen teilnehmen, die dafür im LSF angemeldet sind. Es findet eine entsprechende Kontrolle statt. Halten Sie Ihren Studierendenausweis und Personalausweis bereit.\nDiese Bemerkungen betreffen die Durchführung aus dem Home-Office mit Ihrer Hardware. Bei der Durchführung in Präsenz in den Räumen am Campus Minden werden die technischen Details von uns für Sie vorbereitet sein.\nRechner: Nutzen Sie für die Prüfung einen stationären Rechner oder ein Notebook.\nVermeiden Sie die Verwendung von Tablets und Smartphones! Bei der Verwendung von Tablets kann es unter Umständen zu Darstellungsproblemen kommen. Smartphones sind aufgrund des kleinen Bildschirms für die Prüfungsdurchführung schlicht ungeeignet.\nBei fehlendem Zugang zu einem entsprechenden Endgerät kontaktieren Sie bitte frühzeitig die Prüfungsverantwortlichen.\nNetz: Stabil genug? Belastbar genug?\nWenn Sie keinen Zugang zu einer ausreichend stabilen Internetverbindung haben, setzen Sie sich frühzeitig mit Ihren Prüfungsverantwortlichen in Verbindung.\nVPN: Der Prüfungs-ILIAS ist nur im HSBI-VPN erreichbar.\nInstallieren Sie den VPN-Client (Anleitung: hsbi.de/dvz/faq/cat/7) und testen Sie im Vorfeld der Prüfung bei aktivierter VPN-Verbindung den Zugang zur Prüfungsplattform eassessment.hsbi.de. Zugangsdaten wie im normalen ILIAS.\nAchtung: Auch wenn Sie sich in den Räumen der HSBI befinden, müssen Sie oft die VPN-Verbindung aktivieren, um Zugang zur Prüfungsplattform zu erhalten.\nBrowser: Nutzen Sie einen der Standardbrowser (Edge, Firefox, Safari, Chrome/Chromium) in der Standardeinstellung: insbesondere JavaScript und Cookies müssen aktiviert/erlaubt sein.\nDeaktivieren Sie sämtliche Browser-Erweiterungen wie z.B. Ad-Blocker (AdBlockPlus, uBlock, ...) oder JavaScript-Blocker (No-Script, Ghostery, ...) für den Prüfungszeitraum.\nNutzen Sie Ihren Browser nicht im Privacy-Modus!\nHSBI-Zugangsdaten: Username, Passwort\nBei der Durchführung der Prüfung als Open-Book-Ausarbeitung führen Sie die Prüfung auf Ihrer eigenen Hardware im Home-Office durch. Auch hier müssen Sie sich am Prüfungs-ILIAS anmelden. Dazu benötigen Sie ihre HSBI-Zugangsdaten, mit denen Sie sich auch im \"normalen\" ILIAS anmelden.\nBearbeitung des E-Assessment Lesen Sie sich die Hinweise auf der Startseite durch\nBearbeiten Sie die Aufgaben in einem einzigen Browser-Tab\nÖffnen Sie die Aufgaben NICHT in parallelen Tabs! Es kann sonst zu Fehlfunktionen von ILIAS kommen.\nBewegen Sie sich nicht per Browser-Navigation (\"vor\", \"zurück\" im Browser) durch die Aufgaben, sondern nutzen Sie dafür die Buttons \"nächste Frage\", \"Weiter\" oder \"Zurück\" vom ILIAS!\nHinweis zu Anzeige der restlichen Bearbeitungsdauer\nWenn Sie den Browser bzw. das Tab mit der Prüfung im Laufe der Prüfung verlassen, wird Ihnen bei der Rückkehr unter Umständen eine falsche restliche Bearbeitungsdauer angezeigt. Sie können die Anzeige korrigieren/aktualisieren, indem Sie einfach zu einer vorigen oder nächsten Aufgabe navigieren.\nHinweis: Die restliche Bearbeitungsdauer wird im Test nur dann angezeigt, wenn diese Funktion von den Prüfenden aktiviert wurde.\nParallel zum E-Assessment läuft eine Zoom-Session, dort können Sie Fragen stellen\nVerbindungsprobleme (Home-Office):\nBei kurzzeitigen Verbindungsabbrüchen loggen Sie sich einfach wieder ein Wenn die Probleme länger dauern, gilt der Versuch als nicht unternommen (außer Sie haben die Probleme aktiv herbeigeführt, dann kann das als Täuschungsversuch gewertet werden, vgl. RPO §22a (4)) Fragetypen-Demo In Ihrem ILIAS-Kurs finden Sie eine Fragetypen-Demo mit den wichtigsten Fragetypen. Machen Sie sich mit der Mechanik der Fragetypen vertraut und schauen Sie sich die Kommentare bei den einzelnen Aufgaben an. Sie können die Demo bei Bedarf beliebig oft wiederholen.\nHinweise zu den Inhalten Klausurrelevant: Vorlesung und Praktikum\nFür Verständnis u.U. hilfreich: Studium der vertiefenden Literaturangaben\nFragen:\nSchauen Sie sich die Challenges und/oder Quizzes an ... Schauen Sie sich die Praktikumsaufgaben an ... Überlegen Sie sich, was zu einem Themengebiet im Rahmen einer Prüfung möglich ist und (wie) gefragt werden könnte :) Können vor Kennen :-)\nBeispiele für mögliche Fragen Breitensuche Betrachten Sie den folgenden Graphen:\nFühren Sie eine Handsimulation der Breitensuche durch. Start ist \"K\", Ziel ist \"B\".\nGeben Sie den Zustand der Queue in jedem Schritt an.\nSuche Worin liegt der Unterschied zwischen einem Knoten und einem Zustand? Was ist ein Suchbaum? Wo liegt der Unterschied zum Zustandsraum? Muss der Suchbaum über eine eigene Datenstruktur realisiert werden? Was bedeutet Kantenkonsistenz? Handsimulation Perzeptron Führen Sie den Perzeptron-Lernalgorithmus in Handsimulation durch. Lernen Sie die Boolesche Funktion OR. Die Trainingsmenge ist: ...\nIhr Programm soll eine Tabelle mit den Spalten\n$w_1$, $w_2$, $-\\theta$, $x_1$, $x_2$, $k$, $\\sum_{i=1}^n w_i x_i - \\theta$, Vorhersage, Änderung? (=,+,-)\nausgeben, die eine Zeile für jeden Lernschritt enthält. Geben Sie die vollständige Lerntabelle an.\nAnders als in der Vorlesung besprochen, sollen die Gewichte $w_1$ und $w_2$ sowie die Schwelle $\\theta$ jeweils mit dem Wert $0$ initialisiert werden. Die Lernschrittweite $\\alpha$ sei $0.5$. Nutzen Sie die $\\operatorname{sign}$-Funktion als Aktivierungsfunktion: $$ \\operatorname{sign}(x) = \\left\\{ \\begin{array}{ll} 0 \u0026 \\text{falls } x\u003c0\\\\ 1 \u0026 \\text{sonst} \\end{array} \\right. $$\nZeichnen Sie eine geometrische Interpretation Ihres Perzeptrons nach dem Lernen (Trennebene, Gewichtsvektor, Trainingsbeispiele).\nEntscheidungsbäume mit ID3 Gegeben sei folgender Trainingsdatensatz ...\nBauen Sie mit Hilfe von ID3 einen Entscheidungsbaum auf. Welches Attribut würde als erster Test verwendet werden und warum? Wie bestimmt sich die Reihenfolge der weiteren Attribute?\nAls Hilfe gegeben: Einige wichtige Logarithmen zur Basis 2\nCSP Betrachten Sie das folgende Constraintproblem:\n$\\mathrm{C}_1 = \\{(a,b) | a \\ne b\\}$ $\\mathrm{C}_2 = \\{(b,c) | b \\ne c\\}$ $\\mathrm{C}_3 = \\{(a,c) | a = c\\}$ $D_a=D_b=D_c=\\{1,2,3,4,5\\}$, $D_d=\\{1,2,5\\}$, $D_e=\\{1,3\\}$\nZeichnen Sie den zugehörigen Constraintgraphen.\nZeigen Sie per Handsimulation, wie der Algorithmus AC-3 schrittweise Kantenkonsistenz herstellt.\nBackpropagation In der Vorlesung wurde die Delta-Regel bzw. die Gewichtsupdates bei der Backpropagation für die Ausgabeschicht und die davor liegende letzte Hidden-Layer unter Verwendung der Transferfunktion $g(a) = 1/(1+\\exp(-a))$ vorgerechnet.\nLeiten Sie die Gewichtsupdates für die letzte und die vorletzte Schicht unter Verwendung der alternativen Transferfunktion $g(a) = \\tanh(a)$ her.",
    "description": "Elektronische Klausur: Termin, Materialien Termin Die schriftliche Prüfung erfolgt durch eine Klausur, die als digitale Prüfung auf einem Prüfungs-ILIAS durchgeführt wird.\nEs wird angestrebt, die Klausur in Präsenz in den Rechnerpools am Campus Minden durchzuführen. Falls dies wegen der Corona-Situation oder anderer Umstände nicht möglich sein sollte, wird die Klausur als \"Open-Book-Ausarbeitung\" im Home-Office durchgeführt.\nEs wird in beiden Prüfungszeiträumen ein Termin angeboten. Die Termine werden vom Prüfungsamt bekannt gegeben.",
    "tags": [],
    "title": "Prüfungsvorbereitung (HSBI)",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-hsbi.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Praktikum/Übung",
    "content": "NB.01: Wahlkampf mit Naive Bayes (4P) Betrachten Sie erneut das Szenerio von Aufgabe DTL.01.\n(2P) \"Trainieren\" Sie für den gezeigten Datensatz einen Naive Bayes Klassifikator (manuell).\n(2P) Welchen Kandidaten würde der Klassifikator einem Wähler ($\u003c 35$, niedrig, Bachelor) zuordnen? Erklären Sie die Arbeitsweise des Klassifikators.\nNB.02: Textklassifikation mit Naive Bayes: Spam-Erkennung (6P) Laden Sie sich den Datensatz \"Spam Mails Dataset\" (Kaggle) herunter. Dieser besteht aus knapp 5000 vorklassifizierten Einträgen (Mails mit den Klassen ham bzw. spam).\n(2P) Bereiten Sie diesen Datensatz für das Training eines Naive Bayes Klassifikators vor. Überlegen Sie sich, was mögliche Merkmale sein könnten und schreiben Sie sich ein Skript, welches den Datensatz entsprechend bearbeitet/transformiert. (Tipp: Ein \"Bag-of-Words\" ist ein guter Anfang.)\n(2P) Implementieren Sie einen Naive Bayes Klassifikator in einer Programmiersprache Ihrer Wahl oder machen Sie sich mit existierenden Implementierungen vertraut, beispielsweise in NLTK oder scikit-learn oder Weka.\n(2P) Splitten Sie den vorbereiteten Datensatz in eine Trainings- und eine Testmenge auf und trainieren Sie den Naive Bayes Klassifikator. Wie sieht ihr Klassifikator aus, was sind die wichtigsten Begriffe jeweils für die Klasse spam bzw. ham? Bewerten Sie das Testergebnis.",
    "description": "NB.01: Wahlkampf mit Naive Bayes (4P) Betrachten Sie erneut das Szenerio von Aufgabe DTL.01.\n(2P) \"Trainieren\" Sie für den gezeigten Datensatz einen Naive Bayes Klassifikator (manuell).\n(2P) Welchen Kandidaten würde der Klassifikator einem Wähler ($\u003c 35$, niedrig, Bachelor) zuordnen? Erklären Sie die Arbeitsweise des Klassifikators.\nNB.02: Textklassifikation mit Naive Bayes: Spam-Erkennung (6P) Laden Sie sich den Datensatz \"Spam Mails Dataset\" (Kaggle) herunter. Dieser besteht aus knapp 5000 vorklassifizierten Einträgen (Mails mit den Klassen ham bzw. spam).",
    "tags": [],
    "title": "Übungsblatt: Naive Bayes",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/homework/sheet-nb.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Neuronale Netze",
    "content": "Kurze Übersicht TODO TODO",
    "description": "Kurze Übersicht TODO TODO",
    "tags": [],
    "title": "NN10 - Vorschau Deep Learning (CNN, RNN)",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn10-cnn.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25) \u003e Organisatorisches",
    "content": "Notenzusammensetzung Prüfung Gewicht Zwischenprüfung 40 % Endprüfung 60 % Übung 10 % Bonus für Endprüfung Wenn in der Endprüfung die 40 Punkte Mindestgrenze erreicht wird (Endprüfungsnote ≥40), werden 10 % der Übungspunkte als Bonus zu der Endprüfungsnote hinzugefügt.\nÜbungspunkte Für die Vergabe von Übungspunkten ist eine erfolgreiche Teilnahme an der Übung erforderlich. Dies bedeutet: Aufgaben lösen, gelöste Aufgaben ankreuzen und Lösungen in Übungsstunde vorstellen.\nAufgaben lösen Lösen Sie die Aufgaben auf dem jeweiligen Übungsblatt und laden Sie Ihre Lösungen im Google Classroom hoch. Bei Textaufgaben laden Sie Ihre Lösungen als eine PDF-Datei im Google Classroom hoch. Lösen Sie Programmieraufgaben in einem Jupyter Notebook auf Google Colaboratory und laden Sie dieses als eine .ipnyb Datei hoch. Benennen Sie Ihre Dateien wie folgt, benutzen Sie dabei Ihren vollen Namen: Vorname_Nachname_UE1.pdf Vorname_Nachname_UE1.ipnyb Die Bearbeitung der Aufgaben erfolgt individuell. Das Diskutieren der Themen untereinander ist vorteilhaft, das Teilen von Lösungen nicht! Gelöste Aufgaben ankreuzen Geben Sie im Google Classroom über das bereitgestellte Google Formular an, welche Aufgaben Sie gelöst haben. Sie müssen in der Lage sein, Ihre Lösungen zu den angekreuzten Aufgaben in der Übungsstunde vorzustellen. Die Lösung muss nicht unbedingt 100% korrekt sein, muss aber eine intensive Beschäftigung mit der jeweiligen Aufgabe erkennen lassen (ca. 60%). Lösungen vorstellen Seien Sie pünktlich anwesend in der Übungsstunde und (wenn aufgefordert) stellen Sie Ihre eigene Lösung vor. Die Vorstellung kann mit einem Medium Ihrer Wahl stattfinden. Sie können zum Beispiel Ihren Bildschirm teilen und Ihre Lösung erklären. Achtung: Wenn man angibt, eine Aufgabe gelöst zu haben, dann aber in der Übungsstunde nicht (oder zu spät) erscheint oder die Lösung nicht vorstellen kann, bekommt man für das gesamte Übungsblatt 0 Punkte! Schriftliche Prüfungen Raum und Zeit der Prüfungen werden rechtzeitig angekündigt. Prinzipiell sind alle Themen der Vorlesung auch Teil der Prüfung (wenn nicht anders bekanntgegeben). Sie müssen in der Prüfung kein Code schreiben. Verständnis und Anwendung von Pseudocode kann abgefragt werden. Anwendung und Interpretation von Algorithmen (auf Papier) kann abgefragt werden. Sie dürfen ein mit Ihrer Handschrift befülltes A4 Blatt mit in die Prüfung nehmen Sie dürfen das Blatt beidseitig beliebig füllen. Gedruckte oder fotokopierte Blätter sind nicht erlaubt und werden am Anfang der Prüfung eingesammelt. Außer diesem Blatt dürfen keine weiteren Materialien benutzt werden.",
    "description": "Notenzusammensetzung Prüfung Gewicht Zwischenprüfung 40 % Endprüfung 60 % Übung 10 % Bonus für Endprüfung Wenn in der Endprüfung die 40 Punkte Mindestgrenze erreicht wird (Endprüfungsnote ≥40), werden 10 % der Übungspunkte als Bonus zu der Endprüfungsnote hinzugefügt.\nÜbungspunkte Für die Vergabe von Übungspunkten ist eine erfolgreiche Teilnahme an der Übung erforderlich. Dies bedeutet: Aufgaben lösen, gelöste Aufgaben ankreuzen und Lösungen in Übungsstunde vorstellen.\nAufgaben lösen Lösen Sie die Aufgaben auf dem jeweiligen Übungsblatt und laden Sie Ihre Lösungen im Google Classroom hoch. Bei Textaufgaben laden Sie Ihre Lösungen als eine PDF-Datei im Google Classroom hoch. Lösen Sie Programmieraufgaben in einem Jupyter Notebook auf Google Colaboratory und laden Sie dieses als eine .ipnyb Datei hoch. Benennen Sie Ihre Dateien wie folgt, benutzen Sie dabei Ihren vollen Namen: Vorname_Nachname_UE1.pdf Vorname_Nachname_UE1.ipnyb Die Bearbeitung der Aufgaben erfolgt individuell. Das Diskutieren der Themen untereinander ist vorteilhaft, das Teilen von Lösungen nicht! Gelöste Aufgaben ankreuzen Geben Sie im Google Classroom über das bereitgestellte Google Formular an, welche Aufgaben Sie gelöst haben. Sie müssen in der Lage sein, Ihre Lösungen zu den angekreuzten Aufgaben in der Übungsstunde vorzustellen. Die Lösung muss nicht unbedingt 100% korrekt sein, muss aber eine intensive Beschäftigung mit der jeweiligen Aufgabe erkennen lassen (ca. 60%). Lösungen vorstellen Seien Sie pünktlich anwesend in der Übungsstunde und (wenn aufgefordert) stellen Sie Ihre eigene Lösung vor. Die Vorstellung kann mit einem Medium Ihrer Wahl stattfinden. Sie können zum Beispiel Ihren Bildschirm teilen und Ihre Lösung erklären. Achtung: Wenn man angibt, eine Aufgabe gelöst zu haben, dann aber in der Übungsstunde nicht (oder zu spät) erscheint oder die Lösung nicht vorstellen kann, bekommt man für das gesamte Übungsblatt 0 Punkte! Schriftliche Prüfungen Raum und Zeit der Prüfungen werden rechtzeitig angekündigt. Prinzipiell sind alle Themen der Vorlesung auch Teil der Prüfung (wenn nicht anders bekanntgegeben). Sie müssen in der Prüfung kein Code schreiben. Verständnis und Anwendung von Pseudocode kann abgefragt werden. Anwendung und Interpretation von Algorithmen (auf Papier) kann abgefragt werden. Sie dürfen ein mit Ihrer Handschrift befülltes A4 Blatt mit in die Prüfung nehmen Sie dürfen das Blatt beidseitig beliebig füllen. Gedruckte oder fotokopierte Blätter sind nicht erlaubt und werden am Anfang der Prüfung eingesammelt. Außer diesem Blatt dürfen keine weiteren Materialien benutzt werden.",
    "tags": [],
    "title": "Prüfung \u0026 Noten (TDU)",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/admin/exams-tdu.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/categories.html"
  },
  {
    "breadcrumb": "IFM 3.2 (PO23) / IFM 5.14 (PO18) / INF701: Künstliche Intelligenz (Winter 2024/25)",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/elearning/data/FH-Bielefeld/lm_data/lm_1358898/tags.html"
  }
]
